<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />










  <meta name="baidu-site-verification" content="dXyzlo70j3" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Natural Language Processing,Paper,Deep Learning," />





  <link rel="alternate" href="/atom.xml" title="Jeff-Chiang" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="不知道取什么的一级标题今天还说说这一篇神作了,主要的参考资料是  原paper: https://arxiv.org/abs/1706.03762 剑林大哥博客里的中文浅析(我觉得看paperweekly公众号的朋友应该都会知道他): https://kexue.fm/archives/4765 刘贺的博客 https://www.52coding.com.cn/index.php?/Articl">
<meta name="keywords" content="Natural Language Processing,Paper,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention Is All You Need浅析">
<meta property="og:url" content="https://jeffchy.github.io/2018/10/14/Attention-Is-All-You-Need浅析/index.html">
<meta property="og:site_name" content="Jeff-Chiang">
<meta property="og:description" content="不知道取什么的一级标题今天还说说这一篇神作了,主要的参考资料是  原paper: https://arxiv.org/abs/1706.03762 剑林大哥博客里的中文浅析(我觉得看paperweekly公众号的朋友应该都会知道他): https://kexue.fm/archives/4765 刘贺的博客 https://www.52coding.com.cn/index.php?/Articl">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/68227395.jpg">
<meta property="og:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/67768606.jpg">
<meta property="og:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/75659670.jpg">
<meta property="og:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/24798913.jpg">
<meta property="og:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/41878014.jpg">
<meta property="og:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/20529845.jpg">
<meta property="og:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/41012200.jpg">
<meta property="og:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/35256709.jpg">
<meta property="og:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/63416539.jpg">
<meta property="og:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/60719083.jpg">
<meta property="og:updated_time" content="2018-10-15T15:18:13.050Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Attention Is All You Need浅析">
<meta name="twitter:description" content="不知道取什么的一级标题今天还说说这一篇神作了,主要的参考资料是  原paper: https://arxiv.org/abs/1706.03762 剑林大哥博客里的中文浅析(我觉得看paperweekly公众号的朋友应该都会知道他): https://kexue.fm/archives/4765 刘贺的博客 https://www.52coding.com.cn/index.php?/Articl">
<meta name="twitter:image" content="http://oj4pv4f25.bkt.clouddn.com/18-10-15/68227395.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://jeffchy.github.io/2018/10/14/Attention-Is-All-You-Need浅析/"/>





  <title>Attention Is All You Need浅析 | Jeff-Chiang</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-123760763-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d6324a63a8be2ad81d8f3e82174037f4";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jeff-Chiang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Tech and Life</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/10/14/Attention-Is-All-You-Need浅析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Attention Is All You Need浅析</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-14T23:01:49+08:00">
                2018-10-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Paper/Note/" itemprop="url" rel="index">
                    <span itemprop="name">Note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/14/Attention-Is-All-You-Need浅析/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/10/14/Attention-Is-All-You-Need浅析/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="不知道取什么的一级标题"><a href="#不知道取什么的一级标题" class="headerlink" title="不知道取什么的一级标题"></a>不知道取什么的一级标题</h1><p>今天还说说这一篇神作了,主要的参考资料是</p>
<ul>
<li>原paper: <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external">https://arxiv.org/abs/1706.03762</a></li>
<li>剑林大哥博客里的中文浅析(我觉得看paperweekly公众号的朋友应该都会知道他): <a href="https://kexue.fm/archives/4765" target="_blank" rel="external">https://kexue.fm/archives/4765</a></li>
<li>刘贺的博客 <a href="https://www.52coding.com.cn/index.php?/Articles/single/66" target="_blank" rel="external">https://www.52coding.com.cn/index.php?/Articles/single/66</a></li>
<li>Pytorch开源实现:<a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/" target="_blank" rel="external">https://github.com/jadore801120/attention-is-all-you-need-pytorch/</a></li>
<li>在一篇论文里面(BERT)里看到的,作者推荐的英文tutorial,Good english tutorial and implementation, for English speakers:) from harvard nlp -&gt; refer to.<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="external">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>
</ul>
<h1 id="笔者的学习顺序"><a href="#笔者的学习顺序" class="headerlink" title="笔者的学习顺序"></a>笔者的学习顺序</h1><ol>
<li>笔者先看了剑林大哥和刘贺大哥的博客</li>
<li>笔者准备去读一遍论文</li>
<li>笔者准备去看哈佛的pytorch tutorial,正好最近需要用pytorch,学一下</li>
</ol>
<h1 id="论文提要"><a href="#论文提要" class="headerlink" title="论文提要"></a>论文提要</h1><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>作者先理了理目前解决<strong>sequence modeling and transduction problems</strong>的主流模型</p>
<ul>
<li>Recurrent models(RNN, LSTM, GRU), 利用hidden state或者加上cell state来传递序列的顺序信息,这种递归的结构<strong>无法并行</strong>,而且Recurrent Model没有办法很好的建模全局的结构信息.因为本质与Markov Process相似.</li>
<li>Attention mechanisms使得模型可以更好的建模依赖关系,但是通常我们把它和RNN一起用.</li>
<li>所以本作提出模型<strong>Tranformer</strong>,避免了RNN,完全基于attention机制,使得模型能够更好地建模输入和输出的全局依赖.而且可以并行.</li>
</ul>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>对于<strong>sequence transduction problems</strong>,目前用得最多的还是seq2seq的结构,我们训练一个Encoder $E$和一个Decoder $D$,通过Encoder,我们将一串Input $x={x_1,x_2,\cdots,x_n}$(比如英文句子的embedding表示)编码成$z={z_1, \cdots, z_n}$, 然后将$z$输入一个Decoder,得到target language (比如Chinese) $y={y_1, y_2, \cdots, y_m}$,以前的Encoder-Decoder结构主要用的是Recurrent Neural Network, 而本文的是将RNN单元替换掉,本质还是遵循着EncoderDecoder的基本思想.</p>
<h3 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/68227395.jpg" alt=""></p>
<h3 id="Encoder长这样"><a href="#Encoder长这样" class="headerlink" title="Encoder长这样"></a>Encoder长这样</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/67768606.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/75659670.jpg" alt=""></p>
<script type="math/tex; mode=display">
 L1 = LayerNorm\{multiHeadSelfAttendLayer(input(x)+posiInput(x))+[input(x)+posiInput(x)]\}</script><script type="math/tex; mode=display">
 L2 = LayerNorm\{ fc(L1)+L1 \}</script><p>这里模型使用了multi head self attention layer, 并且运用了 residual connection来保留原始信息的传播,然后每一层作layer-norm. 简单说下layer norm,别和batch norm搞混了.<br>layer norm: 注意layer norm是对layer的input作normalize,而batchnorm是对每个batch的x的某一个维度作normalize:)<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/24798913.jpg" alt=""></p>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder也是非常类似的,除了加上了一层mask的multihead self attention layer,对这个我的理解是,decoding的时候,你需要防止attention去看那些padding补零的entry,所以要去除这个影响.</p>
<h3 id="真正的encoder-decoder是上面的单元的6次stack-以提升表达能力"><a href="#真正的encoder-decoder是上面的单元的6次stack-以提升表达能力" class="headerlink" title="真正的encoder decoder是上面的单元的6次stack.以提升表达能力"></a>真正的encoder decoder是上面的单元的6次stack.以提升表达能力</h3><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>现在的核心问题是解决什么是文章中的Attention,呢个Multi head self attention 到底是什么.</p>
<h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>是Multi-head Attentionde的比本组成单元,长下图这样.其实和点乘计算相似度的attention是相同的.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/41878014.jpg" alt=""></p>
<ul>
<li>$Q$代表一个个query$q_t$组成的矩阵(比如一个词的embedding) </li>
<li>$K$代表Key,$k_t$组成的矩阵.</li>
<li>$V$代表Value,$v_t$组成的矩阵.</li>
<li>解释一下,attention是根据<strong>相似度训练出来的attention weights</strong>,对目标<strong>input或者output</strong>作加权求和.确定重点关注哪一个部分.所以我们先根据$<q_t, k_t="">$的matmul,得到query与每个key的相似度,然后用一个softmax得到对应的attention weights,也就是key对应的value的每个部分的关注度应该是多少. </q_t,></li>
<li>举个例子Attention在NMT的应用中,(这一段参考<a href="https://www.52coding.com.cn/index.php?/Articles/single/66" target="_blank" rel="external">https://www.52coding.com.cn/index.php?/Articles/single/66</a> 同样是一篇好文).<blockquote>
<p>可以理解为，比如刚翻译完主语，接下来想要找谓语，【找谓语】这个信息就是 query，然后 key 是源句子的编码，通过 query 和 key 计算出 attention weight （应该关注的谓语的位置），最后和 value （源句子编码）计算加权和。</p>
</blockquote>
</li>
</ul>
<p>我们的attention:)<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/20529845.jpg" alt=""><br>这样的矩阵运算是搞笑的,而这样的定义是有普适性的. $\frac{1}{\sqrt{d_k}}$用作normalizing,使得softmax更加soft</p>
<h3 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/41012200.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/35256709.jpg" alt=""><br>看图,很清晰,multihead attention就是先Linear()一下QKV,这些投影的weights是学习到的,使得模型有更强的表达能力.然后将这些输入scaled dot-product attention,然后concat起来</p>
<h3 id="中间的position-wise-的Feed-forward小模块"><a href="#中间的position-wise-的Feed-forward小模块" class="headerlink" title="中间的position wise 的Feed forward小模块"></a>中间的position wise 的Feed forward小模块</h3><p>其实就是<br>$Linear(ReLU(Linear(x)))$<br>但是谷歌又风骚地把它命名为$FFN$<br>可以被理解成两个一维的卷积,per layer</p>
<h3 id="为了保留位置信息-position-embedding作为输入是必须的"><a href="#为了保留位置信息-position-embedding作为输入是必须的" class="headerlink" title="为了保留位置信息,position embedding作为输入是必须的"></a>为了保留位置信息,position embedding作为输入是必须的</h3><p>因为模型中如果没有position embedding作为输入,如果改变两个单词的顺序,模型学到的东西是不会变的.<br>文章中直接用了这样的embedding for position<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/63416539.jpg" alt=""></p>
<h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>文章中用的attention主要是self-attention,也就是说QKV分别等于$query(X),X,X$,query(X)表示找寻与X有关系的单词.我们可以认为这样的self-attention能在encoding部分学到词与词之间的依赖关系.<img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/60719083.jpg" alt=""></p>
<h3 id="整个模型大概就是这样-可以说非常巧妙地使用-并且强化了attention地机制"><a href="#整个模型大概就是这样-可以说非常巧妙地使用-并且强化了attention地机制" class="headerlink" title="整个模型大概就是这样.可以说非常巧妙地使用,并且强化了attention地机制."></a>整个模型大概就是这样.可以说非常巧妙地使用,并且强化了attention地机制.</h3><h2 id="再说说代码-这里代码贴的是pytorch开源实现"><a href="#再说说代码-这里代码贴的是pytorch开源实现" class="headerlink" title="再说说代码 这里代码贴的是pytorch开源实现"></a>再说说代码 这里代码贴的是pytorch开源实现</h2><p><a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/" target="_blank" rel="external">https://github.com/jadore801120/attention-is-all-you-need-pytorch/</a></p>
<h3 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="string">''' Define the Layers '''</span></div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">from</span> transformer.SubLayers <span class="keyword">import</span> MultiHeadAttention, PositionwiseFeedForward</div><div class="line"></div><div class="line">__author__ = <span class="string">"Yu-Hsiang Huang"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' Compose with two layers '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super(EncoderLayer, self).__init__()</div><div class="line">        self.slf_attn = MultiHeadAttention(</div><div class="line">            n_head, d_model, d_k, d_v, dropout=dropout)</div><div class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, enc_input, non_pad_mask=None, slf_attn_mask=None)</span>:</span></div><div class="line">        enc_output, enc_slf_attn = self.slf_attn(</div><div class="line">            enc_input, enc_input, enc_input, mask=slf_attn_mask)</div><div class="line">        enc_output *= non_pad_mask</div><div class="line"></div><div class="line">        enc_output = self.pos_ffn(enc_output)</div><div class="line">        enc_output *= non_pad_mask</div><div class="line"></div><div class="line">        <span class="keyword">return</span> enc_output, enc_slf_attn</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' Compose with three layers '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super(DecoderLayer, self).__init__()</div><div class="line">        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</div><div class="line">        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</div><div class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, dec_input, enc_output, non_pad_mask=None, slf_attn_mask=None, dec_enc_attn_mask=None)</span>:</span></div><div class="line">        dec_output, dec_slf_attn = self.slf_attn(</div><div class="line">            dec_input, dec_input, dec_input, mask=slf_attn_mask)</div><div class="line">        dec_output *= non_pad_mask</div><div class="line"></div><div class="line">        dec_output, dec_enc_attn = self.enc_attn(</div><div class="line">            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)</div><div class="line">        dec_output *= non_pad_mask</div><div class="line"></div><div class="line">        dec_output = self.pos_ffn(dec_output)</div><div class="line">        dec_output *= non_pad_mask</div><div class="line"></div><div class="line">        <span class="keyword">return</span> dec_output, dec_slf_attn, dec_enc_attn</div></pre></td></tr></table></figure>
<h3 id="Sublayers-Multihead-attention-PositionwiseFeedforward"><a href="#Sublayers-Multihead-attention-PositionwiseFeedforward" class="headerlink" title="Sublayers Multihead-attention, PositionwiseFeedforward"></a>Sublayers Multihead-attention, PositionwiseFeedforward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Define the sublayers in encoder/decoder layer '''</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> transformer.Modules <span class="keyword">import</span> ScaledDotProductAttention</div><div class="line"></div><div class="line">__author__ = <span class="string">"Yu-Hsiang Huang"</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' Multi-Head Attention module '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_head, d_model, d_k, d_v, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super().__init__()</div><div class="line"></div><div class="line">        self.n_head = n_head</div><div class="line">        self.d_k = d_k</div><div class="line">        self.d_v = d_v</div><div class="line"></div><div class="line">        self.w_qs = nn.Linear(d_model, n_head * d_k)</div><div class="line">        self.w_ks = nn.Linear(d_model, n_head * d_k)</div><div class="line">        self.w_vs = nn.Linear(d_model, n_head * d_v)</div><div class="line">        nn.init.normal_(self.w_qs.weight, mean=<span class="number">0</span>, std=np.sqrt(<span class="number">2.0</span> / (d_model + d_k)))</div><div class="line">        nn.init.normal_(self.w_ks.weight, mean=<span class="number">0</span>, std=np.sqrt(<span class="number">2.0</span> / (d_model + d_k)))</div><div class="line">        nn.init.normal_(self.w_vs.weight, mean=<span class="number">0</span>, std=np.sqrt(<span class="number">2.0</span> / (d_model + d_v)))</div><div class="line"></div><div class="line">        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, <span class="number">0.5</span>))</div><div class="line">        self.layer_norm = nn.LayerNorm(d_model)</div><div class="line"></div><div class="line">        self.fc = nn.Linear(n_head * d_v, d_model)</div><div class="line">        nn.init.xavier_normal_(self.fc.weight)</div><div class="line"></div><div class="line">        self.dropout = nn.Dropout(dropout)</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, q, k, v, mask=None)</span>:</span></div><div class="line"></div><div class="line">        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head</div><div class="line"></div><div class="line">        sz_b, len_q, _ = q.size()</div><div class="line">        sz_b, len_k, _ = k.size()</div><div class="line">        sz_b, len_v, _ = v.size()</div><div class="line"></div><div class="line">        residual = q</div><div class="line"></div><div class="line">        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)</div><div class="line">        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)</div><div class="line">        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)</div><div class="line"></div><div class="line">        q = q.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(<span class="number">-1</span>, len_q, d_k) <span class="comment"># (n*b) x lq x dk</span></div><div class="line">        k = k.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(<span class="number">-1</span>, len_k, d_k) <span class="comment"># (n*b) x lk x dk</span></div><div class="line">        v = v.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(<span class="number">-1</span>, len_v, d_v) <span class="comment"># (n*b) x lv x dv</span></div><div class="line"></div><div class="line">        mask = mask.repeat(n_head, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># (n*b) x .. x ..</span></div><div class="line">        output, attn = self.attention(q, k, v, mask=mask)</div><div class="line"></div><div class="line">        output = output.view(n_head, sz_b, len_q, d_v)</div><div class="line">        output = output.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>).contiguous().view(sz_b, len_q, <span class="number">-1</span>) <span class="comment"># b x lq x (n*dv)</span></div><div class="line"></div><div class="line">        output = self.dropout(self.fc(output))</div><div class="line">        output = self.layer_norm(output + residual)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> output, attn</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' A two-feed-forward-layer module '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_in, d_hid, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super().__init__()</div><div class="line">        self.w_1 = nn.Conv1d(d_in, d_hid, <span class="number">1</span>) <span class="comment"># position-wise</span></div><div class="line">        self.w_2 = nn.Conv1d(d_hid, d_in, <span class="number">1</span>) <span class="comment"># position-wise</span></div><div class="line">        self.layer_norm = nn.LayerNorm(d_in)</div><div class="line">        self.dropout = nn.Dropout(dropout)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        residual = x</div><div class="line">        output = x.transpose(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">        output = self.w_2(F.relu(self.w_1(output)))</div><div class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">        output = self.dropout(output)</div><div class="line">        output = self.layer_norm(output + residual)</div><div class="line">        <span class="keyword">return</span> output</div></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Natural-Language-Processing/" rel="tag"># Natural Language Processing</a>
          
            <a href="/tags/Paper/" rel="tag"># Paper</a>
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/13/简单说说BERT模型-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/" rel="next" title="简单说说BERT模型:Pre-training of Deep Bidirectional Transformers for Language Understanding ">
                <i class="fa fa-chevron-left"></i> 简单说说BERT模型:Pre-training of Deep Bidirectional Transformers for Language Understanding 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/10/16/论文解读-ACL18-Evaluating-and-Improving-their-Ability-to-Predict-Numbers/" rel="prev" title="如何更好地建模数字:ACL18 Evaluating and Improving their Ability to Predict Numbers">
                如何更好地建模数字:ACL18 Evaluating and Improving their Ability to Predict Numbers <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Jeff Chiang" />
          <p class="site-author-name" itemprop="name">Jeff Chiang</p>
           
              <p class="site-description motion-element" itemprop="description">Personal portal for Sharing Computer Science Technology and some petty things in my life</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">81</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">23</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">41</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/jeffchy" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="jiangchy@shanghaitech.edu.cn" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                    
                      E-Mail
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/jeffchiang/" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      微博
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#不知道取什么的一级标题"><span class="nav-number">1.</span> <span class="nav-text">不知道取什么的一级标题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#笔者的学习顺序"><span class="nav-number">2.</span> <span class="nav-text">笔者的学习顺序</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#论文提要"><span class="nav-number">3.</span> <span class="nav-text">论文提要</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Intro"><span class="nav-number">3.1.</span> <span class="nav-text">Intro</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Architecture"><span class="nav-number">3.2.</span> <span class="nav-text">Model Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#总体结构"><span class="nav-number">3.2.1.</span> <span class="nav-text">总体结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder长这样"><span class="nav-number">3.2.2.</span> <span class="nav-text">Encoder长这样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder"><span class="nav-number">3.2.3.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#真正的encoder-decoder是上面的单元的6次stack-以提升表达能力"><span class="nav-number">3.2.4.</span> <span class="nav-text">真正的encoder decoder是上面的单元的6次stack.以提升表达能力</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention"><span class="nav-number">3.3.</span> <span class="nav-text">Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Scaled-Dot-Product-Attention"><span class="nav-number">3.3.1.</span> <span class="nav-text">Scaled Dot-Product Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-head-attention"><span class="nav-number">3.3.2.</span> <span class="nav-text">Multi-head attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#中间的position-wise-的Feed-forward小模块"><span class="nav-number">3.3.3.</span> <span class="nav-text">中间的position wise 的Feed forward小模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为了保留位置信息-position-embedding作为输入是必须的"><span class="nav-number">3.3.4.</span> <span class="nav-text">为了保留位置信息,position embedding作为输入是必须的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-attention"><span class="nav-number">3.3.5.</span> <span class="nav-text">Self-attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#整个模型大概就是这样-可以说非常巧妙地使用-并且强化了attention地机制"><span class="nav-number">3.3.6.</span> <span class="nav-text">整个模型大概就是这样.可以说非常巧妙地使用,并且强化了attention地机制.</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#再说说代码-这里代码贴的是pytorch开源实现"><span class="nav-number">3.4.</span> <span class="nav-text">再说说代码 这里代码贴的是pytorch开源实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-Decoder"><span class="nav-number">3.4.1.</span> <span class="nav-text">Encoder-Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sublayers-Multihead-attention-PositionwiseFeedforward"><span class="nav-number">3.4.2.</span> <span class="nav-text">Sublayers Multihead-attention, PositionwiseFeedforward</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jeff Chiang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jeffchy-github-io.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://jeffchy.github.io/2018/10/14/Attention-Is-All-You-Need浅析/';
          this.page.identifier = '2018/10/14/Attention-Is-All-You-Need浅析/';
          this.page.title = 'Attention Is All You Need浅析';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://https-jeffchy-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  










  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
