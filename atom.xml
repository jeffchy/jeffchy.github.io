<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jeff-Chiang</title>
  
  <subtitle>Tech and Life</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jeffchy.github.io/"/>
  <updated>2019-10-01T04:49:37.976Z</updated>
  <id>https://jeffchy.github.io/</id>
  
  <author>
    <name>Jeff Chiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>热烈庆祝祖国母亲70周年！</title>
    <link href="https://jeffchy.github.io/2019/10/01/%E7%83%AD%E7%83%88%E5%BA%86%E7%A5%9D%E7%A5%96%E5%9B%BD%E6%AF%8D%E4%BA%B270%E5%91%A8%E5%B9%B4%EF%BC%81/"/>
    <id>https://jeffchy.github.io/2019/10/01/热烈庆祝祖国母亲70周年！/</id>
    <published>2019-10-01T04:36:03.000Z</published>
    <updated>2019-10-01T04:49:37.976Z</updated>
    
    <content type="html"><![CDATA[<h1 id="热烈庆祝中华人民共和国70周年！"><a href="#热烈庆祝中华人民共和国70周年！" class="headerlink" title="热烈庆祝中华人民共和国70周年！"></a>热烈庆祝中华人民共和国70周年！</h1><h1 id="Celebrate-the-70th-anniversary-of-People’s-Republic-of-China"><a href="#Celebrate-the-70th-anniversary-of-People’s-Republic-of-China" class="headerlink" title="Celebrate the 70th anniversary of People’s Republic of China!"></a>Celebrate the 70th anniversary of People’s Republic of China!</h1><h1 id="我爱我的祖国！"><a href="#我爱我的祖国！" class="headerlink" title="我爱我的祖国！"></a>我爱我的祖国！</h1><h1 id="I-LOVE-CHINA！"><a href="#I-LOVE-CHINA！" class="headerlink" title="I LOVE CHINA！"></a>I LOVE CHINA！</h1><h1 id="感谢先辈，致以最崇高的敬意！"><a href="#感谢先辈，致以最崇高的敬意！" class="headerlink" title="感谢先辈，致以最崇高的敬意！"></a>感谢先辈，致以最崇高的敬意！</h1><h1 id="Highest-respect-for-the-forerunners！"><a href="#Highest-respect-for-the-forerunners！" class="headerlink" title="Highest respect for the forerunners！"></a>Highest respect for the forerunners！</h1><p><img src="https://image-jeff.oss-cn-hangzhou.aliyuncs.com/a61c2c59eaf94fc5a847a1503ed3489d.jpeg" alt="CHN70"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;热烈庆祝中华人民共和国70周年！&quot;&gt;&lt;a href=&quot;#热烈庆祝中华人民共和国70周年！&quot; class=&quot;headerlink&quot; title=&quot;热烈庆祝中华人民共和国70周年！&quot;&gt;&lt;/a&gt;热烈庆祝中华人民共和国70周年！&lt;/h1&gt;&lt;h1 id=&quot;Celebrate
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Delta Embedding Learning (ACL19) Notes</title>
    <link href="https://jeffchy.github.io/2019/10/01/Delta-Word-Embedding-ACL19-Notes/"/>
    <id>https://jeffchy.github.io/2019/10/01/Delta-Word-Embedding-ACL19-Notes/</id>
    <published>2019-10-01T04:18:24.000Z</published>
    <updated>2019-10-01T04:34:21.098Z</updated>
    
    <content type="html"><![CDATA[<p>Delta Embedding Learning, Zhang et al, ACL19<br><a href="https://aclweb.org/anthology/papers/P/P19/P19-1322/" target="_blank" rel="external">https://aclweb.org/anthology/papers/P/P19/P19-1322/</a></p><p>ACL的一篇short，方法简单地令人震惊，但是从结果看特别有效，我觉得就很离谱，就，分享一下。</p><p>就简要说说，详细的话去看paper，我保证不需要15分钟就看完了。</p><p>传统的word embedding的使用方法通常是，作为模型的 initialization，然后finetune它，或者不finetune，直接fixed这部分。（如果词表太大这样可以减少很多学习的参数）。</p><p>作者提出了一种新的方法，对比 <strong>作为模型的 initialization，然后finetune它</strong>， 作者固定 pretrained embedding，<strong>再加一个可以训练的embedding</strong> 作为模型的输入，然后对这个可以训练的embedding进行约束，确保他是“delta”的，也就是embedding vector norm比较小，而且vector比较稀疏。</p><p>稍微总结一下，就是训练一个小的embedding，用来“扰动”、“纠正”，fixed的pretrain word embedding。</p><p>其实和finetune相比没有引入更多的参数，结果效果在很多情况下比finetune好，就离谱。</p><p>模型就这么简单，图的话如下：</p><p><img src="https://image-jeff.oss-cn-hangzhou.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-10-01%20%E4%B8%8B%E5%8D%8812.33.34.png" alt="Delta-Word-Embedding"></p><p>方法的优劣不是看简不简单，而是看有不有效。一堆方法凑在一起，还不如一个小的改变有效，真是讽刺呢。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Delta Embedding Learning, Zhang et al, ACL19&lt;br&gt;&lt;a href=&quot;https://aclweb.org/anthology/papers/P/P19/P19-1322/&quot; target=&quot;_blank&quot; rel=&quot;extern
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Effective Adversarial Regulariztion for Machine Translation (Notes)</title>
    <link href="https://jeffchy.github.io/2019/09/28/Effective-Adversarial-Regulariztion-for-Machine-Translation-Notes/"/>
    <id>https://jeffchy.github.io/2019/09/28/Effective-Adversarial-Regulariztion-for-Machine-Translation-Notes/</id>
    <published>2019-09-28T00:44:53.000Z</published>
    <updated>2019-09-28T07:44:28.914Z</updated>
    
    <content type="html"><![CDATA[<p>最近想看看别的领域的文章，博客就随意记录一下。<br>文章是 Effective Adversarial Regulariztion for Machine Translation, Motoki Sato et al, ACL 18. <a href="https://www.aclweb.org/anthology/P19-1020" target="_blank" rel="external">https://www.aclweb.org/anthology/P19-1020</a>.</p><p>把 Adversarial Regularization 技术用在MT上。</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>计算机视觉中利用[扰动input]产生adversarial training example去reduce error，但是NLP中很难，因为通常是离散的symbol。之前有方法利用直接去扰动word embedding来达到这个adversarial training的效果。在文本分类上说明有用。作者这篇文章用在NMT的主流baseline上面。</p><h2 id="Adversarial-Training-On-MT"><a href="#Adversarial-Training-On-MT" class="headerlink" title="Adversarial Training On MT"></a>Adversarial Training On MT</h2><p>我不想打公式啊，我决定随便说说，想知道详细的还是去看paper吧。<br>思路很简单，MT train的时候是建模给定onehot sequence $X\{ x_1, x_2, \cdots x_i\}$ 和 参数$\Theta$产生一个sequence $Y=\{y_1, y_2, \cdots, y_j\}$的对数概率分布, MT的training如图所示,<br><img src="https://image-jeff.oss-cn-hangzhou.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-28%20%E4%B8%8B%E5%8D%882.42.42.png" alt="MT Training 1"><br><img src="https://image-jeff.oss-cn-hangzhou.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-28%20%E4%B8%8B%E5%8D%882.41.56.png" alt="MT Training 2"><br>本文的Adversarial，如之前所说，是在embedding部分加了扰动：<br><img src="https://image-jeff.oss-cn-hangzhou.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-28%20%E4%B8%8B%E5%8D%882.42.27.png" alt="Adversarial Training 1"><br><img src="https://image-jeff.oss-cn-hangzhou.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-28%20%E4%B8%8B%E5%8D%882.42.34.png" alt="Adversarial Training 2"><br>我们的目的是得到最能够干扰模型的扰动，所以我们要maximize这个loss，其中$r$是所有$r_i$的一个concatenation. 经过一些近似，我么可以得到最终的adversarial training的表达式：<br><img src="https://image-jeff.oss-cn-hangzhou.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-28%20%E4%B8%8B%E5%8D%883.08.36.png" alt="Adversarial Training 3"><br>对于MT来说，我么可以在 encoder 的embedding上面加，在也可以在decoder 层的embedding上面加！结果自然是好的，我真的懒得截图了，自行移步好吧。</p><p>看完这篇paper我表示，好像不值得我累死累活写博客，额总结下就是，用一下adversarial training 到NMT上，挺无聊的，over。</p><p>下次我看好paper再决定写不写吧，越来越佛了，真的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近想看看别的领域的文章，博客就随意记录一下。&lt;br&gt;文章是 Effective Adversarial Regulariztion for Machine Translation, Motoki Sato et al, ACL 18. &lt;a href=&quot;https://w
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Some Tensor Arithmetic</title>
    <link href="https://jeffchy.github.io/2019/08/06/Some-Tensor-Arithmetic/"/>
    <id>https://jeffchy.github.io/2019/08/06/Some-Tensor-Arithmetic/</id>
    <published>2019-08-06T02:52:05.000Z</published>
    <updated>2019-08-06T02:55:07.988Z</updated>
    
    <content type="html"><![CDATA[<p>Recommend 2 websites for some tensor arithmetics for beginner (ZH), including tensor k-mode product with vector and matrix, tensor matricization …<br>Very good beginner’s guide.</p><p><a href="http://www.xiongfuli.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2016-06/tensor-decomposition-part1.html" target="_blank" rel="external">http://www.xiongfuli.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2016-06/tensor-decomposition-part1.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/24824550" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/24824550</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Recommend 2 websites for some tensor arithmetics for beginner (ZH), including tensor k-mode product with vector and matrix, tensor matric
      
    
    </summary>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/Note/"/>
    
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
      <category term="Algorithm" scheme="https://jeffchy.github.io/tags/Algorithm/"/>
    
      <category term="Machine Learning" scheme="https://jeffchy.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>RNN-properties</title>
    <link href="https://jeffchy.github.io/2019/07/08/RNN-properties/"/>
    <id>https://jeffchy.github.io/2019/07/08/RNN-properties/</id>
    <published>2019-07-08T03:49:17.000Z</published>
    <updated>2019-07-08T04:17:06.637Z</updated>
    
    <content type="html"><![CDATA[<p>RNN computational properties:<br>adopted from NAACL-HLT 2018 paper<br><a href="https://www.aclweb.org/anthology/N18-1205" target="_blank" rel="external">https://www.aclweb.org/anthology/N18-1205</a> [Recurrent Neural Networks as Weighted Language Recognizers, Chen et. al]<br>如图：<img src="https://s2.ax1x.com/2019/07/08/ZDzmkV.png" alt="ZDzmkV.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;RNN computational properties:&lt;br&gt;adopted from NAACL-HLT 2018 paper&lt;br&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/N18-1205&quot; target=&quot;_blank&quot;
      
    
    </summary>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/Note/"/>
    
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="https://jeffchy.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Uncovering divergent linguistic Information in word embeddings - Paper Notes</title>
    <link href="https://jeffchy.github.io/2019/01/28/Uncovering-divergent-linguistic-Information-in-word-embeddings-Paper-Notes/"/>
    <id>https://jeffchy.github.io/2019/01/28/Uncovering-divergent-linguistic-Information-in-word-embeddings-Paper-Notes/</id>
    <published>2019-01-27T16:57:36.000Z</published>
    <updated>2019-01-28T14:58:59.923Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1809.02094.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1809.02094.pdf</a><br>Paper from arxiv, Eneko Agirre, IXA NLP Group</p><p>This paper answered some very interesting questions, In word-embedding evaluations, tasks focused on differrent aspects of (Word Similarity &amp; Relationship)</p><p>Two axis:</p><ul><li>Similar - Relatedness (reflacts 1 or 2 order coocurrance) </li><li>Syntax - Semantics (rings - ring, happy - glad) </li></ul><p>This paper found that, We can use a <strong>linear transformation</strong> to adjust the learned word embedding to the intrinsic task-specific region!<br>Interesting results haha</p><p>This paper interpreted these aspects as axis, and claim that we cannot reach a optimum that have best performance for every perspective, or in other words, we always have some trade-off between them.</p><p>In the downstream tasks, supervised tasks can learn the optimal transformation automaticly, but those unsupervised tasks that dirrectly use the embeddings cannot be transformed.</p><p>So this paper shows that our word embeddings captures more informations than we expected.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1809.02094.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/pdf/1809.02094.pdf&lt;/a&gt;&lt;br&gt;Paper from arxi
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Probability Theory" scheme="https://jeffchy.github.io/tags/Probability-Theory/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Algorithm" scheme="https://jeffchy.github.io/tags/Algorithm/"/>
    
      <category term="Natural Languate Processing" scheme="https://jeffchy.github.io/tags/Natural-Languate-Processing/"/>
    
  </entry>
  
  <entry>
    <title>NeurlPS 2018 Word Embedding (II), FRAGE: frequency-agnostic word representation</title>
    <link href="https://jeffchy.github.io/2019/01/17/NeurlPS-2018-Word-Embedding-II-FRAGE-frequency-agnostic-word-representation/"/>
    <id>https://jeffchy.github.io/2019/01/17/NeurlPS-2018-Word-Embedding-II-FRAGE-frequency-agnostic-word-representation/</id>
    <published>2019-01-17T13:35:38.000Z</published>
    <updated>2019-01-17T13:53:29.672Z</updated>
    
    <content type="html"><![CDATA[<p>说一下 NeurlPS 2018的第二篇Embedding相关文章。<br>Frequency-Agnostic Word Representation <a href="https://arxiv.org/abs/1809.06858" target="_blank" rel="external">https://arxiv.org/abs/1809.06858</a></p><p>首先作者发现了现有的word embedding的一个现象。不常见的词（low-frequency word)和常见的词(high-frequency word)通常聚集在embedding空间的不同区域，如图：<br><a href="https://imgchr.com/i/kpfgjH" target="_blank" rel="external"><img src="https://s2.ax1x.com/2019/01/17/kpfgjH.md.png" alt="kpfgjH.md.png"></a><br>这回导致比如peking这种少见词和beijing这种词的意义不接近。</p><p>作者提出的方法有趣也简单，利用adversarial training，在训练task-specific word embedding的时候，额外训练一个Discriminator判别器，根据词的向量去做一个二元的分类，判断到底是low/high-frequency word。同时word embedding的训练过程也增加了一个目的（task-dependent loss），就是去fool这个discriminator，让他没办法判别正确这个embedding是否稀少。</p><p>一个例子，设定实在language model下<br><a href="https://imgchr.com/i/kpfLuj" target="_blank" rel="external"><img src="https://s2.ax1x.com/2019/01/17/kpfLuj.md.png" alt="kpfLuj.md.png"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;说一下 NeurlPS 2018的第二篇Embedding相关文章。&lt;br&gt;Frequency-Agnostic Word Representation &lt;a href=&quot;https://arxiv.org/abs/1809.06858&quot; target=&quot;_blank&quot; r
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Python Loading Large Dataset</title>
    <link href="https://jeffchy.github.io/2019/01/07/Python-Loading-Large-Dataset/"/>
    <id>https://jeffchy.github.io/2019/01/07/Python-Loading-Large-Dataset/</id>
    <published>2019-01-07T12:20:21.000Z</published>
    <updated>2019-01-07T12:45:49.984Z</updated>
    
    <content type="html"><![CDATA[<p>今天说些写代码时碰到的问题，关于如何save&amp;load一个大的数据object。<br>我们对比两种工具，joblib和pickle。</p><h3 id="首先说结论-如果是numpy-array，joblib各项性能效果比pickle更好，如果是原生自带的Python-object，如列表套列表，pickle远远快于joblib"><a href="#首先说结论-如果是numpy-array，joblib各项性能效果比pickle更好，如果是原生自带的Python-object，如列表套列表，pickle远远快于joblib" class="headerlink" title="首先说结论,如果是numpy array，joblib各项性能效果比pickle更好，如果是原生自带的Python object，如列表套列表，pickle远远快于joblib"></a>首先说结论,如果是numpy array，joblib各项性能效果比pickle更好，如果是原生自带的Python object，如列表套列表，pickle远远快于joblib</h3><h3 id="If-you-are-dumping-amp-loading-Numpy-objects-joblib-is-a-better-choice-while-you-are-dumping-amp-loading-Python-object-something-like-1-1-1-2-‘hello’-joblib-is-insanely-slow-never-use-it-under-this-condition-use-pickle-which-is-much-much-better"><a href="#If-you-are-dumping-amp-loading-Numpy-objects-joblib-is-a-better-choice-while-you-are-dumping-amp-loading-Python-object-something-like-1-1-1-2-‘hello’-joblib-is-insanely-slow-never-use-it-under-this-condition-use-pickle-which-is-much-much-better" class="headerlink" title="If you are dumping &amp; loading Numpy objects, joblib is a better choice, while you are dumping &amp; loading Python object, something like [[1,1],[(1,2)], ‘hello’], joblib is insanely slow, never use it under this condition, use pickle, which is much, much better."></a>If you are dumping &amp; loading Numpy objects, joblib is a better choice, while you are dumping &amp; loading Python object, something like [[1,1],[(1,2)], ‘hello’], joblib is insanely slow, never use it under this condition, use pickle, which is much, much better.</h3><p>测试代码示例：<br>测试使用简单的jupyter notebook %%timeit<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</div><div class="line"><span class="keyword">import</span> pickle</div><div class="line"></div><div class="line">a = np.random.rand(<span class="number">5000</span>, <span class="number">5000</span>) <span class="comment"># large numpy object 200MB</span></div><div class="line">b = [[<span class="number">1000</span>]*<span class="number">10</span>] * <span class="number">100000</span>    <span class="comment"># large python object 30MB</span></div><div class="line"></div><div class="line"><span class="comment"># NUMPY OBJECT DUMP</span></div><div class="line">joblib.dump(a,<span class="string">'a.jbpkl'</span>) <span class="comment"># 211ms</span></div><div class="line">pickle.dump(a, open(<span class="string">'a.pkl'</span>, <span class="string">'wb'</span>)) <span class="comment"># 474ms</span></div><div class="line"></div><div class="line"><span class="comment"># PYTHON OBJECT DUMP</span></div><div class="line">joblib.dump(b,<span class="string">'b.jbpkl'</span>) <span class="comment"># 1.89s</span></div><div class="line">pickle.dump(b, open(<span class="string">'b.pkl'</span>, <span class="string">'wb'</span>)) <span class="comment"># 29ms</span></div><div class="line"></div><div class="line"><span class="comment"># NUMPY OBJECT LOAD</span></div><div class="line">joblib.load(<span class="string">'a.jbpkl'</span>) <span class="comment"># 247ms</span></div><div class="line">pickle.load(open(<span class="string">'a.pkl'</span>, <span class="string">'rb'</span>)) <span class="comment"># 266ms</span></div><div class="line"></div><div class="line"><span class="comment"># PYTHON OBJECT DUMO</span></div><div class="line">joblib.load(<span class="string">'b.jbpkl'</span>) <span class="comment"># 988ms</span></div><div class="line">pickle.load(open(<span class="string">'b.pkl'</span>, <span class="string">'rb'</span>)) <span class="comment"># 19.8</span></div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天说些写代码时碰到的问题，关于如何save&amp;amp;load一个大的数据object。&lt;br&gt;我们对比两种工具，joblib和pickle。&lt;/p&gt;
&lt;h3 id=&quot;首先说结论-如果是numpy-array，joblib各项性能效果比pickle更好，如果是原生自带的Py
      
    
    </summary>
    
      <category term="Tech" scheme="https://jeffchy.github.io/categories/Tech/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/Tech/Note/"/>
    
      <category term="Code" scheme="https://jeffchy.github.io/categories/Tech/Note/Code/"/>
    
    
      <category term="python" scheme="https://jeffchy.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch中tensor的批量赋值以及选择</title>
    <link href="https://jeffchy.github.io/2018/12/18/Pytorch%E4%B8%ADtensor%E7%9A%84%E6%89%B9%E9%87%8F%E8%B5%8B%E5%80%BC%E4%BB%A5%E5%8F%8A%E9%80%89%E6%8B%A9/"/>
    <id>https://jeffchy.github.io/2018/12/18/Pytorch中tensor的批量赋值以及选择/</id>
    <published>2018-12-18T11:41:41.000Z</published>
    <updated>2018-12-18T12:39:18.601Z</updated>
    
    <content type="html"><![CDATA[<p>今天来谈一谈pytorch中快速批量的赋值和选择的操作,这种方法非常简洁,而且比for循环便利i,j,k index的效率高上很多很多倍,并且可以利用gpu加速.用深度学习框架的时候,务必避免for循环出现.</p><p>我们直接上代码:</p><p>首先我们随机生成一个 2x3x4的tensor,结果:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> torch</div><div class="line"></div><div class="line">In [<span class="number">2</span>]: a = torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</div><div class="line"></div><div class="line">In [<span class="number">3</span>]: a</div><div class="line">Out[<span class="number">3</span>]:</div><div class="line">tensor([[[<span class="number">0.8571</span>, <span class="number">0.1589</span>, <span class="number">0.4834</span>, <span class="number">0.5854</span>],</div><div class="line">         [<span class="number">0.5580</span>, <span class="number">0.0499</span>, <span class="number">0.9528</span>, <span class="number">0.4695</span>],</div><div class="line">         [<span class="number">0.0216</span>, <span class="number">0.5603</span>, <span class="number">0.5546</span>, <span class="number">0.8420</span>]],</div><div class="line"></div><div class="line">        [[<span class="number">0.1714</span>, <span class="number">0.5611</span>, <span class="number">0.6885</span>, <span class="number">0.7318</span>],</div><div class="line">         [<span class="number">0.2412</span>, <span class="number">0.0759</span>, <span class="number">0.0850</span>, <span class="number">0.8739</span>],</div><div class="line">         [<span class="number">0.0810</span>, <span class="number">0.9401</span>, <span class="number">0.2520</span>, <span class="number">0.3242</span>]]])</div></pre></td></tr></table></figure></p><p>我们找出tensor中大于0.8的数,得到一个one-hot的一个index的矩阵.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">5</span>]: idxs = a.ge(<span class="number">0.8</span>)</div><div class="line"></div><div class="line">In [<span class="number">6</span>]: idxs</div><div class="line">Out[<span class="number">6</span>]:</div><div class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</div><div class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</div><div class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]],</div><div class="line"></div><div class="line">        [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</div><div class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</div><div class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]]], dtype=torch.uint8)</div><div class="line"></div><div class="line">In [<span class="number">7</span>]: idxs.size()</div><div class="line">Out[<span class="number">7</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</div></pre></td></tr></table></figure></p><p>直接用如下的方法就能轻松取出这些满足条件的(被打上1的标签)的元素了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">8</span>]: a[idxs]</div><div class="line">Out[<span class="number">8</span>]: tensor([<span class="number">0.8571</span>, <span class="number">0.9528</span>, <span class="number">0.8420</span>, <span class="number">0.8739</span>, <span class="number">0.9401</span>])</div><div class="line"></div><div class="line">In [<span class="number">9</span>]: idxs.sum()</div><div class="line">Out[<span class="number">9</span>]: tensor(<span class="number">5</span>) <span class="comment"># 和上一个tensor的长度相同</span></div></pre></td></tr></table></figure></p><p>同样的,我们也可以批量赋值这些entries<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">10</span>]: a[idxs] = <span class="number">999</span></div><div class="line"></div><div class="line">In [<span class="number">11</span>]: a</div><div class="line">Out[<span class="number">11</span>]:</div><div class="line">tensor([[[<span class="number">999.0000</span>,   <span class="number">0.1589</span>,   <span class="number">0.4834</span>,   <span class="number">0.5854</span>],</div><div class="line">         [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">         [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]],</div><div class="line"></div><div class="line">        [[  <span class="number">0.1714</span>,   <span class="number">0.5611</span>,   <span class="number">0.6885</span>,   <span class="number">0.7318</span>],</div><div class="line">         [  <span class="number">0.2412</span>,   <span class="number">0.0759</span>,   <span class="number">0.0850</span>, <span class="number">999.0000</span>],</div><div class="line">         [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]]])</div></pre></td></tr></table></figure></p><h3 id="小小总结一下-如果你有一个张量-想要批量取出-赋值-访问其中的元素-你可以用与这个张量相同维度的onehot表示批量取出-⚠️onehot表示必须是dtype-torch-uint8"><a href="#小小总结一下-如果你有一个张量-想要批量取出-赋值-访问其中的元素-你可以用与这个张量相同维度的onehot表示批量取出-⚠️onehot表示必须是dtype-torch-uint8" class="headerlink" title="小小总结一下:如果你有一个张量,想要批量取出/赋值/访问其中的元素,你可以用与这个张量相同维度的onehot表示批量取出,⚠️onehot表示必须是dtype=torch.uint8"></a>小小总结一下:如果你有一个张量,想要批量取出/赋值/访问其中的元素,你可以用与这个张量相同维度的onehot表示批量取出,⚠️onehot表示必须是dtype=torch.uint8</h3><p>同样,还是这个2x3x4的张量,如何批量得取出/赋值他的最后一维长度的向量?我们只需要构造一个维度是2x3的onehot表示即可,比如下面代码中我们将三个位置标记成了1.我们就取出了对应位置的3个4维向量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">13</span>]: idxs = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]],dtype=torch.uint8)</div><div class="line"></div><div class="line">In [<span class="number">14</span>]: idxs.size()</div><div class="line">Out[<span class="number">14</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</div><div class="line"></div><div class="line">In [<span class="number">15</span>]: a[idxs]</div><div class="line">Out[<span class="number">15</span>]:</div><div class="line">tensor([[<span class="number">999.0000</span>,   <span class="number">0.1589</span>,   <span class="number">0.4834</span>,   <span class="number">0.5854</span>],</div><div class="line">        [  <span class="number">0.1714</span>,   <span class="number">0.5611</span>,   <span class="number">0.6885</span>,   <span class="number">0.7318</span>],</div><div class="line">        [  <span class="number">0.2412</span>,   <span class="number">0.0759</span>,   <span class="number">0.0850</span>, <span class="number">999.0000</span>]])</div><div class="line"></div><div class="line">In [<span class="number">16</span>]: a[idxs].size()</div><div class="line">Out[<span class="number">16</span>]: torch.Size([<span class="number">3</span>, <span class="number">4</span>])</div><div class="line"></div><div class="line">In [<span class="number">17</span>]: idxs.sum()</div><div class="line">Out[<span class="number">17</span>]: tensor(<span class="number">3</span>)</div></pre></td></tr></table></figure></p><p>赋值操作也是一样的.赋值的向量需要和原始向量有相同的dtype,所以最后一个333笔者把它写成了333.0这样他们的dtype都是float了.可以看到,我们tensor的对应entry被赋值成了对应的向量!<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">21</span>]: b = torch.tensor([[<span class="number">111</span>,<span class="number">111</span>,<span class="number">111</span>,<span class="number">111</span>],[<span class="number">222</span>,<span class="number">222</span>,<span class="number">222</span>,<span class="number">222</span>],[<span class="number">333</span>,<span class="number">333</span>,<span class="number">333</span>,<span class="number">333.0</span>]])</div><div class="line"></div><div class="line">In [<span class="number">22</span>]: a[idxs] = b</div><div class="line"></div><div class="line">In [<span class="number">23</span>]: a</div><div class="line">Out[<span class="number">23</span>]:</div><div class="line">tensor([[[<span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>],</div><div class="line">         [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">         [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]],</div><div class="line"></div><div class="line">        [[<span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>],</div><div class="line">         [<span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>],</div><div class="line">         [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]]])</div></pre></td></tr></table></figure></p><h3 id="小小总结一下-如果你有一个张量-想要批量取出-赋值-访问其中的最后某些维度的tensor-你可以用与这个张量前面剩余维度相同维度的onehot表示批量取出-⚠️onehot表示必须是dtype-torch-uint8"><a href="#小小总结一下-如果你有一个张量-想要批量取出-赋值-访问其中的最后某些维度的tensor-你可以用与这个张量前面剩余维度相同维度的onehot表示批量取出-⚠️onehot表示必须是dtype-torch-uint8" class="headerlink" title="小小总结一下:如果你有一个张量,想要批量取出/赋值/访问其中的最后某些维度的tensor,你可以用与这个张量前面剩余维度相同维度的onehot表示批量取出,⚠️onehot表示必须是dtype=torch.uint8"></a>小小总结一下:如果你有一个张量,想要批量取出/赋值/访问其中的最后某些维度的tensor,你可以用与这个张量前面剩余维度相同维度的onehot表示批量取出,⚠️onehot表示必须是dtype=torch.uint8</h3><p>如果不是torch.uint8会发生什么:这同样是一种批量取出的方法:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">24</span>]: idxs = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]])</div><div class="line"></div><div class="line">In [<span class="number">25</span>]: a[idxs]</div><div class="line">Out[<span class="number">25</span>]:</div><div class="line">tensor([[[[<span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>],</div><div class="line">          [<span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>],</div><div class="line">          [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]],</div><div class="line"></div><div class="line">         [[<span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>],</div><div class="line">          [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">          [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]],</div><div class="line"></div><div class="line">         [[<span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>],</div><div class="line">          [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">          [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]]],</div><div class="line"></div><div class="line"></div><div class="line">        [[[<span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>],</div><div class="line">          [<span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>],</div><div class="line">          [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]],</div><div class="line"></div><div class="line">         [[<span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>],</div><div class="line">          [<span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>],</div><div class="line">          [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]],</div><div class="line"></div><div class="line">         [[<span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>],</div><div class="line">          [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">          [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]]]])</div><div class="line"></div><div class="line">In [<span class="number">26</span>]: a[idxs].size()</div><div class="line">Out[<span class="number">26</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>])</div><div class="line"></div><div class="line">In [<span class="number">27</span>]: a[<span class="number">0</span>]</div><div class="line">Out[<span class="number">27</span>]:</div><div class="line">tensor([[<span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>],</div><div class="line">        [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">        [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]])</div><div class="line"></div><div class="line">In [<span class="number">28</span>]: a[<span class="number">1</span>]</div><div class="line">Out[<span class="number">28</span>]:</div><div class="line">tensor([[<span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>],</div><div class="line">        [<span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>],</div><div class="line">        [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]])</div></pre></td></tr></table></figure></p><p>我们看到,仅仅是去掉了torch.uint8,结果发生了非常大的变化.我们这里的idxs的含义不再是onehot表示,而变成了:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a[torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]])] ==</div><div class="line">[[a[<span class="number">1</span>],a[<span class="number">0</span>],a[<span class="number">0</span>]],[a[<span class="number">1</span>],a[<span class="number">1</span>],a[<span class="number">0</span>]]]</div></pre></td></tr></table></figure></p><p>这种也是一种批量取出的有效方法,但是我们就没有办法批量赋值啦.</p><p>希望能帮助到大家.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天来谈一谈pytorch中快速批量的赋值和选择的操作,这种方法非常简洁,而且比for循环便利i,j,k index的效率高上很多很多倍,并且可以利用gpu加速.用深度学习框架的时候,务必避免for循环出现.&lt;/p&gt;
&lt;p&gt;我们直接上代码:&lt;/p&gt;
&lt;p&gt;首先我们随机生成一
      
    
    </summary>
    
      <category term="Code" scheme="https://jeffchy.github.io/categories/Code/"/>
    
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="https://jeffchy.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>On the Dimensionality of Word Embedding - NeuralPS2018 简要note</title>
    <link href="https://jeffchy.github.io/2018/12/14/On-the-Dimensionality-of-Word-Embedding-NeuralPS2018-%E7%AE%80%E8%A6%81note/"/>
    <id>https://jeffchy.github.io/2018/12/14/On-the-Dimensionality-of-Word-Embedding-NeuralPS2018-简要note/</id>
    <published>2018-12-14T12:23:41.000Z</published>
    <updated>2018-12-14T15:20:38.001Z</updated>
    
    <content type="html"><![CDATA[<p>今天简单说一下关于word embedding的一篇论文的大意, <a href="https://arxiv.org/abs/1812.04224" target="_blank" rel="external">https://arxiv.org/abs/1812.04224</a> ,<br>‘On the Dimensionality of Word Embedding’,这是一篇被NeurIPS 2018接受的Oral Paper。文章说了一件核心的事情，通过引入”Pairwise Inner Product(PIP) loss”,我们如何去将word embedding的dimension的selection变成一个bias-variance trade-off的问题，使得我们能够更好地找到最好的dimension。</p><h1 id="Introduction-and-Background"><a href="#Introduction-and-Background" class="headerlink" title="Introduction and Background"></a>Introduction and Background</h1><h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>我们通常把Word Embedding的方法大致分为两种，一种是基于矩阵分解的方法(LSA)，一种是基于神经网络prediction的方法(google word2vec)<br>word embedding我不太想过多的介绍了，其中最有名的是2013年google的word2vec,我相信大家都熟知。这里提一下基于矩阵分解的方法，比如利用SVD分解PMI矩阵。</p><script type="math/tex; mode=display">PIM_{i,j} = log{\frac{p(v_i, v_j)}{p(v_i)p(v_j)}}</script><p>let $PMI = U \Sigma V^T$ (SVD分解)，一个k维度的embedding被证明可以这样产生，</p><script type="math/tex; mode=display">E = U_{1:k} D^{\alpha}_{1:k,1:k},\ \ \  \alpha \in [0,1 ], \text{the power}</script><p>其中$p(v_i)$代表这个单词在Count(w, context)表中出现的总次数，如下图所示：<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/431911.jpg" alt=""><br>有名的glove和Word2vec,比如(Skipgram)方法也被证明过是隐式的matrix factorization.</p><p>这方面如果想要了解更多的内容我推荐</p><ul><li><a href="http://web.stanford.edu/~jurafsky/li15/lec3.vector.pdf" target="_blank" rel="external">http://web.stanford.edu/~jurafsky/li15/lec3.vector.pdf</a> [Stanford 2015 LSA 311 Dan Jurafsky lecture slices 3][EN]</li><li><a href="http://www.shuang0420.com/2017/03/21/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/" target="_blank" rel="external">http://www.shuang0420.com/2017/03/21/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/</a> [真的很棒的中文博客][ZH]</li><li>当然还要相关论文</li></ul><h2 id="Word-Embedding-is-unitary-invariance"><a href="#Word-Embedding-is-unitary-invariance" class="headerlink" title="Word Embedding is unitary invariance"></a>Word Embedding is unitary invariance</h2><p>Unitary Invariance的意思是，一个Word的embedding都乘上一个unitary matrix（比如旋转矩阵）得到了一个新的embedding空间，那么他等于没有改变。（你学习到的embedding整个空间旋转一下他的相对位置以及包含的信息还是一样的不是嘛）<br>Unitary Matrix的定义也很简单：<br>$UU^T=U^TU=I$</p><h1 id="PIP-loss"><a href="#PIP-loss" class="headerlink" title="PIP loss"></a>PIP loss</h1><p>我们想要evaluate我们learned embedding是不是已经足够好，必须要有一个loss function，而他的input应该是一个trained的Embedding Matrix和一个’gold’ Embedding Matrix。所以作者提出了一个<strong>符合Word embedding Unitary Invariance Property</strong>的loss function，如图所示，function和PIP matrix非常简单：<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/49009649.jpg" alt=""><br>这个loss function表示了，两个Embedding空间中的向量们之间的相对位置信息的不同(relative position shifts),同时如果两个Embedding $E_2 = E_1 U$, $E_2 E_2^T = (E_1 U)(U^T E_1^T)=E_1 E_1^T$</p><h1 id="开始推导了"><a href="#开始推导了" class="headerlink" title="开始推导了"></a>开始推导了</h1><p>首先一个lemma吗，后面要用<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/69956709.jpg" alt=""><br>然后对于$\alpha=0$的special case，同时我们假设我们的有一个Oracle embedding （global 语言信息得到一个合理的sigmal matrix [e.g. PMI]）然后根据SVD计算出来的Embedding Matrix, 和一个trained的embedding (根据training set统计出来的Signal Matrix计算出来的Embedding)，同时这两个embedding所有的column是orthonormal的，（内积等于1），我们能能将他们的loss写成bias variance trade off的形式<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/4476990.jpg" alt=""><br>其中的$d-k$代表了近似产生的信息的丢失，会随着$k$的增大而变小，(bias)最后面一项会随着$k$的增加而变大，因为“the noise perturbs the subspace spanned by E”（Matrix Perturbation Theory）(variance)</p><h2 id="更加general的版本："><a href="#更加general的版本：" class="headerlink" title="更加general的版本："></a>更加general的版本：</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/3495160.jpg" alt=""></p><h1 id="如何选择dimension"><a href="#如何选择dimension" class="headerlink" title="如何选择dimension"></a>如何选择dimension</h1><p>作者用蒙特卡洛方法生成模拟signal matrix M以及noise matrix $\hat{M} = M + Z$,具体的方法是：其实我还是不太明白具体的方法….<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/63678809.jpg" alt=""></p><h1 id="一些实验结果"><a href="#一些实验结果" class="headerlink" title="一些实验结果"></a>一些实验结果</h1><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/30624435.jpg" alt=""></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这篇文章感觉还是很牛逼的。。。毕竟是NeurIPS的oral，基本上就是，作者用了个合理的loss建模了什么是好的word embedding，同时在合理的设定下这个loss写成了bias-variance trade off 的形式，它表示出来的bias/variance项也intuitively make sense.这可以说给了一个很不错的理论框架。作者还通过蒙特卡洛方法去估计了不同的方法的最优embedding dimension，总的来说是特别有意义的工作。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天简单说一下关于word embedding的一篇论文的大意, &lt;a href=&quot;https://arxiv.org/abs/1812.04224&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1812.042
      
    
    </summary>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/Paper/"/>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/Paper/NLP/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/Paper/NLP/Note/"/>
    
    
      <category term="Probability Theory" scheme="https://jeffchy.github.io/tags/Probability-Theory/"/>
    
      <category term="Algorithm" scheme="https://jeffchy.github.io/tags/Algorithm/"/>
    
      <category term="Natual Language Processing" scheme="https://jeffchy.github.io/tags/Natual-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>Using Self-organizing map on 1-D data input</title>
    <link href="https://jeffchy.github.io/2018/11/28/Using-Self-organiziing-map-on-1-D-data-input/"/>
    <id>https://jeffchy.github.io/2018/11/28/Using-Self-organiziing-map-on-1-D-data-input/</id>
    <published>2018-11-28T06:57:48.000Z</published>
    <updated>2018-11-30T08:40:54.752Z</updated>
    
    <content type="html"><![CDATA[<p>自组织映射是一种有效的无监督学习的方法，<a href="https://en.wikipedia.org/wiki/Self-organizing_map，" target="_blank" rel="external">https://en.wikipedia.org/wiki/Self-organizing_map，</a> 利用竞争式得学习方法来将（通常是）高维数据映射到低维数据上，可以看成是一种dimension reduction，也可以具体用在聚类任务上。</p><p>举个例子，我们将有三个feature的数据$x^i = [x_1, x_2, x_3]$，训练出一个有着$k$个神经元的SOM，着每</p><p>个$Neuron_k$可以认为是数据的prototype。SOM的好处是他学出的表示能够保留数据的拓扑结构,(preserve the topology of original data),如图所示, SOM(grid)被慢慢拉近到贴合到数据的分布上。<img src="http://r.photo.store.qq.com/psb?/V13VpI7R48odcs/ShG2lMS2nEd2mA*UnY4ERgA9kmWIBPbSq.ZiS*aKOSA!/r/dPMAAAAAAAAA" alt="SOM"></p><p>具体的算法这里不多介绍了，推荐的资源：</p><p>[ZH] <a href="https://www.cnblogs.com/surfzjy/p/7944454.html" target="_blank" rel="external">https://www.cnblogs.com/surfzjy/p/7944454.html</a></p><p>[EN] <a href="https://en.wikipedia.org/wiki/Self-organizing_map" target="_blank" rel="external">https://en.wikipedia.org/wiki/Self-organizing_map</a></p><p>这里主要介绍一个小的例子：找出最具代表性的“数字”。</p><p>我们使用开源的Python Package: minisom <a href="https://github.com/JustGlowing/minisom" target="_blank" rel="external">https://github.com/JustGlowing/minisom</a> ， 首先安装这个package。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install minisom</div></pre></td></tr></table></figure><p>我们这里的输入时N个数字，我们希望能找到最具代表性的k个数字prototype。</p><p>我们的input是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">data = [[ <span class="number">1</span>],</div><div class="line">        [ <span class="number">2</span>],</div><div class="line">        [ <span class="number">3</span>],</div><div class="line">        [ <span class="number">14</span>],</div><div class="line">        [ <span class="number">1114</span>],</div><div class="line">        [ <span class="number">3</span>],</div><div class="line">        [ <span class="number">33</span>],</div><div class="line">        [ <span class="number">0.77</span>],</div><div class="line">        [ <span class="number">0.07</span>],</div><div class="line">        [<span class="number">-1</span>],</div><div class="line">        [<span class="number">-1000000</span>]]</div></pre></td></tr></table></figure><p>由于minisom这个包讲所有的输入数据做了一次normalize，还是按照行做的，所以如果我们的数据是一维的话，我们所有的数据都会被normalize成1，所以我们不能直接使用原来minisom，我们需要将【所有】如下normalize的代码都注释掉。(和导师聊过，我们认为这里的normalize在我们这个问题上是不需要的<a href="https://stackoverflow.com/questions/13687256/is-it-right-to-normalize-data-and-or-weight-vectors-in-a-som)，我么可以直接copy源代码的minisom类，然后将所有normalize的行都注释掉：）" target="_blank" rel="external">https://stackoverflow.com/questions/13687256/is-it-right-to-normalize-data-and-or-weight-vectors-in-a-som)，我么可以直接copy源代码的minisom类，然后将所有normalize的行都注释掉：）</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self._weights[i, j] = self._weights[i, j] / norm</div></pre></td></tr></table></figure><p>接下来我们就可以操作了,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> som <span class="keyword">import</span> MiniSom <span class="keyword">as</span> SOM</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> sys</div></pre></td></tr></table></figure><p>我们构建我们的SOM，我们选择使用一维排布的神经元，5x1。这里其实二维的也是ok的，这个depends on我们输入数据的形式，对于这个问题，我们的数据都在数轴上，那么其实一维的神经元排布更加能反应原本的拓扑结构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">som = SOM(<span class="number">5</span>, <span class="number">1</span>, <span class="number">1</span>, sigma=<span class="number">0.03</span>, learning_rate=<span class="number">0.3</span>) <span class="comment"># initialization of 6x6 SOM</span></div><div class="line">print(<span class="string">"Training..."</span>)</div><div class="line"><span class="comment"># som.random_weights_init(data)</span></div><div class="line">som.train_random(data, <span class="number">1000</span>) <span class="comment"># trains the SOM with 100 iterations</span></div><div class="line">print(<span class="string">"...ready!"</span>)</div></pre></td></tr></table></figure><p>然后很快我们就能得到我们SOM的结果了,我们的数据被分成了5个类别，大家看基本是按照量级区分开了，同时我们weights vector就是我们最具有代表性的几个数字！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">42</span>]:</div><div class="line">som.win_map(data)</div><div class="line">Out[<span class="number">42</span>]:</div><div class="line">defaultdict(list,</div><div class="line">            &#123;(<span class="number">0</span>, <span class="number">0</span>): [[<span class="number">-1000000</span>]],</div><div class="line">             (<span class="number">1</span>, <span class="number">0</span>): [[<span class="number">1114</span>]],</div><div class="line">             (<span class="number">2</span>, <span class="number">0</span>): [[<span class="number">33</span>]],</div><div class="line">             (<span class="number">3</span>, <span class="number">0</span>): [[<span class="number">14</span>]],</div><div class="line">             (<span class="number">4</span>, <span class="number">0</span>): [[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>], [<span class="number">3</span>], [<span class="number">0.77</span>], [<span class="number">0.07</span>], [<span class="number">-1</span>]]&#125;)</div><div class="line">In [<span class="number">43</span>]:</div><div class="line">som.get_weights()</div><div class="line">Out[<span class="number">43</span>]:</div><div class="line">array([[[<span class="number">-9.99999897e+05</span>]],</div><div class="line"></div><div class="line">       [[ <span class="number">1.11399996e+03</span>]],</div><div class="line"></div><div class="line">       [[ <span class="number">3.29995908e+01</span>]],</div><div class="line"></div><div class="line">       [[ <span class="number">1.39999492e+01</span>]],</div><div class="line"></div><div class="line">       [[ <span class="number">1.22937802e+00</span>]]])</div></pre></td></tr></table></figure><p>对于更多的例子大家可以看minisom的example。</p><p>这次blog主要是想传达，</p><ol><li>我们不需要每次都normalize我们SOM的weights。</li><li>1D的数据作为SOM的输入也是make sense的。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;自组织映射是一种有效的无监督学习的方法，&lt;a href=&quot;https://en.wikipedia.org/wiki/Self-organizing_map，&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://en.wikipedia.org/
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="ML" scheme="https://jeffchy.github.io/categories/NLP/ML/"/>
    
      <category term="Code" scheme="https://jeffchy.github.io/categories/NLP/ML/Code/"/>
    
    
      <category term="Algorithm" scheme="https://jeffchy.github.io/tags/Algorithm/"/>
    
      <category term="Natual Language Processing" scheme="https://jeffchy.github.io/tags/Natual-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>take-away tips from a Deep Learning Midterm Project</title>
    <link href="https://jeffchy.github.io/2018/11/13/take-away-tips-from-a-Deep-Learning-Midterm-Project/"/>
    <id>https://jeffchy.github.io/2018/11/13/take-away-tips-from-a-Deep-Learning-Midterm-Project/</id>
    <published>2018-11-13T06:27:54.000Z</published>
    <updated>2018-11-13T06:53:08.299Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要说一下深度学习其中项目的一些tips.深度学习课的期中作业是用pytorch复现一篇比较简单的论文,网络很简单:<a href="https://arxiv.org/abs/1412.6806" target="_blank" rel="external">https://arxiv.org/abs/1412.6806</a> ALLCNN,我的代码暂时还不能开源,accuracy还差一些,而且作业ddl还没有到,主要说一些训练期间感觉到的小tips</p><h2 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h2><ul><li>Adam的效果不一定是最好的, sgd + momentum + milestones + learning rate decay的灵活性和效果可能更好. </li><li>Adam的weight decay和sgd中的weight decay,在现有的框架实现中是不等效的,Adam可能会弱化weight decay regularization的效果</li><li>我们通常说, validation accuracy 和 test accuracy 是两回事,通常validation accuracy会偏高一些. 但是<strong>有的时候validation accuracy</strong>也并不能完全展现你的模型能力,举一个例子: cifar10上我一开始的train:validation split是9:1, 45000张图片进行训练, 5000张validate, 10000张test. 这样我训练的过程中 validation accuracy无论怎么调参都有些偏低. 除了参数可能还不够好以外,非常容易被忽略的原因是,我拿了十分之一的训练数据去做了validation,我训练数据变少了,我的模型accuracy自然会变差.事实证明,我只用1000张图片做validation,我的acc明面上提高了2-3个点.事实上,我们最后会重新拿所有的数据做一次训练.但是在之前训练的过程中,看到validation accuracy偏低的时候不要完全灰心,这部分数据如果也拿来训练的话也许模型效果会更好一些.</li><li>Initialization matters.我初始化conv layer的weights使用了Xavier初始化方法,一切都比较正常,而我有的同学没有用,他们的网络的loss很难下降,会卡住.之后我做实验的时候又试了一下据说对relu很友好的kaiming initialization,效果却差强人意,原因我没有细究,欢迎讨论.Initialization真的非常重要,和learning rate一样,有莫大的影响</li><li>关于设备. 我使用的是自己的1080 GPU,我同学使用的是集群K80,我的速度却比他快很多很多…在batch size比较小(显存占用比较小),吞吐量比较高的情况下,emmm不要盲目迷信GPU的价格,就像Cifar10的数据集,你完全没有必要用二十多GB的显存的K80…(我也不知道为啥我比他快那么多)</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章主要说一下深度学习其中项目的一些tips.深度学习课的期中作业是用pytorch复现一篇比较简单的论文,网络很简单:&lt;a href=&quot;https://arxiv.org/abs/1412.6806&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ht
      
    
    </summary>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/Note/"/>
    
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="https://jeffchy.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Marrying Up Regular Expression with Neural Network</title>
    <link href="https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/"/>
    <id>https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/</id>
    <published>2018-10-26T09:15:27.000Z</published>
    <updated>2019-01-17T13:36:22.296Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个简洁的论文笔记,关于这篇文章:发表于ACL18<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/47023549.jpg" alt=""></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>作者探讨了如何用正则表达式,提神神经网络模型的效果.</p><h1 id="tasks-Speaking-Language-Understanding"><a href="#tasks-Speaking-Language-Understanding" class="headerlink" title="tasks Speaking Language Understanding"></a>tasks Speaking Language Understanding</h1><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/72024242.jpg" alt=""></p><ul><li>一种是意图识别, intent classification, 其实就是一种text classification</li><li>还有一种是插槽, 其实就是一种sequence labeling任务</li></ul><h1 id="Base-Model"><a href="#Base-Model" class="headerlink" title="Base Model"></a>Base Model</h1><h2 id="Intent-classification"><a href="#Intent-classification" class="headerlink" title="Intent classification"></a>Intent classification</h2><p>作者使用BiLSTM+self-attention得到句子表示去当baseline.<br>self attention就是query和每个key进行相似度计算得到attention权重，然后用对应的hidden state然后接一个softmax去预测label.$W$是一个权重矩阵,$c$是一个可以训练的信息向量,用于分类的信息词,我的理解就是Q=c, K,V都等于hidden state, W是一个weight matrix<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/16459427.jpg" alt=""></p><h2 id="sequence-labeling"><a href="#sequence-labeling" class="headerlink" title="sequence labeling"></a>sequence labeling</h2><p>作者直接使用BiLSTM过一遍整个句子.</p><h1 id="如何apply-RE-我们主要拿intent-classification举例子"><a href="#如何apply-RE-我们主要拿intent-classification举例子" class="headerlink" title="如何apply RE 我们主要拿intent classification举例子"></a>如何apply RE 我们主要拿intent classification举例子</h1><p>对于Intent classification的任务:<br>举个具体例子,一句话</p><h2 id="Input-level-toy-example"><a href="#Input-level-toy-example" class="headerlink" title="Input level toy example"></a>Input level toy example</h2><p>比如数据中的label有: flights, airline<br>我们对这两个label分别设立他们的正则表达式,意思是,如果一个句子满足这个正则表达式,我们就认为这个句子应该属于这个label.<br>airline: RE: /list(the)?__AIRLINE/<br>flights: RE: /list(\w+){0,3} flights?/</p><p>现在我们碰到了一个句子:</p><blockquote><p>sentence: list the Delta airlines flights to Miami<br>label: flights</p></blockquote><p>我们会先对上面两个airline, flights的RE跑一下,我们发现都匹配了!<br>所以我们给他们打上了airline, flights两个REtags,我们对所有的REtags有一个可以训练的embedding,所以我们把两个tag做一下平均,得到一个aggregate embedding,就是我们symbolic rules产生的额外信息,于是我们可以把这个向量加入到它最后的soft Max表示那里,做预测.<br>如图中的(1)<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/18645714.jpg" alt=""><br>作者之前还试过我们直接把这个embedding和每个词的embedding结合起来,结果发现,这些tag会重复很多次,所以最终performance不好.</p><h2 id="NN-module-Network-level"><a href="#NN-module-Network-level" class="headerlink" title="NN module Network level"></a>NN module Network level</h2><p>在神经网络的层面,我们可以利用我们得到的tags来指导attention.<br>attention的式子如图所示<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/48204053.jpg" alt=""><br>这里面$h_i$就是BiLSTM对每个word的隐藏层.alpha是每个位置的attention weights, c是一个可以训练的,用于分类的信息词.<br>self attention就是query和每个key进行相似度计算得到attention权重，然后用对应的hidden state然后接一个softmax去预测label.W是一个权重矩阵,c是一个可以训练的信息向量,用于分类的信息词,我的理解就是Q=c, K,V都等于hidden state, W是一个weight matrix</p><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/61121072.jpg" alt=""><br>直觉上来说,我们对一个句子针对flights这个label的规则成功匹配了flights from这个表达式,那么也就意味着(flights from)这两个词应该对最终的模型预测结果有更多的贡献, 为了区别不同的intents, 我们给每个intent label k一个不同的attention, 于是我们可以得到k个s矩阵,</p><ul><li>然后我们可以分别计算每个s的一个分数,组成logits,然后用一个softmax分类器去得到不同的intent label的概率,这个叫做aggregate attention<br>(我的理解是原先softmax是把s矩阵的维度映射成k维,现在相当于有k个s矩阵,然后都有一个向量w映射成一维度的,最后组成k维)</li><li>然后我们加上一个attention loss去指导这个attention,其中$t_{ki}$为0,如果没有matching的RE,等于$\frac{1}{l_k}$如果matching word的长度是lk.</li><li>我们会发现,然后我们利用这个loss去指导attention weights和推理出的matching word保持某一种一致性(我的理解是,越大的attention weights, 这个式子就会越接近0,其实我觉得应该有个负号)<br>然后我们把这个loss和之前的classification loss加起来就行了<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/22919727.jpg" alt=""><h2 id="Output-level"><a href="#Output-level" class="headerlink" title="Output level"></a>Output level</h2><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/91104759.jpg" alt=""><br>同样规则可以在输出层影响NN.<br>之前我们aggregate attention计算logic的结果可以进一步被规则指导.<br>这里$w$是trainable的, $z_k$是一个indicator,1代表有matching,0代表没有,$w_k$是trainable的weights,直接在logits这一层也可以被rules影响.直觉上来说,如果有被matching,那么我就提高这一类别的分数(为什么不直接在softmax之后成为概率之后操作?也是可以,但是效果不好,因为logits是unconstrained real value)</li></ul><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>文章用了这个数据集,发现在小的数据结果下,效果是相当不错的,超过了baseline不少<br>(two就是它除了我们刚才说的attention以外又加上了negative的attention,我们刚才是希望匹配到的词鼓励attention weights, 可能也会有某些规则不鼓励 attention weights.)<br>(+pos/neg就是加了negative loss和positive loss)<br>mem是另一个模型.<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/50843081.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这是一个简洁的论文笔记,关于这篇文章:发表于ACL18&lt;br&gt;&lt;img src=&quot;http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/47023549.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Introd
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>深度好奇-神经规则引擎: Neural Rule Engine</title>
    <link href="https://jeffchy.github.io/2018/10/26/%E6%B7%B1%E5%BA%A6%E5%A5%BD%E5%A5%87-%E7%A5%9E%E7%BB%8F%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E-Neural-Rule-Engine/"/>
    <id>https://jeffchy.github.io/2018/10/26/深度好奇-神经规则引擎-Neural-Rule-Engine/</id>
    <published>2018-10-26T06:12:28.000Z</published>
    <updated>2018-10-27T16:54:22.800Z</updated>
    
    <content type="html"><![CDATA[<p>今天继续读一下深度好奇的系列工作: 神经规则引擎 Neural Rule Engine, 文章链接: <a href="https://arxiv.org/abs/1808.10326" target="_blank" rel="external">https://arxiv.org/abs/1808.10326</a><br>GENERALIZE SYMBOLIC KNOWLEDGE WITH NEURAL RULE ENGINE<br>之前的如果大家看了OONP或者微信公众号的话,也许能够先get到深度好奇工作的一些中心思想,可以看一下我之前的一篇博客 <a href="https://jeffchy.github.io/2018/10/24/OONP-Object-oriented-neural-programming/">https://jeffchy.github.io/2018/10/24/OONP-Object-oriented-neural-programming/</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>这篇文章的想法是,希望将symbolic rule 和 neural network 的优点结合起来.<br>(Symbolic rule -&gt; neural network ) = Better Performance<br>现有的很多工作是将符号的规则融入到神经网络中,但是这篇文章的主要思想是:</p><blockquote><p>learn knowledge explicitly from logic rules and then generalize them implicitly with neural networks</p></blockquote><p><strong>将符号推理学会的知识,用神经网络使得他们有更好地泛化到任务上</strong></p><p>NRE使用了Neural Module Network实现,每个模块表示了一种逻辑规则的实施.<br>NRE可以极大地提升召回率</p><h1 id="一些背景-主要是我挑选总结"><a href="#一些背景-主要是我挑选总结" class="headerlink" title="一些背景(主要是我挑选总结)"></a>一些背景(主要是我挑选总结)</h1><p>符号与神经网络的结合有很多很多种方式,我简单看了几个模型,希望能给大家一些intuition上的warm up.</p><h2 id="Teacher-teach-logic-to-student"><a href="#Teacher-teach-logic-to-student" class="headerlink" title="Teacher teach logic to student"></a>Teacher teach logic to student</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/28236079.jpg" alt=""><br>一种方法,用一个teacher student的framework,将teacher模型学到的逻辑迁移到学生身上.<br>每次迭代将student network投影到一个被规则限制了的子空间上,优化得到teacher network (soft FOL+PR) Student network在模仿teacher network还是去预测正确的label之间做权衡,最终训练得到student network</p><h2 id="OONP-symbolic-computed-result-as-input-of-NN-after-embedding"><a href="#OONP-symbolic-computed-result-as-input-of-NN-after-embedding" class="headerlink" title="OONP: symbolic computed result as input of NN (after embedding)"></a>OONP: symbolic computed result as input of NN (after embedding)</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/79743020.jpg" alt=""><br>这个是之前我们介绍的OONP,他构造了一个非常复杂的规则和神经网络结合的系统,大家应该还记得,我们拿其中一个部分举例子:<br>我们知道它的object memory里面除了这种symbolic的符号化的本体图表示以外,还会同时有向量化的表示.<br>当我们顺序地去读文本的时候,我们是会根据符号的逻辑来决定使用什么action的,比如生成一个叫做Name属性是Jeff的人类object,同时我们也会用他附近的上下文去生成一个向量表示,这个就相当于经过符号逻辑计算以后的输出,经过嵌入成为了神经网络的输入.<br>这也是一个例子</p><h2 id="Marrying-up-regular-expressions-with-NN"><a href="#Marrying-up-regular-expressions-with-NN" class="headerlink" title="Marrying up regular expressions with NN"></a>Marrying up regular expressions with NN</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/47023549.jpg" alt=""><br>这个工作是用正则表达式去提升神经网络:),正则表达式其实就是一种规则,正则的matching就是规则的matching. 利用规则,我们可以去计算出额外的信息,嵌入网络作为输入,也可以做用在网络一些模块中,也可以在网络的输出层做手脚,详细可以看我的另外一篇笔记: <a href="https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/">https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/</a> 这篇工作也是ACL18上的</p><h2 id="暂时性总结一下"><a href="#暂时性总结一下" class="headerlink" title="暂时性总结一下"></a>暂时性总结一下</h2><p>我们可以看到,上面列举的很多工作都是: </p><ul><li>利用rule计算出来的结果去提升神经网络的结果.<br>比如,用符号运算计算出一些额外信息,然后嵌入之后输入神经网络,比如刚才说的正则表达式的工作,还有OONP的一个部分</li><li>或者用这种结果去影响神经网络的一些模块,比如attention</li><li>或者用规则去修改神经网络输出层的结果.<ul><li>之前Johnson的文章也是一种直接用规则去修改神经网络输出结果的方法,实际上任意的正则表达式都可以表示为一种有限状态机FSM</li><li><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/36579701.jpg" alt=""></li></ul></li><li>或者是Teacher student的framework<br>本质都是 <strong>利用制定规则去得到一些信息,目的是提升神经网络的效果</strong></li></ul><h1 id="正篇Neural-Rule-Engine"><a href="#正篇Neural-Rule-Engine" class="headerlink" title="正篇Neural Rule Engine"></a>正篇Neural Rule Engine</h1><p>反其道而行之,它希望用神经网络来提高规则的泛化能力</p><h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>神经规则引擎把规则(正则表达式RE)转化成有符号化结构的模块神经网络,具体的步骤主要是两步</p><ul><li>我们会定义很多action, 这些actions都是由neural/symbolic的模块来实现,每个action都是某一个算法或者模型,比如说是一个神经网络,或者是一个symbolic的算法</li><li>我们会将正则表达式 Parse成一个action tree, 具体的Parse可以用Symbolic的或者是神经网络的parser来预测, 可以理解为我们定义了很多模块,然后我们解析规则去把他们组织起来</li></ul><p>总体来说是这样一个框架</p><h2 id="RE"><a href="#RE" class="headerlink" title="RE"></a>RE</h2><p>我们看一下它的正则表达式,其实和前面的那篇文章是一模一样的框架<br><a href="https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/">https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/</a><br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/50443767.jpg" alt=""><br>比如说,我们要做classification的任务(上半部分):<br>某一个label是[尾随作案]<br>我们定义positive和negative的正则表达式,比如所如果匹配到跟在谁谁后面这种,一般会倾向认为是认为是尾随作案,但是如果事主也在跟随的话那就不是.</p><h2 id="Actions和action-tree"><a href="#Actions和action-tree" class="headerlink" title="Actions和action tree"></a>Actions和action tree</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/58540341.jpg" alt=""><br>这是他主要的action,他们认为这6种action就能表示绝大部分他们用正则表达式定义的规则.比如下面的例子:<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-28/11620217.jpg" alt=""></p><h2 id="Action-tree的表示-Reverse-Polish-Notation"><a href="#Action-tree的表示-Reverse-Polish-Notation" class="headerlink" title="Action tree的表示 Reverse Polish Notation"></a>Action tree的表示 Reverse Polish Notation</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-28/68006179.jpg" alt=""><br>Reverse Polish Notation(RPN)其实就是后序遍历:)</p><h1 id="用Seq2seq-rule-parser"><a href="#用Seq2seq-rule-parser" class="headerlink" title="用Seq2seq rule parser"></a>用Seq2seq rule parser</h1><p>了解了我们的rules还有action是什么以后,我们来看我们这个框架的第一部分,rule parser, 是一个seq2seq的结构:), 我们将我们的规则输入进模型中,然后用3层BiLSTM+1层LSTM做encoder,然后做两个阶段的decoder,先decoder actions,然后encode这些actions的类别,decode出这些action传入的参数.这样子做的原因是他们实验发现同时predict action和他们的参数太困难了,很难训练,如是采取了这样层级的结构<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-28/46786537.jpg" alt=""><br>然后优化目标是cross entropy loss, 他们还用了一个trick去加速训练,因为word vectors是fix的,所以他们不是去预测正确的word的id,而是让模型直接去预测正确的word vector<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-28/43490760.jpg" alt=""></p><h2 id="他们的baseline"><a href="#他们的baseline" class="headerlink" title="他们的baseline"></a>他们的baseline</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-28/98245107.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天继续读一下深度好奇的系列工作: 神经规则引擎 Neural Rule Engine, 文章链接: &lt;a href=&quot;https://arxiv.org/abs/1808.10326&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxi
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Natual Language Processing" scheme="https://jeffchy.github.io/tags/Natual-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>OONP-Object oriented neural programming</title>
    <link href="https://jeffchy.github.io/2018/10/24/OONP-Object-oriented-neural-programming/"/>
    <id>https://jeffchy.github.io/2018/10/24/OONP-Object-oriented-neural-programming/</id>
    <published>2018-10-24T13:38:01.000Z</published>
    <updated>2018-10-25T14:32:01.730Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Object-Oriented-Neural-Programming"><a href="#Object-Oriented-Neural-Programming" class="headerlink" title="Object Oriented Neural Programming"></a>Object Oriented Neural Programming</h1><p>这一篇是深度好奇 <a href="http://deeplycurious.ai/#/" target="_blank" rel="external">http://deeplycurious.ai/#/</a> 发表在ACL18上的工作, 意在将符号逻辑(Symbolism)和深度学习神经网络(connectionism)结合起来,利用它们各自的优点,解决自然语言理解中遇到的困呐:</p><ul><li>长距离的逻辑依赖难以有效学习到</li><li>自然语言的模糊性</li><li>对常识(common sense)的依赖</li><li>语义表示的形式(我们如何用一个结构很好的表示语义(semantic parsing))<br>[文章前半部分参考深度好奇微信公众号文章<a href="https://mp.weixin.qq.com/s/MqcL0272MVA0aXKy4nQTxQ" target="_blank" rel="external">https://mp.weixin.qq.com/s/MqcL0272MVA0aXKy4nQTxQ</a>]</li></ul><h1 id="神经网络和符号主义的优劣势"><a href="#神经网络和符号主义的优劣势" class="headerlink" title="神经网络和符号主义的优劣势"></a>神经网络和符号主义的优劣势</h1><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-24/64258485.jpg" alt=""></p><h1 id="如何将神经系统和符号只能进行融合"><a href="#如何将神经系统和符号只能进行融合" class="headerlink" title="如何将神经系统和符号只能进行融合"></a>如何将神经系统和符号只能进行融合</h1><ul><li><p>原则I 生成神经和符号的链接<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-24/57721035.jpg" alt=""><br>神经网络最终会输出离散的符号表示,经过符号推理得到的符号嵌入作为输入,进一步指导神经网络的下一步操作.</p></li><li><p>原则II 形成神经符号的并列和对应<br>并列是指两条符号和神经网络的计算通路,之间有着密切的信息交换,鼓励两条通路的一致性进行训练.<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-24/61048772.jpg" alt=""><br>对应是指离散的规则表示和参数知识可以互相转化,利用各自的优势来选择神经还是符号.(Neural Rule Engine)<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-24/38523962.jpg" alt=""></p></li><li><p>原则III 中央调控机制去选择控制、规划,来确定使用 神经、符号、神经+符号<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-24/92410082.jpg" alt=""></p></li></ul><h1 id="OONP-利用上述中心思想的-面向自然语言理解问题的新框架"><a href="#OONP-利用上述中心思想的-面向自然语言理解问题的新框架" class="headerlink" title="OONP 利用上述中心思想的,面向自然语言理解问题的新框架."></a>OONP 利用上述中心思想的,面向自然语言理解问题的新框架.</h1><ul><li>OONP的目的训练一个semantic的parser使得: document-&gt;predesigned data structure </li><li>可以用监督学习、强化学习、或者两种的融合去做</li><li>一个例子OONP运用了面向对象的思路,每个方框是一个类别的instance,每个颜色代表了一个类别,对象间同时存在一些link,来表示他们之间的关系,这里又有点像关系型数据库,这些关系(relation),class,class_attribute,都是我们事先根据任务设置好的</li><li><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/61028456.jpg" alt=""></li></ul><h1 id="Details-of-OONP"><a href="#Details-of-OONP" class="headerlink" title="Details of OONP"></a>Details of OONP</h1><h2 id="OONP-Overview"><a href="#OONP-Overview" class="headerlink" title="OONP Overview"></a>OONP Overview</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/88730549.jpg" alt=""><br>D表示词语的distributed表示(word vectors), S表示逻辑symbol的表示,暂时没有给例子,我觉得OONP和神经图灵机很相似,都是模拟某种读取、处理(计算)然后得到输出的方法.<a href="https://www.jiqizhixin.com/articles/2017-04-11-7" target="_blank" rel="external">https://www.jiqizhixin.com/articles/2017-04-11-7</a><br>OONP主要分为三个部分:</p><ul><li>Inline Memory 原始文本经过一些预处理的步骤扔进Inline Memory,就类似于一个处理的buffer,不带有什么结构信息,就是一个序列</li><li>Reader 读取器从头到尾去读取|写入这个inline memory,有可能不止读一次,通过交互去利用inline memory的信息去更新(读取|写入)Carry-On Memory的信息.<ul><li>Reader相当于OONP的控制中心, 包括一个Neural Net Controller(包含一个policy net),这里的控制系统其实和神经图灵机NTM的控制器有一些相似,决定是否写入读取Matrix Memory,inline memory,同时, Policy Net可以给出当前时间合适的action,用来更新object memory的结构,也可以更新inline memory,最终会加入到action history中,后面会举具体的例子, </li><li>同时reader中间还有三个symbolic processors,分别是symbolic matching, symb olic reasoner, symbolic analyzer, 可以利用Object Memory, inline memory, action history, policy net中symbolic的表示来计算,我们之后得看看具体的例子.</li><li><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/78964408.jpg" alt=""></li></ul></li><li>Carry On Memory - 这个memory是一个reader从inline memory中读取到的信息的聚合以及记忆总结,包括了:<ul><li>Object Memory, 这个表示是distributed representation或者是symbolic的,后面会详细说,他保存了本体图的带有结构化的信息,可以理解为就是本体图的结构表示</li><li>Matrix Memory, 是differentiable的,一个例子就是RNN的hidden state,他表示的是知识是不确定的,不完整的(需要后续知识的)</li><li>Action History, 记录了所有parsing过程中的Action,这个是symbolic的,举个例子比如说:First Order Logic</li></ul></li></ul><h2 id="Details-of-Object-Momory"><a href="#Details-of-Object-Momory" class="headerlink" title="Details of Object Momory"></a>Details of Object Momory</h2><h3 id="Symbolic-part-Ontology-class-object-class-attributes-relations"><a href="#Symbolic-part-Ontology-class-object-class-attributes-relations" class="headerlink" title="Symbolic part: Ontology (class+object+class attributes+relations)"></a>Symbolic part: Ontology (class+object+class attributes+relations)</h3><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/72257414.jpg" alt=""></p><h3 id="Distributed-Representation-object-embedding"><a href="#Distributed-Representation-object-embedding" class="headerlink" title="Distributed Representation: object embedding"></a>Distributed Representation: object embedding</h3><p>对于每一个对象,除了一个符号化的图表示以外,还有一个对应的分布表示,目的是为了能够与reader中从matrix memory以及Inline memnory中的分布表示进行交互.举个例子,对于一句非常简单的话:</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><blockquote><p>Jeff is handsome, he has a cute cat named MaoMao </p></blockquote><p>我们建立了两个object: 一个是Jeff,一个是MaoMao,他们有各自的属性,分数PERSON类和ANIMAL类,他们之间的关系是Pet of.所以我们对他们有一个框框表示,也就是symbolic的表示<br>除此之外,我们会也会保存一个对应的分布表示,比如说对于Object: Jeff,来说,他的信息以及关系的建立与“Jeff is handsome, he has a cute cat named MaoMao”这整个句子有关,所以我们举个例子,把这个句子的单词换成word vectors然后过了个LSTM获得隐藏表示$h_t$我们把这个作为他的distributed representation,也就是所谓的object embedding!</p><h3 id="整个Object-Memory就由所有Object的一个Symbolic-Graph-Representation-和每个object对应的-distributed-object-embedding-表示"><a href="#整个Object-Memory就由所有Object的一个Symbolic-Graph-Representation-和每个object对应的-distributed-object-embedding-表示" class="headerlink" title="整个Object Memory就由所有Object的一个Symbolic Graph Representation 和每个object对应的 distributed object embedding 表示"></a>整个Object Memory就由所有Object的一个Symbolic Graph Representation 和每个object对应的 distributed object embedding 表示</h3><h3 id="Parsing出ontology的两种模式"><a href="#Parsing出ontology的两种模式" class="headerlink" title="Parsing出ontology的两种模式"></a>Parsing出ontology的两种模式</h3><ul><li>stationary: 现在你parsing出来的本体图一定会是最终本体图的一个部分,你只是暂时缺少信息而已</li><li>dynamic: 现有的parsing出来的本体图可能会最终被改变<br>这两种模式的选择同时还依赖着本体ontology的定义.比如这句话,如果Person:Tom和Item:Car之间的关系是sell,那么就是stationary,如果是Ownership那么就是dynamic<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/52987697.jpg" alt=""></li></ul><h2 id="Details-of-Inline-Memory"><a href="#Details-of-Inline-Memory" class="headerlink" title="Details of Inline Memory"></a>Details of Inline Memory</h2><p>之前我们看到,Inline Memory就是一个buffer,内涵需要被处理的序列,可能包含了Distributed和Symbolic的表示,下面举一些例子,Inline Memory里面是什么.</p><ul><li>Distributed: Context Dependent Word Embeddings, Hidden state of NNs</li><li>Symbolic: 通过Sequence Labeling等方法,识别、归纳出的符号表示. 比如: Apple:fruit<br>Parsing的时候Reader也是可以去写入Inline Memory的,我们把这个过程叫做”Notes-taking”,如果是distributed的话,这个就等价于interactive attention,如果reader返回的信号是离散的,symbolic的,我们可以理解为输出了一个action动作.</li></ul><h2 id="Details-of-Reader"><a href="#Details-of-Reader" class="headerlink" title="Details of Reader"></a>Details of Reader</h2><p>Reasoner的结构我们之前说过了,我复制一波</p><ul><li>Reader 读取器从头到尾去读取|写入这个inline memory,有可能不止读一次,通过交互去利用inline memory的信息去更新(读取|写入)Carry-On Memory的信息.<ul><li>Reader相当于OONP的控制中心, 包括一个Neural Net Controller(包含一个policy net),这里的控制系统其实和神经图灵机NTM的控制器有一些相似,决定是否写入读取Matrix Memory,inline memory,同时, Policy Net可以给出当前时间合适的action,用来更新object memory的结构,也可以更新inline memory,最终会加入到action history中,后面会举具体的例子, </li><li>同时reader中间还有三个symbolic processors,分别是symbolic matching, symb olic reasoner, symbolic analyzer, 可以利用Object Memory, inline memory, action history, policy net中symbolic的表示来计算,我们之后得看看具体的例子.</li><li><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/78964408.jpg" alt=""></li></ul></li></ul><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/4775618.jpg" alt=""><br>对于t时刻来说reasoner的运作的步骤:</p><ul><li>Step1: Input: Action History -&gt; Symbolic Analyzer -&gt; 获得能指导后续action选择的特征.</li><li>Step2: $s_t=$NeuralNetController.get(MatrixMemory) 控制器从Matrix Memory中获得向量表示</li><li>Step3: $x_t^{(s)}=$NeuralNetController.get(InlineMemory.symbolic), $x_t^{(d)}=$NeuralNetController.get(InlineMemory.distributed)</li><li>Step4: NeuralNetController.fuse($s_t, x_t^{(d)}, x_t^{(s)}$)</li><li>Step5: 根据$x_t^{(s)}$去筛选object的candidate,这些candidate应该从Object Memory里面来?然后让这些candidate去meet $x_t^{(d)}$,应该是某个classifier训练出来之后,feed in $x_t^{(d)}$去选择candidates.</li><li>Step6: 根据$x_t^{(s)}$去筛选object的candidate,这些candidate应该从Object Memory里面来?然后让这些candidate去meet Step4的结果,和step5类似</li><li>Step7: 利用policy net去根据step6/step5的结果(应该是前面确定了object),去output出一个action</li><li>Step8: Apply action,然后更新Object Memory, Inline Memory, matrix Memory</li><li>Step9: 将Object Memory扔进Symbolic Reasoner 去做逻辑推断.得到一些暂时的结论,比如做一次information retrieval</li></ul><blockquote><p>着尼玛也太复杂了,搞我心态</p></blockquote><h2 id="OONP-Actions"><a href="#OONP-Actions" class="headerlink" title="OONP Actions"></a>OONP Actions</h2><p>我们来看看OONP有哪些actions.(被Policy-Net output出来的action)</p><ul><li>New-Assign: instantiate一个object,或者将手中的信息更新到某个已经又的object中</li><li>Update.X: 决定更新哪一个内部属性X</li><li>Update2what: 决定把这个内部属性更新成什么<br>经常会出现的: New-Assign一个某个类型的空object</li></ul><h3 id="New-assign有三种"><a href="#New-assign有三种" class="headerlink" title="New assign有三种"></a>New assign有三种</h3><ul><li>生成新的object $p(c, new|S_t)$ c是class类别</li><li>把现有的信息$S_t$填入已经有的object中 $p(c,k | S_t)$</li><li>什么都不做 $p(none | S_t)$<br>是否生成新的object主要由两个决定:</li><li>现有的信息$S_t$不能被已经有的object囊括</li><li>语法、语义的信息显示需要有新的object了<br>我们利用scores来计算所有的可能性:<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/21139338.jpg" alt=""><br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/72257414.jpg" alt=""><br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/8940273.jpg" alt=""><br>每一种颜色代表一个class的type.我们一共有6个object,所以会有六种可能$p(c, k|S_t)$,一共有三个类型,所以我们可能会产生三种新的object$p(c, new|S_t)$, 还有一种是什么都不做,所以一共考虑10种可能性.这些概率都是由policynet来决定</li></ul><h3 id="Updating-objects"><a href="#Updating-objects" class="headerlink" title="Updating objects"></a>Updating objects</h3><p>New Assign过后,Updata.X 会选择去update external link还是update internal property,然后Update2what会进一步决定如何更新</p><h3 id="关于action的例子"><a href="#关于action的例子" class="headerlink" title="关于action的例子"></a>关于action的例子</h3><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/42525372.jpg" alt=""><br>我们逐步建立了我们的ontology图,然后我们就可以用这个图进行推理、提取信息啦</p><h2 id="Symbolic扮演了很大的作用"><a href="#Symbolic扮演了很大的作用" class="headerlink" title="Symbolic扮演了很大的作用"></a>Symbolic扮演了很大的作用</h2><ul><li>利用分析action history给Reader提供信息比如:“The system just New an object with Person-class five words ago”-这个是Symbolic Analizer</li><li>利用object memory的符号表示:本体图来更新一些信息,比如 小红拿着苹果+小红去了厨房=》苹果也在厨房</li><li>之前提到的New Assign,我们决定使用是否新建对象的时候,需要比对现有的信息和object,这个比对需要symbolic的帮助下完成: Symbolic Matching</li></ul><h1 id="OONP-的-Learning"><a href="#OONP-的-Learning" class="headerlink" title="OONP 的 Learning"></a>OONP 的 Learning</h1><h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><p>参数是所有的distributed representation 以及后续需要处理它们产生的参数,标注“right action at each time step”,训练模型去maximize所有的这个decision的truth的likelihood<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/7911857.jpg" alt=""></p><h2 id="reinforcement-learning"><a href="#reinforcement-learning" class="headerlink" title="reinforcement learning"></a>reinforcement learning</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/46004951.jpg" alt=""><br>policy gradient的方法去学:<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/54569091.jpg" alt=""></p><h2 id="都用上"><a href="#都用上" class="headerlink" title="都用上"></a>都用上</h2><p>对于一些确定的任务,static ontology, 用SL不错<br>如果一些无法确定的任务,不能reverse engineer作为监督的我们需要用RL<br>combine 他们也非常简单<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/58341177.jpg" alt=""></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="BABI"><a href="#BABI" class="headerlink" title="BABI"></a>BABI</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/60663467.jpg" alt=""></p><h2 id="Police-Report"><a href="#Police-Report" class="headerlink" title="Police Report"></a>Police Report</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/60153941.jpg" alt=""><br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/54681811.jpg" alt=""></p><h2 id="Court-Judgement"><a href="#Court-Judgement" class="headerlink" title="Court Judgement"></a>Court Judgement</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/11465547.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Object-Oriented-Neural-Programming&quot;&gt;&lt;a href=&quot;#Object-Oriented-Neural-Programming&quot; class=&quot;headerlink&quot; title=&quot;Object Oriented Neural P
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/NLP/Paper/Note/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>2018-10-17组会takeaway</title>
    <link href="https://jeffchy.github.io/2018/10/17/2018-10-17%E7%BB%84%E4%BC%9Atakeaway/"/>
    <id>https://jeffchy.github.io/2018/10/17/2018-10-17组会takeaway/</id>
    <published>2018-10-17T11:58:34.000Z</published>
    <updated>2018-10-17T12:42:59.883Z</updated>
    
    <content type="html"><![CDATA[<h1 id="极简takeaway"><a href="#极简takeaway" class="headerlink" title="极简takeaway"></a>极简takeaway</h1><h2 id="Multitask-Learning-的用法"><a href="#Multitask-Learning-的用法" class="headerlink" title="Multitask Learning 的用法"></a>Multitask Learning 的用法</h2><ul><li>Predict What You Want 比如NER中的name对OOV非常敏感,所以我们可以先predict是否是name,这也就force模型学习到识别name的信息,然后再分别做</li><li>Reverse What You Dislike, 比如你有一个任务,Domain Adaption,你想要转换到不同的domain,所以你不希望区分domain的特征,所以在这个任务上加了一个classifier但是梯度反过来<img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/20164286.jpg" alt=""></li><li>用作data agumentation的trick.<img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/55329766.jpg" alt=""></li><li>添加任务使得神经网络更难.比如NMT中额外加了个任务预测parse tree,发现提升了NMT的效果.</li></ul><h2 id="Constrained-Decoder"><a href="#Constrained-Decoder" class="headerlink" title="Constrained Decoder"></a>Constrained Decoder</h2><p>用有限状态机做decoder,利用别的更容易得到的信息做decoder,也算一种multitask,mark Johnson的<br>Guided Open Vocabulary Image Captiong with Constrained Beamn Search<br>还有Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;极简takeaway&quot;&gt;&lt;a href=&quot;#极简takeaway&quot; class=&quot;headerlink&quot; title=&quot;极简takeaway&quot;&gt;&lt;/a&gt;极简takeaway&lt;/h1&gt;&lt;h2 id=&quot;Multitask-Learning-的用法&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="Group Meeting" scheme="https://jeffchy.github.io/tags/Group-Meeting/"/>
    
  </entry>
  
  <entry>
    <title>Self Organizing Map</title>
    <link href="https://jeffchy.github.io/2018/10/17/Self-Orgnizing-Map/"/>
    <id>https://jeffchy.github.io/2018/10/17/Self-Orgnizing-Map/</id>
    <published>2018-10-17T05:22:27.000Z</published>
    <updated>2018-10-17T06:44:07.395Z</updated>
    
    <content type="html"><![CDATA[<p>今天我们来学习一下Self organizing map,自组织映射.本文主要先参考Youtube的一些tutorial<br><a href="https://www.youtube.com/watch?v=H9H6s-x-0YE" target="_blank" rel="external">https://www.youtube.com/watch?v=H9H6s-x-0YE</a> [How SOM Works]<br><a href="https://www.youtube.com/watch?v=_Euwc9fWBJw" target="_blank" rel="external">https://www.youtube.com/watch?v=_Euwc9fWBJw</a> [How SOM Learn]</p><h1 id="SOM的输入输出"><a href="#SOM的输入输出" class="headerlink" title="SOM的输入输出"></a>SOM的输入输出</h1><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/3748111.jpg" alt=""><br>如图所示:</p><ul><li>Input<ul><li>$X=x_1 \cdots x_n$,每个$x_i$是一个feature的向量,每个entry可以理解成是一个attribute</li><li>Neural的数目</li><li>Epoch</li><li>Learning Rate</li><li>因为是无监督的一种人工神经网络所以我们不需要labelY</li></ul></li><li>Output<ul><li>每个$x$的类别 </li></ul></li></ul><h1 id="SOM-如何学习"><a href="#SOM-如何学习" class="headerlink" title="SOM 如何学习"></a>SOM 如何学习</h1><h2 id="传统的人工神经网络"><a href="#传统的人工神经网络" class="headerlink" title="传统的人工神经网络"></a>传统的人工神经网络</h2><p>如下图,假设输入X有三个column(feature),传统的ANN的weights是乘上x的某个feature,然后最后加起来<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/81541287.jpg" alt=""><br>也就是$\sum_i w_ix_i$</p><h2 id="SOM有很大的区别"><a href="#SOM有很大的区别" class="headerlink" title="SOM有很大的区别"></a>SOM有很大的区别</h2><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/7001971.jpg" alt=""><br>SOM的weights不是乘上feature用的了,而变成了Node本身的attributes.可以理解为,一个Ghost Data Point!或者说想象出的datapoint.</p><h2 id="Ghosts-Node-Neurons-与每个具体的datapoint的欧式距离评估并且互相竞争"><a href="#Ghosts-Node-Neurons-与每个具体的datapoint的欧式距离评估并且互相竞争" class="headerlink" title="Ghosts Node (Neurons)与每个具体的datapoint的欧式距离评估并且互相竞争"></a>Ghosts Node (Neurons)与每个具体的datapoint的欧式距离评估并且互相竞争</h2><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/27314565.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/23398176.jpg" alt=""><br>距离最小的neuron我们将它称为BMU(best matching unit)</p><ul><li>首先对每个neuron initialize feature</li><li>给定某个input $x_i$,$X$的某一行(列为feature),我们找出那个BMU</li><li>然后我们更新这个BMU<strong>以及它附近的neuron</strong>的权值.使得这一部分点更贴近这个数据</li><li><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/91384068.jpg" alt="图from wikipedia"></li></ul><h2 id="如果有多个BMU-挨个儿drag"><a href="#如果有多个BMU-挨个儿drag" class="headerlink" title="如果有多个BMU,挨个儿drag"></a>如果有多个BMU,挨个儿drag</h2><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/40857344.jpg" alt=""><br>每次radius的大小会越来越小.最后会变成这样<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/29158314.jpg" alt=""></p><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/73266070.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天我们来学习一下Self organizing map,自组织映射.本文主要先参考Youtube的一些tutorial&lt;br&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=H9H6s-x-0YE&quot; target=&quot;_blank&quot; rel
      
    
    </summary>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/Note/"/>
    
    
      <category term="Algorithm" scheme="https://jeffchy.github.io/tags/Algorithm/"/>
    
      <category term="Machine Learning" scheme="https://jeffchy.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>如何更好地建模数字:ACL18 Evaluating and Improving their Ability to Predict Numbers</title>
    <link href="https://jeffchy.github.io/2018/10/16/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-ACL18-Evaluating-and-Improving-their-Ability-to-Predict-Numbers/"/>
    <id>https://jeffchy.github.io/2018/10/16/论文解读-ACL18-Evaluating-and-Improving-their-Ability-to-Predict-Numbers/</id>
    <published>2018-10-16T07:17:58.000Z</published>
    <updated>2018-10-16T16:01:34.025Z</updated>
    
    <content type="html"><![CDATA[<p>今天说一下这篇ACL18的文章<br><a href="https://arxiv.org/abs/1805.08154" target="_blank" rel="external">https://arxiv.org/abs/1805.08154</a><br>Numeracy for Language Models:<br>Evaluating and Improving their Ability to Predict Numbers</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="现在表示数字的方法不合理"><a href="#现在表示数字的方法不合理" class="headerlink" title="现在表示数字的方法不合理"></a>现在表示数字的方法不合理</h2><p>本文讨论了language model中如何更好地predict数字的问题.language model能够建模一个单词sequence在文本下出现的概率,如果语法越符合规范,越符合实际,那么这个sequence的概率应该越高.<br><strong>realistic and grammatical</strong>,现有的方法不能很好地建模数字:</p><ul><li>新的数字出现out of vocabulary的概率比单词要高很多,很多单词都会被归为UNKNOWN</li><li>现有很多方法,比如GloVe中的embedding,数字只是很少的一部分,只有3.79%左右,所以很多现有模型并没有考虑数字的影响,因为本身微弱.但是同时在有些数据集中,比如儿童杂志、诊所报告中,数字出现的非常多</li><li><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/48112826.jpg" alt=""><h2 id="本文的贡献"><a href="#本文的贡献" class="headerlink" title="本文的贡献"></a>本文的贡献</h2></li><li>本文主要探索并且提出了一些方案去建模数字,比如<ul><li>digit-by-digit composition</li><li>memorisation</li><li>提出了一个深度学习模型,基于连续Probabiligy density function</li></ul></li><li>本文提出了对out of vocabulary的问题的一些处理方法</li><li>本文在clinical/scientific数据集上跑实验,并且发现<ul><li>把数字和单词分开处理会提升LM的困惑度</li><li>不同的建模数字的strategy对不同的上下文环境(数据集)的效果是不同的</li><li>连续概率密度函数可以提升LM的预测准确度</li></ul></li></ul><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>其实主要就是language model.language model:<br>$X = x_1, x_2, \cdots, x_n$, 利用chain-rule: $P(x_1,x_2,\cdots,x_n)=\prod_{i=1}^n P(x_i|x_1,\cdots,x_{i-1})$, 通常我们的做法是学习一个embedding,$E=R^{|V| \times D}$,将每个出现过的单词(属于vocabulary),用一个$D$维的向量表示.然后每个词用一个onehot表示去提取这个向量,然后输入模型中.</p><h2 id="char-embedding"><a href="#char-embedding" class="headerlink" title="char embedding"></a>char embedding</h2><p>除了单词之外,用char做embedding可以capture每个单词的前缀后缀这样的信息.比如love,就可以通过char level的embedding,通常是过一个RNN $e^{chars}=RNN(l,o,v,e)$,用hidden layer来encode词的每个字母的信息(数字的每个位同理).<br>这个在本为中只针对nuneral!</p><h2 id="Output-of-RNN"><a href="#Output-of-RNN" class="headerlink" title="Output of RNN"></a>Output of RNN</h2><p>$p(s_t|h_t)=softmax(\phi(s_t))$, $\phi$是score function, $h_t$代表t时刻的hidden layer</p><h2 id="Training-and-Evaluation"><a href="#Training-and-Evaluation" class="headerlink" title="Training and Evaluation"></a>Training and Evaluation</h2><p>Training $H_{train}=-\frac{1}{N}\sum_{s \in train} logp(s_t | s_{&lt;t})$<br>Evaluation主要用困惑度,其实就是,我们希望我们的模型预测出test set中这句话的概率越大越好.<br>$exp(H_{test})$</p><h1 id="Modeling-the-Numerals"><a href="#Modeling-the-Numerals" class="headerlink" title="Modeling the Numerals."></a>Modeling the Numerals.</h1><h2 id="Softmax-Model-and-Variants"><a href="#Softmax-Model-and-Variants" class="headerlink" title="Softmax Model and Variants"></a>Softmax Model and Variants</h2><h3 id="Basic-softmax"><a href="#Basic-softmax" class="headerlink" title="Basic softmax"></a>Basic softmax</h3><p>也就是传统的方法.这里的score function $\phi(s_t)=h_t^T e_{s_t}^{token}=h_t^T E_{out} w_{s_t}$这里的$E_{out}$其实Embedding Matrix, 而$e_{s_t}$是其中的一行.所以这里的$E_{out} \in R^{D\times |V|}$.其实就是原始的方法,这种方法假设,你一开始就知道所有的词,并且存成vocabulary,因此你遇见没有见过的新的词或者新的数字的话,你得用special word, 比如’UNKword’,’UNKnumeral’来表示他们.</p><h3 id="Softmax-with-Digit-Based-Embeddings"><a href="#Softmax-with-Digit-Based-Embeddings" class="headerlink" title="Softmax with Digit-Based Embeddings"></a>Softmax with Digit-Based Embeddings</h3><p>这里就是在原来的基础之上加上了char-level RNN,也是已经有了的工作了.不过把他们的表示直接加上而不是concat,这样也是可以的,关于相加和concat,挺玄学的,直觉上觉得concat保留了更多的信息,但是相加可能也会有很好的效果.至于为什么我就不太清楚了,欢迎指正.<br>所以我们的score function变成:</p><script type="math/tex; mode=display">\phi(s_t)=h_t^T e_{s_t}^{token}+h_t^T e_{t}^{chars}=h_t^T E_{out}w_{s_t}+h_t^T E^{RNN}_{out}w_{s_t}</script><p>这里的$e^{chars}$就是前面提到的char embedding了.</p><p>$E^{RNN}_{out}$的组成我们需要说一下:<br><strong>对于所有的in-vocabulary的numeral,我们有对应的char-embedding,对于in-vocabulary的word,我们还是给他原本的token embedding $e_{s_t}$,对于OOV的numeral和char embedding,该怎么样还是怎么样,我们还是要用UNK来替代他</strong></p><blockquote><p>这里我的疑问就是,为什么作者选择用char-level只对numeral做encoding,对word却还是用原来的token embedding呢?为什么不能用同样的strategy?是因为这样的区别印证了作者开头说的:把word和numeral分开处理效果更好吗?而且,对于新的OOV的数字,为什么不能直接同样地做char-level encoding呢?</p></blockquote><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>作者使用了层级softmax,就是先用一个softmax classifier,判断你是接下来生成数字还是单词,判断完成之后,每个分支继续softmax,比如对于生成数字的分支,我们的softmax相当于:$p(s_t|c_t=numeral, h_t)$,这两个分支之间是不共享任何参数的,所以我们彻底将word和numeral分割了开来.</p><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/78335419.jpg" alt=""><br>C={word,numeral}</p><h3 id="所以利用Hierarchical-Softmax-作者之后的工作就集中在如何表示Number这一块-也就是说如何建模-p-s-t-c-t-numeral-h-t-这个distribution"><a href="#所以利用Hierarchical-Softmax-作者之后的工作就集中在如何表示Number这一块-也就是说如何建模-p-s-t-c-t-numeral-h-t-这个distribution" class="headerlink" title="所以利用Hierarchical Softmax,作者之后的工作就集中在如何表示Number这一块,也就是说如何建模$p(s_t|c_t=numeral, h_t)$这个distribution"></a>所以利用Hierarchical Softmax,作者之后的工作就集中在如何表示Number这一块,也就是说如何建模$p(s_t|c_t=numeral, h_t)$这个distribution</h3><h2 id="Digit-RNN-Model"><a href="#Digit-RNN-Model" class="headerlink" title="Digit-RNN Model"></a>Digit-RNN Model</h2><p>emmm其实就是char level的RNN,只不过我们把之前token level得到的$h_t$feed进这个char level RNN而已,这样就在保留了上文的信息的同时,也用RNN来给数字提取了一个表示.这样我们就不需要用UNKnumeral这样的symbol了,对于任何新的词,我们都可以用RNN和之前的hidden state得到它的一个比较好的encoding,来predict它的概率.</p><h2 id="MoG"><a href="#MoG" class="headerlink" title="MoG"></a>MoG</h2><p>我们需要的是predict numeral的概率,作者想到用Mixture of Gaussian去计算.</p><script type="math/tex; mode=display">q(v)=\sum_{k=1}^{K} \pi_k N_k(v; \mu_k, \sigma_k^2)</script><script type="math/tex; mode=display">\pi_k = softmax(B^T h_t)</script><p>我们根据的hidden state来计算每个gaussian component的weight.对于一个连续的随机变量,对于它等于某个值的概率都是0,所以我们需要用cdf的差值近似出这个pdf.这个近似的精度和这个数字的十位数小数精度有关,这是也我认为这个模型有趣的地方.</p><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/5781206.jpg" alt=""><br>作者将一个数字的概率拆解成,留几位小数点的概率乘上已知小数点位数,值是多少的概率.</p><script type="math/tex; mode=display">p(s)=p(v,r)=p(r)\tilde{Q}(v|r)</script><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/78862107.jpg" alt=""></p><h2 id="Combination"><a href="#Combination" class="headerlink" title="Combination"></a>Combination</h2><p>作者又很骚地将三个模型combine了起来…相当于又加了一个hierarchy的softmax,首先根据hidden state预测使用哪种strategy,{h-softmax, d-RNN, MoG},然后扔进不同的模型中…最后求一个weighted sum.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/94549504.jpg" alt=""><br>其实和ensemble learning的思想是非常像的,不过ensemble learning的普通的bagging是取average.这个会自动选择weights.</p><h2 id="Perplexity-evaluation-APP"><a href="#Perplexity-evaluation-APP" class="headerlink" title="Perplexity evaluation APP"></a>Perplexity evaluation APP</h2><p>作者使用了adjust perplexity,因为perplexity对 OOV 的词非常敏感.<br>所以作者penalize了OOV的词的概率<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/17773389.jpg" alt=""></p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/40441429.jpg" alt=""></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>这里稍微提一下,作者用了EM算法无监督初始化高斯分布的$\sigma,\mu$<br>还有这个MoG模型没有办法直接学习到embedding,因为他会predict概率.而没有表示的过程.<br>这个是softmax学到的number embedding<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/99781036.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/98342249.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天说一下这篇ACL18的文章&lt;br&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.08154&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1805.08154&lt;/a&gt;&lt;br&gt;Nume
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Attention Is All You Need浅析</title>
    <link href="https://jeffchy.github.io/2018/10/14/Attention-Is-All-You-Need%E6%B5%85%E6%9E%90/"/>
    <id>https://jeffchy.github.io/2018/10/14/Attention-Is-All-You-Need浅析/</id>
    <published>2018-10-14T15:01:49.000Z</published>
    <updated>2018-10-15T15:18:13.050Z</updated>
    
    <content type="html"><![CDATA[<h1 id="不知道取什么的一级标题"><a href="#不知道取什么的一级标题" class="headerlink" title="不知道取什么的一级标题"></a>不知道取什么的一级标题</h1><p>今天还说说这一篇神作了,主要的参考资料是</p><ul><li>原paper: <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external">https://arxiv.org/abs/1706.03762</a></li><li>剑林大哥博客里的中文浅析(我觉得看paperweekly公众号的朋友应该都会知道他): <a href="https://kexue.fm/archives/4765" target="_blank" rel="external">https://kexue.fm/archives/4765</a></li><li>刘贺的博客 <a href="https://www.52coding.com.cn/index.php?/Articles/single/66" target="_blank" rel="external">https://www.52coding.com.cn/index.php?/Articles/single/66</a></li><li>Pytorch开源实现:<a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/" target="_blank" rel="external">https://github.com/jadore801120/attention-is-all-you-need-pytorch/</a></li><li>在一篇论文里面(BERT)里看到的,作者推荐的英文tutorial,Good english tutorial and implementation, for English speakers:) from harvard nlp -&gt; refer to.<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="external">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li></ul><h1 id="笔者的学习顺序"><a href="#笔者的学习顺序" class="headerlink" title="笔者的学习顺序"></a>笔者的学习顺序</h1><ol><li>笔者先看了剑林大哥和刘贺大哥的博客</li><li>笔者准备去读一遍论文</li><li>笔者准备去看哈佛的pytorch tutorial,正好最近需要用pytorch,学一下</li></ol><h1 id="论文提要"><a href="#论文提要" class="headerlink" title="论文提要"></a>论文提要</h1><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>作者先理了理目前解决<strong>sequence modeling and transduction problems</strong>的主流模型</p><ul><li>Recurrent models(RNN, LSTM, GRU), 利用hidden state或者加上cell state来传递序列的顺序信息,这种递归的结构<strong>无法并行</strong>,而且Recurrent Model没有办法很好的建模全局的结构信息.因为本质与Markov Process相似.</li><li>Attention mechanisms使得模型可以更好的建模依赖关系,但是通常我们把它和RNN一起用.</li><li>所以本作提出模型<strong>Tranformer</strong>,避免了RNN,完全基于attention机制,使得模型能够更好地建模输入和输出的全局依赖.而且可以并行.</li></ul><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>对于<strong>sequence transduction problems</strong>,目前用得最多的还是seq2seq的结构,我们训练一个Encoder $E$和一个Decoder $D$,通过Encoder,我们将一串Input $x={x_1,x_2,\cdots,x_n}$(比如英文句子的embedding表示)编码成$z={z_1, \cdots, z_n}$, 然后将$z$输入一个Decoder,得到target language (比如Chinese) $y={y_1, y_2, \cdots, y_m}$,以前的Encoder-Decoder结构主要用的是Recurrent Neural Network, 而本文的是将RNN单元替换掉,本质还是遵循着EncoderDecoder的基本思想.</p><h3 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/68227395.jpg" alt=""></p><h3 id="Encoder长这样"><a href="#Encoder长这样" class="headerlink" title="Encoder长这样"></a>Encoder长这样</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/67768606.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/75659670.jpg" alt=""></p><script type="math/tex; mode=display"> L1 = LayerNorm\{multiHeadSelfAttendLayer(input(x)+posiInput(x))+[input(x)+posiInput(x)]\}</script><script type="math/tex; mode=display"> L2 = LayerNorm\{ fc(L1)+L1 \}</script><p>这里模型使用了multi head self attention layer, 并且运用了 residual connection来保留原始信息的传播,然后每一层作layer-norm. 简单说下layer norm,别和batch norm搞混了.<br>layer norm: 注意layer norm是对layer的input作normalize,而batchnorm是对每个batch的x的某一个维度作normalize:)<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/24798913.jpg" alt=""></p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder也是非常类似的,除了加上了一层mask的multihead self attention layer,对这个我的理解是,decoding的时候,你需要防止attention去看那些padding补零的entry,所以要去除这个影响.</p><h3 id="真正的encoder-decoder是上面的单元的6次stack-以提升表达能力"><a href="#真正的encoder-decoder是上面的单元的6次stack-以提升表达能力" class="headerlink" title="真正的encoder decoder是上面的单元的6次stack.以提升表达能力"></a>真正的encoder decoder是上面的单元的6次stack.以提升表达能力</h3><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>现在的核心问题是解决什么是文章中的Attention,呢个Multi head self attention 到底是什么.</p><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>是Multi-head Attentionde的比本组成单元,长下图这样.其实和点乘计算相似度的attention是相同的.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/41878014.jpg" alt=""></p><ul><li>$Q$代表一个个query$q_t$组成的矩阵(比如一个词的embedding) </li><li>$K$代表Key,$k_t$组成的矩阵.</li><li>$V$代表Value,$v_t$组成的矩阵.</li><li>解释一下,attention是根据<strong>相似度训练出来的attention weights</strong>,对目标<strong>input或者output</strong>作加权求和.确定重点关注哪一个部分.所以我们先根据$<q_t, k_t="">$的matmul,得到query与每个key的相似度,然后用一个softmax得到对应的attention weights,也就是key对应的value的每个部分的关注度应该是多少. </q_t,></li><li>举个例子Attention在NMT的应用中,(这一段参考<a href="https://www.52coding.com.cn/index.php?/Articles/single/66" target="_blank" rel="external">https://www.52coding.com.cn/index.php?/Articles/single/66</a> 同样是一篇好文).<blockquote><p>可以理解为，比如刚翻译完主语，接下来想要找谓语，【找谓语】这个信息就是 query，然后 key 是源句子的编码，通过 query 和 key 计算出 attention weight （应该关注的谓语的位置），最后和 value （源句子编码）计算加权和。</p></blockquote></li></ul><p>我们的attention:)<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/20529845.jpg" alt=""><br>这样的矩阵运算是搞笑的,而这样的定义是有普适性的. $\frac{1}{\sqrt{d_k}}$用作normalizing,使得softmax更加soft</p><h3 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/41012200.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/35256709.jpg" alt=""><br>看图,很清晰,multihead attention就是先Linear()一下QKV,这些投影的weights是学习到的,使得模型有更强的表达能力.然后将这些输入scaled dot-product attention,然后concat起来</p><h3 id="中间的position-wise-的Feed-forward小模块"><a href="#中间的position-wise-的Feed-forward小模块" class="headerlink" title="中间的position wise 的Feed forward小模块"></a>中间的position wise 的Feed forward小模块</h3><p>其实就是<br>$Linear(ReLU(Linear(x)))$<br>但是谷歌又风骚地把它命名为$FFN$<br>可以被理解成两个一维的卷积,per layer</p><h3 id="为了保留位置信息-position-embedding作为输入是必须的"><a href="#为了保留位置信息-position-embedding作为输入是必须的" class="headerlink" title="为了保留位置信息,position embedding作为输入是必须的"></a>为了保留位置信息,position embedding作为输入是必须的</h3><p>因为模型中如果没有position embedding作为输入,如果改变两个单词的顺序,模型学到的东西是不会变的.<br>文章中直接用了这样的embedding for position<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/63416539.jpg" alt=""></p><h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>文章中用的attention主要是self-attention,也就是说QKV分别等于$query(X),X,X$,query(X)表示找寻与X有关系的单词.我们可以认为这样的self-attention能在encoding部分学到词与词之间的依赖关系.<img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/60719083.jpg" alt=""></p><h3 id="整个模型大概就是这样-可以说非常巧妙地使用-并且强化了attention地机制"><a href="#整个模型大概就是这样-可以说非常巧妙地使用-并且强化了attention地机制" class="headerlink" title="整个模型大概就是这样.可以说非常巧妙地使用,并且强化了attention地机制."></a>整个模型大概就是这样.可以说非常巧妙地使用,并且强化了attention地机制.</h3><h2 id="再说说代码-这里代码贴的是pytorch开源实现"><a href="#再说说代码-这里代码贴的是pytorch开源实现" class="headerlink" title="再说说代码 这里代码贴的是pytorch开源实现"></a>再说说代码 这里代码贴的是pytorch开源实现</h2><p><a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/" target="_blank" rel="external">https://github.com/jadore801120/attention-is-all-you-need-pytorch/</a></p><h3 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="string">''' Define the Layers '''</span></div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">from</span> transformer.SubLayers <span class="keyword">import</span> MultiHeadAttention, PositionwiseFeedForward</div><div class="line"></div><div class="line">__author__ = <span class="string">"Yu-Hsiang Huang"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' Compose with two layers '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super(EncoderLayer, self).__init__()</div><div class="line">        self.slf_attn = MultiHeadAttention(</div><div class="line">            n_head, d_model, d_k, d_v, dropout=dropout)</div><div class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, enc_input, non_pad_mask=None, slf_attn_mask=None)</span>:</span></div><div class="line">        enc_output, enc_slf_attn = self.slf_attn(</div><div class="line">            enc_input, enc_input, enc_input, mask=slf_attn_mask)</div><div class="line">        enc_output *= non_pad_mask</div><div class="line"></div><div class="line">        enc_output = self.pos_ffn(enc_output)</div><div class="line">        enc_output *= non_pad_mask</div><div class="line"></div><div class="line">        <span class="keyword">return</span> enc_output, enc_slf_attn</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' Compose with three layers '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super(DecoderLayer, self).__init__()</div><div class="line">        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</div><div class="line">        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</div><div class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, dec_input, enc_output, non_pad_mask=None, slf_attn_mask=None, dec_enc_attn_mask=None)</span>:</span></div><div class="line">        dec_output, dec_slf_attn = self.slf_attn(</div><div class="line">            dec_input, dec_input, dec_input, mask=slf_attn_mask)</div><div class="line">        dec_output *= non_pad_mask</div><div class="line"></div><div class="line">        dec_output, dec_enc_attn = self.enc_attn(</div><div class="line">            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)</div><div class="line">        dec_output *= non_pad_mask</div><div class="line"></div><div class="line">        dec_output = self.pos_ffn(dec_output)</div><div class="line">        dec_output *= non_pad_mask</div><div class="line"></div><div class="line">        <span class="keyword">return</span> dec_output, dec_slf_attn, dec_enc_attn</div></pre></td></tr></table></figure><h3 id="Sublayers-Multihead-attention-PositionwiseFeedforward"><a href="#Sublayers-Multihead-attention-PositionwiseFeedforward" class="headerlink" title="Sublayers Multihead-attention, PositionwiseFeedforward"></a>Sublayers Multihead-attention, PositionwiseFeedforward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Define the sublayers in encoder/decoder layer '''</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> transformer.Modules <span class="keyword">import</span> ScaledDotProductAttention</div><div class="line"></div><div class="line">__author__ = <span class="string">"Yu-Hsiang Huang"</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' Multi-Head Attention module '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_head, d_model, d_k, d_v, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super().__init__()</div><div class="line"></div><div class="line">        self.n_head = n_head</div><div class="line">        self.d_k = d_k</div><div class="line">        self.d_v = d_v</div><div class="line"></div><div class="line">        self.w_qs = nn.Linear(d_model, n_head * d_k)</div><div class="line">        self.w_ks = nn.Linear(d_model, n_head * d_k)</div><div class="line">        self.w_vs = nn.Linear(d_model, n_head * d_v)</div><div class="line">        nn.init.normal_(self.w_qs.weight, mean=<span class="number">0</span>, std=np.sqrt(<span class="number">2.0</span> / (d_model + d_k)))</div><div class="line">        nn.init.normal_(self.w_ks.weight, mean=<span class="number">0</span>, std=np.sqrt(<span class="number">2.0</span> / (d_model + d_k)))</div><div class="line">        nn.init.normal_(self.w_vs.weight, mean=<span class="number">0</span>, std=np.sqrt(<span class="number">2.0</span> / (d_model + d_v)))</div><div class="line"></div><div class="line">        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, <span class="number">0.5</span>))</div><div class="line">        self.layer_norm = nn.LayerNorm(d_model)</div><div class="line"></div><div class="line">        self.fc = nn.Linear(n_head * d_v, d_model)</div><div class="line">        nn.init.xavier_normal_(self.fc.weight)</div><div class="line"></div><div class="line">        self.dropout = nn.Dropout(dropout)</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, q, k, v, mask=None)</span>:</span></div><div class="line"></div><div class="line">        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head</div><div class="line"></div><div class="line">        sz_b, len_q, _ = q.size()</div><div class="line">        sz_b, len_k, _ = k.size()</div><div class="line">        sz_b, len_v, _ = v.size()</div><div class="line"></div><div class="line">        residual = q</div><div class="line"></div><div class="line">        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)</div><div class="line">        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)</div><div class="line">        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)</div><div class="line"></div><div class="line">        q = q.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(<span class="number">-1</span>, len_q, d_k) <span class="comment"># (n*b) x lq x dk</span></div><div class="line">        k = k.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(<span class="number">-1</span>, len_k, d_k) <span class="comment"># (n*b) x lk x dk</span></div><div class="line">        v = v.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(<span class="number">-1</span>, len_v, d_v) <span class="comment"># (n*b) x lv x dv</span></div><div class="line"></div><div class="line">        mask = mask.repeat(n_head, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># (n*b) x .. x ..</span></div><div class="line">        output, attn = self.attention(q, k, v, mask=mask)</div><div class="line"></div><div class="line">        output = output.view(n_head, sz_b, len_q, d_v)</div><div class="line">        output = output.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>).contiguous().view(sz_b, len_q, <span class="number">-1</span>) <span class="comment"># b x lq x (n*dv)</span></div><div class="line"></div><div class="line">        output = self.dropout(self.fc(output))</div><div class="line">        output = self.layer_norm(output + residual)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> output, attn</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' A two-feed-forward-layer module '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_in, d_hid, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super().__init__()</div><div class="line">        self.w_1 = nn.Conv1d(d_in, d_hid, <span class="number">1</span>) <span class="comment"># position-wise</span></div><div class="line">        self.w_2 = nn.Conv1d(d_hid, d_in, <span class="number">1</span>) <span class="comment"># position-wise</span></div><div class="line">        self.layer_norm = nn.LayerNorm(d_in)</div><div class="line">        self.dropout = nn.Dropout(dropout)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        residual = x</div><div class="line">        output = x.transpose(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">        output = self.w_2(F.relu(self.w_1(output)))</div><div class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">        output = self.dropout(output)</div><div class="line">        output = self.layer_norm(output + residual)</div><div class="line">        <span class="keyword">return</span> output</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;不知道取什么的一级标题&quot;&gt;&lt;a href=&quot;#不知道取什么的一级标题&quot; class=&quot;headerlink&quot; title=&quot;不知道取什么的一级标题&quot;&gt;&lt;/a&gt;不知道取什么的一级标题&lt;/h1&gt;&lt;p&gt;今天还说说这一篇神作了,主要的参考资料是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/NLP/Paper/Note/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>简单说说BERT模型:Pre-training of Deep Bidirectional Transformers for Language Understanding </title>
    <link href="https://jeffchy.github.io/2018/10/13/%E7%AE%80%E5%8D%95%E8%AF%B4%E8%AF%B4BERT%E6%A8%A1%E5%9E%8B-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/"/>
    <id>https://jeffchy.github.io/2018/10/13/简单说说BERT模型-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/</id>
    <published>2018-10-13T12:36:50.000Z</published>
    <updated>2018-10-13T13:55:24.858Z</updated>
    
    <content type="html"><![CDATA[<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>最近朋友圈又被<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="external">BERT</a>刷屏了,ELMO刷屏之后还没多久,谷歌就提出了吊打一切的BERT,用它可以获得更好的单词在文本中的隐藏表示,在11大NLP的任务中达到了变态的结果,太不给机会了.</p><h2 id="BERT的中心思想和碎碎念"><a href="#BERT的中心思想和碎碎念" class="headerlink" title="BERT的中心思想和碎碎念"></a>BERT的中心思想和碎碎念</h2><p>BERT是Bidirectional Encoder Representations from Transformers的缩写,产生了一个pretrained的Encoder,可以很好的建模单词的上下文信息,获得一个单词的隐藏表示.和ELMO类似的地方就是,它算是一个Encoder,我们需要在corpus上跑一下这个模型,(Bidirectional Transformer),得到每一个单词在上下文中的hidden representation.并不是像word vector一样即插即用.但是它的优势是:</p><ul><li>相对其他这样的模型来讲还是比较快速的</li><li>效果变态好</li><li>finetuning的过程也并不复杂</li></ul><h2 id="一些文章推荐"><a href="#一些文章推荐" class="headerlink" title="一些文章推荐"></a>一些文章推荐</h2><p>BERT的详细介绍很多推送已经说过了,重复一下好像意义不大,给一个传送门吧:<br><a href="https://zhuanlan.zhihu.com/p/46656854" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/46656854</a> 机器之心[中文]<br><a href="https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/" target="_blank" rel="external">https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/</a> Reddit上第一作者的帖子,非常简明[英文,需要f q]</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>可以看到,模型本身就是Bidirectional-transformer.如果你了解Transformer模型的话,那么这个模型其实就很好理解了,没什么可以说的.论文中也直接跳过,refer to到了Attention is all you need这一篇神作. 如果你不太清楚transformer模型,之后我也会介绍一下.<br>整个BERT模型的设计原则就是, Language Model的训练其实制约了word representation, 从左到右、或者结合了从右到左这样的顺序性隐藏在了之前的模型中.这篇文章利用双向的transformer,(最左边的那个),可以真正地将词和context直接而密集地链接起来.<br><img src="http://oj4pv4f25.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-13%20%E4%B8%8B%E5%8D%889.54.46.png" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-13%20%E4%B8%8B%E5%8D%889.21.20.png" alt=""></p><h2 id="模型训练的任务"><a href="#模型训练的任务" class="headerlink" title="模型训练的任务"></a>模型训练的任务</h2><p>作者在两个任务上训练.</p><ol><li><p>Masked LM 随机扔掉一些词,然后预测他们</p><blockquote><p>Input:<br>the man [MASK1] to [MASK2] store<br>Label:<br>[MASK1] = went; [MASK2] = store</p></blockquote></li><li><p>Next Sentence Prediction 打乱一些句子,构造数据集,预测句子A的后面是否是句子B, true or false  </p></li></ol><ul><li>Input:the man went to the store [SEP] he bought a gallon of milk<br>Label: IsNext</li><li>Input: the man went to the store [SEP] penguins are flightless birds<br>Label: NotNext</li></ul><p>通过这两个任务的训练得到的Bidirectional Transformer, 成为了一个足够全面,强力的,能建模token level(词和词之间), sequence level (句子和句子之间)信息的Encoder.</p><h2 id="作者在各种任务上做了测试"><a href="#作者在各种任务上做了测试" class="headerlink" title="作者在各种任务上做了测试"></a>作者在各种任务上做了测试</h2><p>吊打一切.jpg<br>真的是吊打一切,只能这么形容了…<br>详情看paper :)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;BERT&quot;&gt;&lt;a href=&quot;#BERT&quot; class=&quot;headerlink&quot; title=&quot;BERT&quot;&gt;&lt;/a&gt;BERT&lt;/h2&gt;&lt;p&gt;最近朋友圈又被&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot; target=&quot;
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Natual Language Processing" scheme="https://jeffchy.github.io/tags/Natual-Language-Processing/"/>
    
  </entry>
  
</feed>
