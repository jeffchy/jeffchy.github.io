<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jeff-Chiang</title>
  
  <subtitle>Tech and Life</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jeffchy.github.io/"/>
  <updated>2019-01-28T14:58:59.923Z</updated>
  <id>https://jeffchy.github.io/</id>
  
  <author>
    <name>Jeff Chiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Uncovering divergent linguistic Information in word embeddings - Paper Notes</title>
    <link href="https://jeffchy.github.io/2019/01/28/Uncovering-divergent-linguistic-Information-in-word-embeddings-Paper-Notes/"/>
    <id>https://jeffchy.github.io/2019/01/28/Uncovering-divergent-linguistic-Information-in-word-embeddings-Paper-Notes/</id>
    <published>2019-01-27T16:57:36.000Z</published>
    <updated>2019-01-28T14:58:59.923Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1809.02094.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1809.02094.pdf</a><br>Paper from arxiv, Eneko Agirre, IXA NLP Group</p><p>This paper answered some very interesting questions, In word-embedding evaluations, tasks focused on differrent aspects of (Word Similarity &amp; Relationship)</p><p>Two axis:</p><ul><li>Similar - Relatedness (reflacts 1 or 2 order coocurrance) </li><li>Syntax - Semantics (rings - ring, happy - glad) </li></ul><p>This paper found that, We can use a <strong>linear transformation</strong> to adjust the learned word embedding to the intrinsic task-specific region!<br>Interesting results haha</p><p>This paper interpreted these aspects as axis, and claim that we cannot reach a optimum that have best performance for every perspective, or in other words, we always have some trade-off between them.</p><p>In the downstream tasks, supervised tasks can learn the optimal transformation automaticly, but those unsupervised tasks that dirrectly use the embeddings cannot be transformed.</p><p>So this paper shows that our word embeddings captures more informations than we expected.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1809.02094.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/pdf/1809.02094.pdf&lt;/a&gt;&lt;br&gt;Paper from arxi
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Probability Theory" scheme="https://jeffchy.github.io/tags/Probability-Theory/"/>
    
      <category term="Algorithm" scheme="https://jeffchy.github.io/tags/Algorithm/"/>
    
      <category term="Natural Languate Processing" scheme="https://jeffchy.github.io/tags/Natural-Languate-Processing/"/>
    
  </entry>
  
  <entry>
    <title>NeurlPS 2018 Word Embedding (II), FRAGE: frequency-agnostic word representation</title>
    <link href="https://jeffchy.github.io/2019/01/17/NeurlPS-2018-Word-Embedding-II-FRAGE-frequency-agnostic-word-representation/"/>
    <id>https://jeffchy.github.io/2019/01/17/NeurlPS-2018-Word-Embedding-II-FRAGE-frequency-agnostic-word-representation/</id>
    <published>2019-01-17T13:35:38.000Z</published>
    <updated>2019-01-17T13:53:29.672Z</updated>
    
    <content type="html"><![CDATA[<p>说一下 NeurlPS 2018的第二篇Embedding相关文章。<br>Frequency-Agnostic Word Representation <a href="https://arxiv.org/abs/1809.06858" target="_blank" rel="external">https://arxiv.org/abs/1809.06858</a></p><p>首先作者发现了现有的word embedding的一个现象。不常见的词（low-frequency word)和常见的词(high-frequency word)通常聚集在embedding空间的不同区域，如图：<br><a href="https://imgchr.com/i/kpfgjH" target="_blank" rel="external"><img src="https://s2.ax1x.com/2019/01/17/kpfgjH.md.png" alt="kpfgjH.md.png"></a><br>这回导致比如peking这种少见词和beijing这种词的意义不接近。</p><p>作者提出的方法有趣也简单，利用adversarial training，在训练task-specific word embedding的时候，额外训练一个Discriminator判别器，根据词的向量去做一个二元的分类，判断到底是low/high-frequency word。同时word embedding的训练过程也增加了一个目的（task-dependent loss），就是去fool这个discriminator，让他没办法判别正确这个embedding是否稀少。</p><p>一个例子，设定实在language model下<br><a href="https://imgchr.com/i/kpfLuj" target="_blank" rel="external"><img src="https://s2.ax1x.com/2019/01/17/kpfLuj.md.png" alt="kpfLuj.md.png"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;说一下 NeurlPS 2018的第二篇Embedding相关文章。&lt;br&gt;Frequency-Agnostic Word Representation &lt;a href=&quot;https://arxiv.org/abs/1809.06858&quot; target=&quot;_blank&quot; r
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Python Loading Large Dataset</title>
    <link href="https://jeffchy.github.io/2019/01/07/Python-Loading-Large-Dataset/"/>
    <id>https://jeffchy.github.io/2019/01/07/Python-Loading-Large-Dataset/</id>
    <published>2019-01-07T12:20:21.000Z</published>
    <updated>2019-01-07T12:45:49.984Z</updated>
    
    <content type="html"><![CDATA[<p>今天说些写代码时碰到的问题，关于如何save&amp;load一个大的数据object。<br>我们对比两种工具，joblib和pickle。</p><h3 id="首先说结论-如果是numpy-array，joblib各项性能效果比pickle更好，如果是原生自带的Python-object，如列表套列表，pickle远远快于joblib"><a href="#首先说结论-如果是numpy-array，joblib各项性能效果比pickle更好，如果是原生自带的Python-object，如列表套列表，pickle远远快于joblib" class="headerlink" title="首先说结论,如果是numpy array，joblib各项性能效果比pickle更好，如果是原生自带的Python object，如列表套列表，pickle远远快于joblib"></a>首先说结论,如果是numpy array，joblib各项性能效果比pickle更好，如果是原生自带的Python object，如列表套列表，pickle远远快于joblib</h3><h3 id="If-you-are-dumping-amp-loading-Numpy-objects-joblib-is-a-better-choice-while-you-are-dumping-amp-loading-Python-object-something-like-1-1-1-2-‘hello’-joblib-is-insanely-slow-never-use-it-under-this-condition-use-pickle-which-is-much-much-better"><a href="#If-you-are-dumping-amp-loading-Numpy-objects-joblib-is-a-better-choice-while-you-are-dumping-amp-loading-Python-object-something-like-1-1-1-2-‘hello’-joblib-is-insanely-slow-never-use-it-under-this-condition-use-pickle-which-is-much-much-better" class="headerlink" title="If you are dumping &amp; loading Numpy objects, joblib is a better choice, while you are dumping &amp; loading Python object, something like [[1,1],[(1,2)], ‘hello’], joblib is insanely slow, never use it under this condition, use pickle, which is much, much better."></a>If you are dumping &amp; loading Numpy objects, joblib is a better choice, while you are dumping &amp; loading Python object, something like [[1,1],[(1,2)], ‘hello’], joblib is insanely slow, never use it under this condition, use pickle, which is much, much better.</h3><p>测试代码示例：<br>测试使用简单的jupyter notebook %%timeit<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</div><div class="line"><span class="keyword">import</span> pickle</div><div class="line"></div><div class="line">a = np.random.rand(<span class="number">5000</span>, <span class="number">5000</span>) <span class="comment"># large numpy object 200MB</span></div><div class="line">b = [[<span class="number">1000</span>]*<span class="number">10</span>] * <span class="number">100000</span>    <span class="comment"># large python object 30MB</span></div><div class="line"></div><div class="line"><span class="comment"># NUMPY OBJECT DUMP</span></div><div class="line">joblib.dump(a,<span class="string">'a.jbpkl'</span>) <span class="comment"># 211ms</span></div><div class="line">pickle.dump(a, open(<span class="string">'a.pkl'</span>, <span class="string">'wb'</span>)) <span class="comment"># 474ms</span></div><div class="line"></div><div class="line"><span class="comment"># PYTHON OBJECT DUMP</span></div><div class="line">joblib.dump(b,<span class="string">'b.jbpkl'</span>) <span class="comment"># 1.89s</span></div><div class="line">pickle.dump(b, open(<span class="string">'b.pkl'</span>, <span class="string">'wb'</span>)) <span class="comment"># 29ms</span></div><div class="line"></div><div class="line"><span class="comment"># NUMPY OBJECT LOAD</span></div><div class="line">joblib.load(<span class="string">'a.jbpkl'</span>) <span class="comment"># 247ms</span></div><div class="line">pickle.load(open(<span class="string">'a.pkl'</span>, <span class="string">'rb'</span>)) <span class="comment"># 266ms</span></div><div class="line"></div><div class="line"><span class="comment"># PYTHON OBJECT DUMO</span></div><div class="line">joblib.load(<span class="string">'b.jbpkl'</span>) <span class="comment"># 988ms</span></div><div class="line">pickle.load(open(<span class="string">'b.pkl'</span>, <span class="string">'rb'</span>)) <span class="comment"># 19.8</span></div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天说些写代码时碰到的问题，关于如何save&amp;amp;load一个大的数据object。&lt;br&gt;我们对比两种工具，joblib和pickle。&lt;/p&gt;
&lt;h3 id=&quot;首先说结论-如果是numpy-array，joblib各项性能效果比pickle更好，如果是原生自带的Py
      
    
    </summary>
    
      <category term="Tech" scheme="https://jeffchy.github.io/categories/Tech/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/Tech/Note/"/>
    
      <category term="Code" scheme="https://jeffchy.github.io/categories/Tech/Note/Code/"/>
    
    
      <category term="python" scheme="https://jeffchy.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch中tensor的批量赋值以及选择</title>
    <link href="https://jeffchy.github.io/2018/12/18/Pytorch%E4%B8%ADtensor%E7%9A%84%E6%89%B9%E9%87%8F%E8%B5%8B%E5%80%BC%E4%BB%A5%E5%8F%8A%E9%80%89%E6%8B%A9/"/>
    <id>https://jeffchy.github.io/2018/12/18/Pytorch中tensor的批量赋值以及选择/</id>
    <published>2018-12-18T11:41:41.000Z</published>
    <updated>2018-12-18T12:39:18.601Z</updated>
    
    <content type="html"><![CDATA[<p>今天来谈一谈pytorch中快速批量的赋值和选择的操作,这种方法非常简洁,而且比for循环便利i,j,k index的效率高上很多很多倍,并且可以利用gpu加速.用深度学习框架的时候,务必避免for循环出现.</p><p>我们直接上代码:</p><p>首先我们随机生成一个 2x3x4的tensor,结果:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> torch</div><div class="line"></div><div class="line">In [<span class="number">2</span>]: a = torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</div><div class="line"></div><div class="line">In [<span class="number">3</span>]: a</div><div class="line">Out[<span class="number">3</span>]:</div><div class="line">tensor([[[<span class="number">0.8571</span>, <span class="number">0.1589</span>, <span class="number">0.4834</span>, <span class="number">0.5854</span>],</div><div class="line">         [<span class="number">0.5580</span>, <span class="number">0.0499</span>, <span class="number">0.9528</span>, <span class="number">0.4695</span>],</div><div class="line">         [<span class="number">0.0216</span>, <span class="number">0.5603</span>, <span class="number">0.5546</span>, <span class="number">0.8420</span>]],</div><div class="line"></div><div class="line">        [[<span class="number">0.1714</span>, <span class="number">0.5611</span>, <span class="number">0.6885</span>, <span class="number">0.7318</span>],</div><div class="line">         [<span class="number">0.2412</span>, <span class="number">0.0759</span>, <span class="number">0.0850</span>, <span class="number">0.8739</span>],</div><div class="line">         [<span class="number">0.0810</span>, <span class="number">0.9401</span>, <span class="number">0.2520</span>, <span class="number">0.3242</span>]]])</div></pre></td></tr></table></figure></p><p>我们找出tensor中大于0.8的数,得到一个one-hot的一个index的矩阵.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">5</span>]: idxs = a.ge(<span class="number">0.8</span>)</div><div class="line"></div><div class="line">In [<span class="number">6</span>]: idxs</div><div class="line">Out[<span class="number">6</span>]:</div><div class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</div><div class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</div><div class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]],</div><div class="line"></div><div class="line">        [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</div><div class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</div><div class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]]], dtype=torch.uint8)</div><div class="line"></div><div class="line">In [<span class="number">7</span>]: idxs.size()</div><div class="line">Out[<span class="number">7</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</div></pre></td></tr></table></figure></p><p>直接用如下的方法就能轻松取出这些满足条件的(被打上1的标签)的元素了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">8</span>]: a[idxs]</div><div class="line">Out[<span class="number">8</span>]: tensor([<span class="number">0.8571</span>, <span class="number">0.9528</span>, <span class="number">0.8420</span>, <span class="number">0.8739</span>, <span class="number">0.9401</span>])</div><div class="line"></div><div class="line">In [<span class="number">9</span>]: idxs.sum()</div><div class="line">Out[<span class="number">9</span>]: tensor(<span class="number">5</span>) <span class="comment"># 和上一个tensor的长度相同</span></div></pre></td></tr></table></figure></p><p>同样的,我们也可以批量赋值这些entries<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">10</span>]: a[idxs] = <span class="number">999</span></div><div class="line"></div><div class="line">In [<span class="number">11</span>]: a</div><div class="line">Out[<span class="number">11</span>]:</div><div class="line">tensor([[[<span class="number">999.0000</span>,   <span class="number">0.1589</span>,   <span class="number">0.4834</span>,   <span class="number">0.5854</span>],</div><div class="line">         [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">         [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]],</div><div class="line"></div><div class="line">        [[  <span class="number">0.1714</span>,   <span class="number">0.5611</span>,   <span class="number">0.6885</span>,   <span class="number">0.7318</span>],</div><div class="line">         [  <span class="number">0.2412</span>,   <span class="number">0.0759</span>,   <span class="number">0.0850</span>, <span class="number">999.0000</span>],</div><div class="line">         [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]]])</div></pre></td></tr></table></figure></p><h3 id="小小总结一下-如果你有一个张量-想要批量取出-赋值-访问其中的元素-你可以用与这个张量相同维度的onehot表示批量取出-⚠️onehot表示必须是dtype-torch-uint8"><a href="#小小总结一下-如果你有一个张量-想要批量取出-赋值-访问其中的元素-你可以用与这个张量相同维度的onehot表示批量取出-⚠️onehot表示必须是dtype-torch-uint8" class="headerlink" title="小小总结一下:如果你有一个张量,想要批量取出/赋值/访问其中的元素,你可以用与这个张量相同维度的onehot表示批量取出,⚠️onehot表示必须是dtype=torch.uint8"></a>小小总结一下:如果你有一个张量,想要批量取出/赋值/访问其中的元素,你可以用与这个张量相同维度的onehot表示批量取出,⚠️onehot表示必须是dtype=torch.uint8</h3><p>同样,还是这个2x3x4的张量,如何批量得取出/赋值他的最后一维长度的向量?我们只需要构造一个维度是2x3的onehot表示即可,比如下面代码中我们将三个位置标记成了1.我们就取出了对应位置的3个4维向量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">13</span>]: idxs = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]],dtype=torch.uint8)</div><div class="line"></div><div class="line">In [<span class="number">14</span>]: idxs.size()</div><div class="line">Out[<span class="number">14</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</div><div class="line"></div><div class="line">In [<span class="number">15</span>]: a[idxs]</div><div class="line">Out[<span class="number">15</span>]:</div><div class="line">tensor([[<span class="number">999.0000</span>,   <span class="number">0.1589</span>,   <span class="number">0.4834</span>,   <span class="number">0.5854</span>],</div><div class="line">        [  <span class="number">0.1714</span>,   <span class="number">0.5611</span>,   <span class="number">0.6885</span>,   <span class="number">0.7318</span>],</div><div class="line">        [  <span class="number">0.2412</span>,   <span class="number">0.0759</span>,   <span class="number">0.0850</span>, <span class="number">999.0000</span>]])</div><div class="line"></div><div class="line">In [<span class="number">16</span>]: a[idxs].size()</div><div class="line">Out[<span class="number">16</span>]: torch.Size([<span class="number">3</span>, <span class="number">4</span>])</div><div class="line"></div><div class="line">In [<span class="number">17</span>]: idxs.sum()</div><div class="line">Out[<span class="number">17</span>]: tensor(<span class="number">3</span>)</div></pre></td></tr></table></figure></p><p>赋值操作也是一样的.赋值的向量需要和原始向量有相同的dtype,所以最后一个333笔者把它写成了333.0这样他们的dtype都是float了.可以看到,我们tensor的对应entry被赋值成了对应的向量!<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">21</span>]: b = torch.tensor([[<span class="number">111</span>,<span class="number">111</span>,<span class="number">111</span>,<span class="number">111</span>],[<span class="number">222</span>,<span class="number">222</span>,<span class="number">222</span>,<span class="number">222</span>],[<span class="number">333</span>,<span class="number">333</span>,<span class="number">333</span>,<span class="number">333.0</span>]])</div><div class="line"></div><div class="line">In [<span class="number">22</span>]: a[idxs] = b</div><div class="line"></div><div class="line">In [<span class="number">23</span>]: a</div><div class="line">Out[<span class="number">23</span>]:</div><div class="line">tensor([[[<span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>],</div><div class="line">         [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">         [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]],</div><div class="line"></div><div class="line">        [[<span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>],</div><div class="line">         [<span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>],</div><div class="line">         [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]]])</div></pre></td></tr></table></figure></p><h3 id="小小总结一下-如果你有一个张量-想要批量取出-赋值-访问其中的最后某些维度的tensor-你可以用与这个张量前面剩余维度相同维度的onehot表示批量取出-⚠️onehot表示必须是dtype-torch-uint8"><a href="#小小总结一下-如果你有一个张量-想要批量取出-赋值-访问其中的最后某些维度的tensor-你可以用与这个张量前面剩余维度相同维度的onehot表示批量取出-⚠️onehot表示必须是dtype-torch-uint8" class="headerlink" title="小小总结一下:如果你有一个张量,想要批量取出/赋值/访问其中的最后某些维度的tensor,你可以用与这个张量前面剩余维度相同维度的onehot表示批量取出,⚠️onehot表示必须是dtype=torch.uint8"></a>小小总结一下:如果你有一个张量,想要批量取出/赋值/访问其中的最后某些维度的tensor,你可以用与这个张量前面剩余维度相同维度的onehot表示批量取出,⚠️onehot表示必须是dtype=torch.uint8</h3><p>如果不是torch.uint8会发生什么:这同样是一种批量取出的方法:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">24</span>]: idxs = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]])</div><div class="line"></div><div class="line">In [<span class="number">25</span>]: a[idxs]</div><div class="line">Out[<span class="number">25</span>]:</div><div class="line">tensor([[[[<span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>],</div><div class="line">          [<span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>],</div><div class="line">          [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]],</div><div class="line"></div><div class="line">         [[<span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>],</div><div class="line">          [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">          [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]],</div><div class="line"></div><div class="line">         [[<span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>],</div><div class="line">          [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">          [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]]],</div><div class="line"></div><div class="line"></div><div class="line">        [[[<span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>],</div><div class="line">          [<span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>],</div><div class="line">          [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]],</div><div class="line"></div><div class="line">         [[<span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>],</div><div class="line">          [<span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>],</div><div class="line">          [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]],</div><div class="line"></div><div class="line">         [[<span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>],</div><div class="line">          [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">          [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]]]])</div><div class="line"></div><div class="line">In [<span class="number">26</span>]: a[idxs].size()</div><div class="line">Out[<span class="number">26</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>])</div><div class="line"></div><div class="line">In [<span class="number">27</span>]: a[<span class="number">0</span>]</div><div class="line">Out[<span class="number">27</span>]:</div><div class="line">tensor([[<span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>, <span class="number">111.0000</span>],</div><div class="line">        [  <span class="number">0.5580</span>,   <span class="number">0.0499</span>, <span class="number">999.0000</span>,   <span class="number">0.4695</span>],</div><div class="line">        [  <span class="number">0.0216</span>,   <span class="number">0.5603</span>,   <span class="number">0.5546</span>, <span class="number">999.0000</span>]])</div><div class="line"></div><div class="line">In [<span class="number">28</span>]: a[<span class="number">1</span>]</div><div class="line">Out[<span class="number">28</span>]:</div><div class="line">tensor([[<span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>, <span class="number">222.0000</span>],</div><div class="line">        [<span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>, <span class="number">333.0000</span>],</div><div class="line">        [  <span class="number">0.0810</span>, <span class="number">999.0000</span>,   <span class="number">0.2520</span>,   <span class="number">0.3242</span>]])</div></pre></td></tr></table></figure></p><p>我们看到,仅仅是去掉了torch.uint8,结果发生了非常大的变化.我们这里的idxs的含义不再是onehot表示,而变成了:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a[torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]])] ==</div><div class="line">[[a[<span class="number">1</span>],a[<span class="number">0</span>],a[<span class="number">0</span>]],[a[<span class="number">1</span>],a[<span class="number">1</span>],a[<span class="number">0</span>]]]</div></pre></td></tr></table></figure></p><p>这种也是一种批量取出的有效方法,但是我们就没有办法批量赋值啦.</p><p>希望能帮助到大家.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天来谈一谈pytorch中快速批量的赋值和选择的操作,这种方法非常简洁,而且比for循环便利i,j,k index的效率高上很多很多倍,并且可以利用gpu加速.用深度学习框架的时候,务必避免for循环出现.&lt;/p&gt;
&lt;p&gt;我们直接上代码:&lt;/p&gt;
&lt;p&gt;首先我们随机生成一
      
    
    </summary>
    
      <category term="Code" scheme="https://jeffchy.github.io/categories/Code/"/>
    
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="https://jeffchy.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>On the Dimensionality of Word Embedding - NeuralPS2018 简要note</title>
    <link href="https://jeffchy.github.io/2018/12/14/On-the-Dimensionality-of-Word-Embedding-NeuralPS2018-%E7%AE%80%E8%A6%81note/"/>
    <id>https://jeffchy.github.io/2018/12/14/On-the-Dimensionality-of-Word-Embedding-NeuralPS2018-简要note/</id>
    <published>2018-12-14T12:23:41.000Z</published>
    <updated>2018-12-14T15:20:38.001Z</updated>
    
    <content type="html"><![CDATA[<p>今天简单说一下关于word embedding的一篇论文的大意, <a href="https://arxiv.org/abs/1812.04224" target="_blank" rel="external">https://arxiv.org/abs/1812.04224</a> ,<br>‘On the Dimensionality of Word Embedding’,这是一篇被NeurIPS 2018接受的Oral Paper。文章说了一件核心的事情，通过引入”Pairwise Inner Product(PIP) loss”,我们如何去将word embedding的dimension的selection变成一个bias-variance trade-off的问题，使得我们能够更好地找到最好的dimension。</p><h1 id="Introduction-and-Background"><a href="#Introduction-and-Background" class="headerlink" title="Introduction and Background"></a>Introduction and Background</h1><h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>我们通常把Word Embedding的方法大致分为两种，一种是基于矩阵分解的方法(LSA)，一种是基于神经网络prediction的方法(google word2vec)<br>word embedding我不太想过多的介绍了，其中最有名的是2013年google的word2vec,我相信大家都熟知。这里提一下基于矩阵分解的方法，比如利用SVD分解PMI矩阵。</p><script type="math/tex; mode=display">PIM_{i,j} = log{\frac{p(v_i, v_j)}{p(v_i)p(v_j)}}</script><p>let $PMI = U \Sigma V^T$ (SVD分解)，一个k维度的embedding被证明可以这样产生，</p><script type="math/tex; mode=display">E = U_{1:k} D^{\alpha}_{1:k,1:k},\ \ \  \alpha \in [0,1 ], \text{the power}</script><p>其中$p(v_i)$代表这个单词在Count(w, context)表中出现的总次数，如下图所示：<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/431911.jpg" alt=""><br>有名的glove和Word2vec,比如(Skipgram)方法也被证明过是隐式的matrix factorization.</p><p>这方面如果想要了解更多的内容我推荐</p><ul><li><a href="http://web.stanford.edu/~jurafsky/li15/lec3.vector.pdf" target="_blank" rel="external">http://web.stanford.edu/~jurafsky/li15/lec3.vector.pdf</a> [Stanford 2015 LSA 311 Dan Jurafsky lecture slices 3][EN]</li><li><a href="http://www.shuang0420.com/2017/03/21/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/" target="_blank" rel="external">http://www.shuang0420.com/2017/03/21/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/</a> [真的很棒的中文博客][ZH]</li><li>当然还要相关论文</li></ul><h2 id="Word-Embedding-is-unitary-invariance"><a href="#Word-Embedding-is-unitary-invariance" class="headerlink" title="Word Embedding is unitary invariance"></a>Word Embedding is unitary invariance</h2><p>Unitary Invariance的意思是，一个Word的embedding都乘上一个unitary matrix（比如旋转矩阵）得到了一个新的embedding空间，那么他等于没有改变。（你学习到的embedding整个空间旋转一下他的相对位置以及包含的信息还是一样的不是嘛）<br>Unitary Matrix的定义也很简单：<br>$UU^T=U^TU=I$</p><h1 id="PIP-loss"><a href="#PIP-loss" class="headerlink" title="PIP loss"></a>PIP loss</h1><p>我们想要evaluate我们learned embedding是不是已经足够好，必须要有一个loss function，而他的input应该是一个trained的Embedding Matrix和一个’gold’ Embedding Matrix。所以作者提出了一个<strong>符合Word embedding Unitary Invariance Property</strong>的loss function，如图所示，function和PIP matrix非常简单：<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/49009649.jpg" alt=""><br>这个loss function表示了，两个Embedding空间中的向量们之间的相对位置信息的不同(relative position shifts),同时如果两个Embedding $E_2 = E_1 U$, $E_2 E_2^T = (E_1 U)(U^T E_1^T)=E_1 E_1^T$</p><h1 id="开始推导了"><a href="#开始推导了" class="headerlink" title="开始推导了"></a>开始推导了</h1><p>首先一个lemma吗，后面要用<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/69956709.jpg" alt=""><br>然后对于$\alpha=0$的special case，同时我们假设我们的有一个Oracle embedding （global 语言信息得到一个合理的sigmal matrix [e.g. PMI]）然后根据SVD计算出来的Embedding Matrix, 和一个trained的embedding (根据training set统计出来的Signal Matrix计算出来的Embedding)，同时这两个embedding所有的column是orthonormal的，（内积等于1），我们能能将他们的loss写成bias variance trade off的形式<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/4476990.jpg" alt=""><br>其中的$d-k$代表了近似产生的信息的丢失，会随着$k$的增大而变小，(bias)最后面一项会随着$k$的增加而变大，因为“the noise perturbs the subspace spanned by E”（Matrix Perturbation Theory）(variance)</p><h2 id="更加general的版本："><a href="#更加general的版本：" class="headerlink" title="更加general的版本："></a>更加general的版本：</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/3495160.jpg" alt=""></p><h1 id="如何选择dimension"><a href="#如何选择dimension" class="headerlink" title="如何选择dimension"></a>如何选择dimension</h1><p>作者用蒙特卡洛方法生成模拟signal matrix M以及noise matrix $\hat{M} = M + Z$,具体的方法是：其实我还是不太明白具体的方法….<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/63678809.jpg" alt=""></p><h1 id="一些实验结果"><a href="#一些实验结果" class="headerlink" title="一些实验结果"></a>一些实验结果</h1><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-12-14/30624435.jpg" alt=""></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这篇文章感觉还是很牛逼的。。。毕竟是NeurIPS的oral，基本上就是，作者用了个合理的loss建模了什么是好的word embedding，同时在合理的设定下这个loss写成了bias-variance trade off 的形式，它表示出来的bias/variance项也intuitively make sense.这可以说给了一个很不错的理论框架。作者还通过蒙特卡洛方法去估计了不同的方法的最优embedding dimension，总的来说是特别有意义的工作。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天简单说一下关于word embedding的一篇论文的大意, &lt;a href=&quot;https://arxiv.org/abs/1812.04224&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1812.042
      
    
    </summary>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/Paper/"/>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/Paper/NLP/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/Paper/NLP/Note/"/>
    
    
      <category term="Probability Theory" scheme="https://jeffchy.github.io/tags/Probability-Theory/"/>
    
      <category term="Algorithm" scheme="https://jeffchy.github.io/tags/Algorithm/"/>
    
      <category term="Natual Language Processing" scheme="https://jeffchy.github.io/tags/Natual-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>Using Self-organizing map on 1-D data input</title>
    <link href="https://jeffchy.github.io/2018/11/28/Using-Self-organiziing-map-on-1-D-data-input/"/>
    <id>https://jeffchy.github.io/2018/11/28/Using-Self-organiziing-map-on-1-D-data-input/</id>
    <published>2018-11-28T06:57:48.000Z</published>
    <updated>2018-11-30T08:40:54.752Z</updated>
    
    <content type="html"><![CDATA[<p>自组织映射是一种有效的无监督学习的方法，<a href="https://en.wikipedia.org/wiki/Self-organizing_map，" target="_blank" rel="external">https://en.wikipedia.org/wiki/Self-organizing_map，</a> 利用竞争式得学习方法来将（通常是）高维数据映射到低维数据上，可以看成是一种dimension reduction，也可以具体用在聚类任务上。</p><p>举个例子，我们将有三个feature的数据$x^i = [x_1, x_2, x_3]$，训练出一个有着$k$个神经元的SOM，着每</p><p>个$Neuron_k$可以认为是数据的prototype。SOM的好处是他学出的表示能够保留数据的拓扑结构,(preserve the topology of original data),如图所示, SOM(grid)被慢慢拉近到贴合到数据的分布上。<img src="http://r.photo.store.qq.com/psb?/V13VpI7R48odcs/ShG2lMS2nEd2mA*UnY4ERgA9kmWIBPbSq.ZiS*aKOSA!/r/dPMAAAAAAAAA" alt="SOM"></p><p>具体的算法这里不多介绍了，推荐的资源：</p><p>[ZH] <a href="https://www.cnblogs.com/surfzjy/p/7944454.html" target="_blank" rel="external">https://www.cnblogs.com/surfzjy/p/7944454.html</a></p><p>[EN] <a href="https://en.wikipedia.org/wiki/Self-organizing_map" target="_blank" rel="external">https://en.wikipedia.org/wiki/Self-organizing_map</a></p><p>这里主要介绍一个小的例子：找出最具代表性的“数字”。</p><p>我们使用开源的Python Package: minisom <a href="https://github.com/JustGlowing/minisom" target="_blank" rel="external">https://github.com/JustGlowing/minisom</a> ， 首先安装这个package。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install minisom</div></pre></td></tr></table></figure><p>我们这里的输入时N个数字，我们希望能找到最具代表性的k个数字prototype。</p><p>我们的input是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">data = [[ <span class="number">1</span>],</div><div class="line">        [ <span class="number">2</span>],</div><div class="line">        [ <span class="number">3</span>],</div><div class="line">        [ <span class="number">14</span>],</div><div class="line">        [ <span class="number">1114</span>],</div><div class="line">        [ <span class="number">3</span>],</div><div class="line">        [ <span class="number">33</span>],</div><div class="line">        [ <span class="number">0.77</span>],</div><div class="line">        [ <span class="number">0.07</span>],</div><div class="line">        [<span class="number">-1</span>],</div><div class="line">        [<span class="number">-1000000</span>]]</div></pre></td></tr></table></figure><p>由于minisom这个包讲所有的输入数据做了一次normalize，还是按照行做的，所以如果我们的数据是一维的话，我们所有的数据都会被normalize成1，所以我们不能直接使用原来minisom，我们需要将【所有】如下normalize的代码都注释掉。(和导师聊过，我们认为这里的normalize在我们这个问题上是不需要的<a href="https://stackoverflow.com/questions/13687256/is-it-right-to-normalize-data-and-or-weight-vectors-in-a-som)，我么可以直接copy源代码的minisom类，然后将所有normalize的行都注释掉：）" target="_blank" rel="external">https://stackoverflow.com/questions/13687256/is-it-right-to-normalize-data-and-or-weight-vectors-in-a-som)，我么可以直接copy源代码的minisom类，然后将所有normalize的行都注释掉：）</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self._weights[i, j] = self._weights[i, j] / norm</div></pre></td></tr></table></figure><p>接下来我们就可以操作了,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> som <span class="keyword">import</span> MiniSom <span class="keyword">as</span> SOM</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> sys</div></pre></td></tr></table></figure><p>我们构建我们的SOM，我们选择使用一维排布的神经元，5x1。这里其实二维的也是ok的，这个depends on我们输入数据的形式，对于这个问题，我们的数据都在数轴上，那么其实一维的神经元排布更加能反应原本的拓扑结构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">som = SOM(<span class="number">5</span>, <span class="number">1</span>, <span class="number">1</span>, sigma=<span class="number">0.03</span>, learning_rate=<span class="number">0.3</span>) <span class="comment"># initialization of 6x6 SOM</span></div><div class="line">print(<span class="string">"Training..."</span>)</div><div class="line"><span class="comment"># som.random_weights_init(data)</span></div><div class="line">som.train_random(data, <span class="number">1000</span>) <span class="comment"># trains the SOM with 100 iterations</span></div><div class="line">print(<span class="string">"...ready!"</span>)</div></pre></td></tr></table></figure><p>然后很快我们就能得到我们SOM的结果了,我们的数据被分成了5个类别，大家看基本是按照量级区分开了，同时我们weights vector就是我们最具有代表性的几个数字！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">42</span>]:</div><div class="line">som.win_map(data)</div><div class="line">Out[<span class="number">42</span>]:</div><div class="line">defaultdict(list,</div><div class="line">            &#123;(<span class="number">0</span>, <span class="number">0</span>): [[<span class="number">-1000000</span>]],</div><div class="line">             (<span class="number">1</span>, <span class="number">0</span>): [[<span class="number">1114</span>]],</div><div class="line">             (<span class="number">2</span>, <span class="number">0</span>): [[<span class="number">33</span>]],</div><div class="line">             (<span class="number">3</span>, <span class="number">0</span>): [[<span class="number">14</span>]],</div><div class="line">             (<span class="number">4</span>, <span class="number">0</span>): [[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>], [<span class="number">3</span>], [<span class="number">0.77</span>], [<span class="number">0.07</span>], [<span class="number">-1</span>]]&#125;)</div><div class="line">In [<span class="number">43</span>]:</div><div class="line">som.get_weights()</div><div class="line">Out[<span class="number">43</span>]:</div><div class="line">array([[[<span class="number">-9.99999897e+05</span>]],</div><div class="line"></div><div class="line">       [[ <span class="number">1.11399996e+03</span>]],</div><div class="line"></div><div class="line">       [[ <span class="number">3.29995908e+01</span>]],</div><div class="line"></div><div class="line">       [[ <span class="number">1.39999492e+01</span>]],</div><div class="line"></div><div class="line">       [[ <span class="number">1.22937802e+00</span>]]])</div></pre></td></tr></table></figure><p>对于更多的例子大家可以看minisom的example。</p><p>这次blog主要是想传达，</p><ol><li>我们不需要每次都normalize我们SOM的weights。</li><li>1D的数据作为SOM的输入也是make sense的。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;自组织映射是一种有效的无监督学习的方法，&lt;a href=&quot;https://en.wikipedia.org/wiki/Self-organizing_map，&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://en.wikipedia.org/
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="ML" scheme="https://jeffchy.github.io/categories/NLP/ML/"/>
    
      <category term="Code" scheme="https://jeffchy.github.io/categories/NLP/ML/Code/"/>
    
    
      <category term="Algorithm" scheme="https://jeffchy.github.io/tags/Algorithm/"/>
    
      <category term="Natual Language Processing" scheme="https://jeffchy.github.io/tags/Natual-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>take-away tips from a Deep Learning Midterm Project</title>
    <link href="https://jeffchy.github.io/2018/11/13/take-away-tips-from-a-Deep-Learning-Midterm-Project/"/>
    <id>https://jeffchy.github.io/2018/11/13/take-away-tips-from-a-Deep-Learning-Midterm-Project/</id>
    <published>2018-11-13T06:27:54.000Z</published>
    <updated>2018-11-13T06:53:08.299Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要说一下深度学习其中项目的一些tips.深度学习课的期中作业是用pytorch复现一篇比较简单的论文,网络很简单:<a href="https://arxiv.org/abs/1412.6806" target="_blank" rel="external">https://arxiv.org/abs/1412.6806</a> ALLCNN,我的代码暂时还不能开源,accuracy还差一些,而且作业ddl还没有到,主要说一些训练期间感觉到的小tips</p><h2 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h2><ul><li>Adam的效果不一定是最好的, sgd + momentum + milestones + learning rate decay的灵活性和效果可能更好. </li><li>Adam的weight decay和sgd中的weight decay,在现有的框架实现中是不等效的,Adam可能会弱化weight decay regularization的效果</li><li>我们通常说, validation accuracy 和 test accuracy 是两回事,通常validation accuracy会偏高一些. 但是<strong>有的时候validation accuracy</strong>也并不能完全展现你的模型能力,举一个例子: cifar10上我一开始的train:validation split是9:1, 45000张图片进行训练, 5000张validate, 10000张test. 这样我训练的过程中 validation accuracy无论怎么调参都有些偏低. 除了参数可能还不够好以外,非常容易被忽略的原因是,我拿了十分之一的训练数据去做了validation,我训练数据变少了,我的模型accuracy自然会变差.事实证明,我只用1000张图片做validation,我的acc明面上提高了2-3个点.事实上,我们最后会重新拿所有的数据做一次训练.但是在之前训练的过程中,看到validation accuracy偏低的时候不要完全灰心,这部分数据如果也拿来训练的话也许模型效果会更好一些.</li><li>Initialization matters.我初始化conv layer的weights使用了Xavier初始化方法,一切都比较正常,而我有的同学没有用,他们的网络的loss很难下降,会卡住.之后我做实验的时候又试了一下据说对relu很友好的kaiming initialization,效果却差强人意,原因我没有细究,欢迎讨论.Initialization真的非常重要,和learning rate一样,有莫大的影响</li><li>关于设备. 我使用的是自己的1080 GPU,我同学使用的是集群K80,我的速度却比他快很多很多…在batch size比较小(显存占用比较小),吞吐量比较高的情况下,emmm不要盲目迷信GPU的价格,就像Cifar10的数据集,你完全没有必要用二十多GB的显存的K80…(我也不知道为啥我比他快那么多)</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章主要说一下深度学习其中项目的一些tips.深度学习课的期中作业是用pytorch复现一篇比较简单的论文,网络很简单:&lt;a href=&quot;https://arxiv.org/abs/1412.6806&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ht
      
    
    </summary>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/Note/"/>
    
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="https://jeffchy.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Marrying Up Regular Expression with Neural Network</title>
    <link href="https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/"/>
    <id>https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/</id>
    <published>2018-10-26T09:15:27.000Z</published>
    <updated>2019-01-17T13:36:22.296Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个简洁的论文笔记,关于这篇文章:发表于ACL18<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/47023549.jpg" alt=""></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>作者探讨了如何用正则表达式,提神神经网络模型的效果.</p><h1 id="tasks-Speaking-Language-Understanding"><a href="#tasks-Speaking-Language-Understanding" class="headerlink" title="tasks Speaking Language Understanding"></a>tasks Speaking Language Understanding</h1><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/72024242.jpg" alt=""></p><ul><li>一种是意图识别, intent classification, 其实就是一种text classification</li><li>还有一种是插槽, 其实就是一种sequence labeling任务</li></ul><h1 id="Base-Model"><a href="#Base-Model" class="headerlink" title="Base Model"></a>Base Model</h1><h2 id="Intent-classification"><a href="#Intent-classification" class="headerlink" title="Intent classification"></a>Intent classification</h2><p>作者使用BiLSTM+self-attention得到句子表示去当baseline.<br>self attention就是query和每个key进行相似度计算得到attention权重，然后用对应的hidden state然后接一个softmax去预测label.$W$是一个权重矩阵,$c$是一个可以训练的信息向量,用于分类的信息词,我的理解就是Q=c, K,V都等于hidden state, W是一个weight matrix<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/16459427.jpg" alt=""></p><h2 id="sequence-labeling"><a href="#sequence-labeling" class="headerlink" title="sequence labeling"></a>sequence labeling</h2><p>作者直接使用BiLSTM过一遍整个句子.</p><h1 id="如何apply-RE-我们主要拿intent-classification举例子"><a href="#如何apply-RE-我们主要拿intent-classification举例子" class="headerlink" title="如何apply RE 我们主要拿intent classification举例子"></a>如何apply RE 我们主要拿intent classification举例子</h1><p>对于Intent classification的任务:<br>举个具体例子,一句话</p><h2 id="Input-level-toy-example"><a href="#Input-level-toy-example" class="headerlink" title="Input level toy example"></a>Input level toy example</h2><p>比如数据中的label有: flights, airline<br>我们对这两个label分别设立他们的正则表达式,意思是,如果一个句子满足这个正则表达式,我们就认为这个句子应该属于这个label.<br>airline: RE: /list(the)?__AIRLINE/<br>flights: RE: /list(\w+){0,3} flights?/</p><p>现在我们碰到了一个句子:</p><blockquote><p>sentence: list the Delta airlines flights to Miami<br>label: flights</p></blockquote><p>我们会先对上面两个airline, flights的RE跑一下,我们发现都匹配了!<br>所以我们给他们打上了airline, flights两个REtags,我们对所有的REtags有一个可以训练的embedding,所以我们把两个tag做一下平均,得到一个aggregate embedding,就是我们symbolic rules产生的额外信息,于是我们可以把这个向量加入到它最后的soft Max表示那里,做预测.<br>如图中的(1)<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/18645714.jpg" alt=""><br>作者之前还试过我们直接把这个embedding和每个词的embedding结合起来,结果发现,这些tag会重复很多次,所以最终performance不好.</p><h2 id="NN-module-Network-level"><a href="#NN-module-Network-level" class="headerlink" title="NN module Network level"></a>NN module Network level</h2><p>在神经网络的层面,我们可以利用我们得到的tags来指导attention.<br>attention的式子如图所示<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/48204053.jpg" alt=""><br>这里面$h_i$就是BiLSTM对每个word的隐藏层.alpha是每个位置的attention weights, c是一个可以训练的,用于分类的信息词.<br>self attention就是query和每个key进行相似度计算得到attention权重，然后用对应的hidden state然后接一个softmax去预测label.W是一个权重矩阵,c是一个可以训练的信息向量,用于分类的信息词,我的理解就是Q=c, K,V都等于hidden state, W是一个weight matrix</p><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/61121072.jpg" alt=""><br>直觉上来说,我们对一个句子针对flights这个label的规则成功匹配了flights from这个表达式,那么也就意味着(flights from)这两个词应该对最终的模型预测结果有更多的贡献, 为了区别不同的intents, 我们给每个intent label k一个不同的attention, 于是我们可以得到k个s矩阵,</p><ul><li>然后我们可以分别计算每个s的一个分数,组成logits,然后用一个softmax分类器去得到不同的intent label的概率,这个叫做aggregate attention<br>(我的理解是原先softmax是把s矩阵的维度映射成k维,现在相当于有k个s矩阵,然后都有一个向量w映射成一维度的,最后组成k维)</li><li>然后我们加上一个attention loss去指导这个attention,其中$t_{ki}$为0,如果没有matching的RE,等于$\frac{1}{l_k}$如果matching word的长度是lk.</li><li>我们会发现,然后我们利用这个loss去指导attention weights和推理出的matching word保持某一种一致性(我的理解是,越大的attention weights, 这个式子就会越接近0,其实我觉得应该有个负号)<br>然后我们把这个loss和之前的classification loss加起来就行了<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/22919727.jpg" alt=""><h2 id="Output-level"><a href="#Output-level" class="headerlink" title="Output level"></a>Output level</h2><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/91104759.jpg" alt=""><br>同样规则可以在输出层影响NN.<br>之前我们aggregate attention计算logic的结果可以进一步被规则指导.<br>这里$w$是trainable的, $z_k$是一个indicator,1代表有matching,0代表没有,$w_k$是trainable的weights,直接在logits这一层也可以被rules影响.直觉上来说,如果有被matching,那么我就提高这一类别的分数(为什么不直接在softmax之后成为概率之后操作?也是可以,但是效果不好,因为logits是unconstrained real value)</li></ul><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>文章用了这个数据集,发现在小的数据结果下,效果是相当不错的,超过了baseline不少<br>(two就是它除了我们刚才说的attention以外又加上了negative的attention,我们刚才是希望匹配到的词鼓励attention weights, 可能也会有某些规则不鼓励 attention weights.)<br>(+pos/neg就是加了negative loss和positive loss)<br>mem是另一个模型.<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/50843081.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这是一个简洁的论文笔记,关于这篇文章:发表于ACL18&lt;br&gt;&lt;img src=&quot;http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/47023549.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Introd
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>深度好奇-神经规则引擎: Neural Rule Engine</title>
    <link href="https://jeffchy.github.io/2018/10/26/%E6%B7%B1%E5%BA%A6%E5%A5%BD%E5%A5%87-%E7%A5%9E%E7%BB%8F%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E-Neural-Rule-Engine/"/>
    <id>https://jeffchy.github.io/2018/10/26/深度好奇-神经规则引擎-Neural-Rule-Engine/</id>
    <published>2018-10-26T06:12:28.000Z</published>
    <updated>2018-10-27T16:54:22.800Z</updated>
    
    <content type="html"><![CDATA[<p>今天继续读一下深度好奇的系列工作: 神经规则引擎 Neural Rule Engine, 文章链接: <a href="https://arxiv.org/abs/1808.10326" target="_blank" rel="external">https://arxiv.org/abs/1808.10326</a><br>GENERALIZE SYMBOLIC KNOWLEDGE WITH NEURAL RULE ENGINE<br>之前的如果大家看了OONP或者微信公众号的话,也许能够先get到深度好奇工作的一些中心思想,可以看一下我之前的一篇博客 <a href="https://jeffchy.github.io/2018/10/24/OONP-Object-oriented-neural-programming/">https://jeffchy.github.io/2018/10/24/OONP-Object-oriented-neural-programming/</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>这篇文章的想法是,希望将symbolic rule 和 neural network 的优点结合起来.<br>(Symbolic rule -&gt; neural network ) = Better Performance<br>现有的很多工作是将符号的规则融入到神经网络中,但是这篇文章的主要思想是:</p><blockquote><p>learn knowledge explicitly from logic rules and then generalize them implicitly with neural networks</p></blockquote><p><strong>将符号推理学会的知识,用神经网络使得他们有更好地泛化到任务上</strong></p><p>NRE使用了Neural Module Network实现,每个模块表示了一种逻辑规则的实施.<br>NRE可以极大地提升召回率</p><h1 id="一些背景-主要是我挑选总结"><a href="#一些背景-主要是我挑选总结" class="headerlink" title="一些背景(主要是我挑选总结)"></a>一些背景(主要是我挑选总结)</h1><p>符号与神经网络的结合有很多很多种方式,我简单看了几个模型,希望能给大家一些intuition上的warm up.</p><h2 id="Teacher-teach-logic-to-student"><a href="#Teacher-teach-logic-to-student" class="headerlink" title="Teacher teach logic to student"></a>Teacher teach logic to student</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/28236079.jpg" alt=""><br>一种方法,用一个teacher student的framework,将teacher模型学到的逻辑迁移到学生身上.<br>每次迭代将student network投影到一个被规则限制了的子空间上,优化得到teacher network (soft FOL+PR) Student network在模仿teacher network还是去预测正确的label之间做权衡,最终训练得到student network</p><h2 id="OONP-symbolic-computed-result-as-input-of-NN-after-embedding"><a href="#OONP-symbolic-computed-result-as-input-of-NN-after-embedding" class="headerlink" title="OONP: symbolic computed result as input of NN (after embedding)"></a>OONP: symbolic computed result as input of NN (after embedding)</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/79743020.jpg" alt=""><br>这个是之前我们介绍的OONP,他构造了一个非常复杂的规则和神经网络结合的系统,大家应该还记得,我们拿其中一个部分举例子:<br>我们知道它的object memory里面除了这种symbolic的符号化的本体图表示以外,还会同时有向量化的表示.<br>当我们顺序地去读文本的时候,我们是会根据符号的逻辑来决定使用什么action的,比如生成一个叫做Name属性是Jeff的人类object,同时我们也会用他附近的上下文去生成一个向量表示,这个就相当于经过符号逻辑计算以后的输出,经过嵌入成为了神经网络的输入.<br>这也是一个例子</p><h2 id="Marrying-up-regular-expressions-with-NN"><a href="#Marrying-up-regular-expressions-with-NN" class="headerlink" title="Marrying up regular expressions with NN"></a>Marrying up regular expressions with NN</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-26/47023549.jpg" alt=""><br>这个工作是用正则表达式去提升神经网络:),正则表达式其实就是一种规则,正则的matching就是规则的matching. 利用规则,我们可以去计算出额外的信息,嵌入网络作为输入,也可以做用在网络一些模块中,也可以在网络的输出层做手脚,详细可以看我的另外一篇笔记: <a href="https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/">https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/</a> 这篇工作也是ACL18上的</p><h2 id="暂时性总结一下"><a href="#暂时性总结一下" class="headerlink" title="暂时性总结一下"></a>暂时性总结一下</h2><p>我们可以看到,上面列举的很多工作都是: </p><ul><li>利用rule计算出来的结果去提升神经网络的结果.<br>比如,用符号运算计算出一些额外信息,然后嵌入之后输入神经网络,比如刚才说的正则表达式的工作,还有OONP的一个部分</li><li>或者用这种结果去影响神经网络的一些模块,比如attention</li><li>或者用规则去修改神经网络输出层的结果.<ul><li>之前Johnson的文章也是一种直接用规则去修改神经网络输出结果的方法,实际上任意的正则表达式都可以表示为一种有限状态机FSM</li><li><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/36579701.jpg" alt=""></li></ul></li><li>或者是Teacher student的framework<br>本质都是 <strong>利用制定规则去得到一些信息,目的是提升神经网络的效果</strong></li></ul><h1 id="正篇Neural-Rule-Engine"><a href="#正篇Neural-Rule-Engine" class="headerlink" title="正篇Neural Rule Engine"></a>正篇Neural Rule Engine</h1><p>反其道而行之,它希望用神经网络来提高规则的泛化能力</p><h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>神经规则引擎把规则(正则表达式RE)转化成有符号化结构的模块神经网络,具体的步骤主要是两步</p><ul><li>我们会定义很多action, 这些actions都是由neural/symbolic的模块来实现,每个action都是某一个算法或者模型,比如说是一个神经网络,或者是一个symbolic的算法</li><li>我们会将正则表达式 Parse成一个action tree, 具体的Parse可以用Symbolic的或者是神经网络的parser来预测, 可以理解为我们定义了很多模块,然后我们解析规则去把他们组织起来</li></ul><p>总体来说是这样一个框架</p><h2 id="RE"><a href="#RE" class="headerlink" title="RE"></a>RE</h2><p>我们看一下它的正则表达式,其实和前面的那篇文章是一模一样的框架<br><a href="https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/">https://jeffchy.github.io/2018/10/26/Marrying-Up-Regular-Expression-with-Neural-Network/</a><br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/50443767.jpg" alt=""><br>比如说,我们要做classification的任务(上半部分):<br>某一个label是[尾随作案]<br>我们定义positive和negative的正则表达式,比如所如果匹配到跟在谁谁后面这种,一般会倾向认为是认为是尾随作案,但是如果事主也在跟随的话那就不是.</p><h2 id="Actions和action-tree"><a href="#Actions和action-tree" class="headerlink" title="Actions和action tree"></a>Actions和action tree</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-27/58540341.jpg" alt=""><br>这是他主要的action,他们认为这6种action就能表示绝大部分他们用正则表达式定义的规则.比如下面的例子:<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-28/11620217.jpg" alt=""></p><h2 id="Action-tree的表示-Reverse-Polish-Notation"><a href="#Action-tree的表示-Reverse-Polish-Notation" class="headerlink" title="Action tree的表示 Reverse Polish Notation"></a>Action tree的表示 Reverse Polish Notation</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-28/68006179.jpg" alt=""><br>Reverse Polish Notation(RPN)其实就是后序遍历:)</p><h1 id="用Seq2seq-rule-parser"><a href="#用Seq2seq-rule-parser" class="headerlink" title="用Seq2seq rule parser"></a>用Seq2seq rule parser</h1><p>了解了我们的rules还有action是什么以后,我们来看我们这个框架的第一部分,rule parser, 是一个seq2seq的结构:), 我们将我们的规则输入进模型中,然后用3层BiLSTM+1层LSTM做encoder,然后做两个阶段的decoder,先decoder actions,然后encode这些actions的类别,decode出这些action传入的参数.这样子做的原因是他们实验发现同时predict action和他们的参数太困难了,很难训练,如是采取了这样层级的结构<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-28/46786537.jpg" alt=""><br>然后优化目标是cross entropy loss, 他们还用了一个trick去加速训练,因为word vectors是fix的,所以他们不是去预测正确的word的id,而是让模型直接去预测正确的word vector<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-28/43490760.jpg" alt=""></p><h2 id="他们的baseline"><a href="#他们的baseline" class="headerlink" title="他们的baseline"></a>他们的baseline</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-28/98245107.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天继续读一下深度好奇的系列工作: 神经规则引擎 Neural Rule Engine, 文章链接: &lt;a href=&quot;https://arxiv.org/abs/1808.10326&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxi
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Natual Language Processing" scheme="https://jeffchy.github.io/tags/Natual-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>OONP-Object oriented neural programming</title>
    <link href="https://jeffchy.github.io/2018/10/24/OONP-Object-oriented-neural-programming/"/>
    <id>https://jeffchy.github.io/2018/10/24/OONP-Object-oriented-neural-programming/</id>
    <published>2018-10-24T13:38:01.000Z</published>
    <updated>2018-10-25T14:32:01.730Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Object-Oriented-Neural-Programming"><a href="#Object-Oriented-Neural-Programming" class="headerlink" title="Object Oriented Neural Programming"></a>Object Oriented Neural Programming</h1><p>这一篇是深度好奇 <a href="http://deeplycurious.ai/#/" target="_blank" rel="external">http://deeplycurious.ai/#/</a> 发表在ACL18上的工作, 意在将符号逻辑(Symbolism)和深度学习神经网络(connectionism)结合起来,利用它们各自的优点,解决自然语言理解中遇到的困呐:</p><ul><li>长距离的逻辑依赖难以有效学习到</li><li>自然语言的模糊性</li><li>对常识(common sense)的依赖</li><li>语义表示的形式(我们如何用一个结构很好的表示语义(semantic parsing))<br>[文章前半部分参考深度好奇微信公众号文章<a href="https://mp.weixin.qq.com/s/MqcL0272MVA0aXKy4nQTxQ" target="_blank" rel="external">https://mp.weixin.qq.com/s/MqcL0272MVA0aXKy4nQTxQ</a>]</li></ul><h1 id="神经网络和符号主义的优劣势"><a href="#神经网络和符号主义的优劣势" class="headerlink" title="神经网络和符号主义的优劣势"></a>神经网络和符号主义的优劣势</h1><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-24/64258485.jpg" alt=""></p><h1 id="如何将神经系统和符号只能进行融合"><a href="#如何将神经系统和符号只能进行融合" class="headerlink" title="如何将神经系统和符号只能进行融合"></a>如何将神经系统和符号只能进行融合</h1><ul><li><p>原则I 生成神经和符号的链接<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-24/57721035.jpg" alt=""><br>神经网络最终会输出离散的符号表示,经过符号推理得到的符号嵌入作为输入,进一步指导神经网络的下一步操作.</p></li><li><p>原则II 形成神经符号的并列和对应<br>并列是指两条符号和神经网络的计算通路,之间有着密切的信息交换,鼓励两条通路的一致性进行训练.<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-24/61048772.jpg" alt=""><br>对应是指离散的规则表示和参数知识可以互相转化,利用各自的优势来选择神经还是符号.(Neural Rule Engine)<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-24/38523962.jpg" alt=""></p></li><li><p>原则III 中央调控机制去选择控制、规划,来确定使用 神经、符号、神经+符号<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-24/92410082.jpg" alt=""></p></li></ul><h1 id="OONP-利用上述中心思想的-面向自然语言理解问题的新框架"><a href="#OONP-利用上述中心思想的-面向自然语言理解问题的新框架" class="headerlink" title="OONP 利用上述中心思想的,面向自然语言理解问题的新框架."></a>OONP 利用上述中心思想的,面向自然语言理解问题的新框架.</h1><ul><li>OONP的目的训练一个semantic的parser使得: document-&gt;predesigned data structure </li><li>可以用监督学习、强化学习、或者两种的融合去做</li><li>一个例子OONP运用了面向对象的思路,每个方框是一个类别的instance,每个颜色代表了一个类别,对象间同时存在一些link,来表示他们之间的关系,这里又有点像关系型数据库,这些关系(relation),class,class_attribute,都是我们事先根据任务设置好的</li><li><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/61028456.jpg" alt=""></li></ul><h1 id="Details-of-OONP"><a href="#Details-of-OONP" class="headerlink" title="Details of OONP"></a>Details of OONP</h1><h2 id="OONP-Overview"><a href="#OONP-Overview" class="headerlink" title="OONP Overview"></a>OONP Overview</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/88730549.jpg" alt=""><br>D表示词语的distributed表示(word vectors), S表示逻辑symbol的表示,暂时没有给例子,我觉得OONP和神经图灵机很相似,都是模拟某种读取、处理(计算)然后得到输出的方法.<a href="https://www.jiqizhixin.com/articles/2017-04-11-7" target="_blank" rel="external">https://www.jiqizhixin.com/articles/2017-04-11-7</a><br>OONP主要分为三个部分:</p><ul><li>Inline Memory 原始文本经过一些预处理的步骤扔进Inline Memory,就类似于一个处理的buffer,不带有什么结构信息,就是一个序列</li><li>Reader 读取器从头到尾去读取|写入这个inline memory,有可能不止读一次,通过交互去利用inline memory的信息去更新(读取|写入)Carry-On Memory的信息.<ul><li>Reader相当于OONP的控制中心, 包括一个Neural Net Controller(包含一个policy net),这里的控制系统其实和神经图灵机NTM的控制器有一些相似,决定是否写入读取Matrix Memory,inline memory,同时, Policy Net可以给出当前时间合适的action,用来更新object memory的结构,也可以更新inline memory,最终会加入到action history中,后面会举具体的例子, </li><li>同时reader中间还有三个symbolic processors,分别是symbolic matching, symb olic reasoner, symbolic analyzer, 可以利用Object Memory, inline memory, action history, policy net中symbolic的表示来计算,我们之后得看看具体的例子.</li><li><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/78964408.jpg" alt=""></li></ul></li><li>Carry On Memory - 这个memory是一个reader从inline memory中读取到的信息的聚合以及记忆总结,包括了:<ul><li>Object Memory, 这个表示是distributed representation或者是symbolic的,后面会详细说,他保存了本体图的带有结构化的信息,可以理解为就是本体图的结构表示</li><li>Matrix Memory, 是differentiable的,一个例子就是RNN的hidden state,他表示的是知识是不确定的,不完整的(需要后续知识的)</li><li>Action History, 记录了所有parsing过程中的Action,这个是symbolic的,举个例子比如说:First Order Logic</li></ul></li></ul><h2 id="Details-of-Object-Momory"><a href="#Details-of-Object-Momory" class="headerlink" title="Details of Object Momory"></a>Details of Object Momory</h2><h3 id="Symbolic-part-Ontology-class-object-class-attributes-relations"><a href="#Symbolic-part-Ontology-class-object-class-attributes-relations" class="headerlink" title="Symbolic part: Ontology (class+object+class attributes+relations)"></a>Symbolic part: Ontology (class+object+class attributes+relations)</h3><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/72257414.jpg" alt=""></p><h3 id="Distributed-Representation-object-embedding"><a href="#Distributed-Representation-object-embedding" class="headerlink" title="Distributed Representation: object embedding"></a>Distributed Representation: object embedding</h3><p>对于每一个对象,除了一个符号化的图表示以外,还有一个对应的分布表示,目的是为了能够与reader中从matrix memory以及Inline memnory中的分布表示进行交互.举个例子,对于一句非常简单的话:</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><blockquote><p>Jeff is handsome, he has a cute cat named MaoMao </p></blockquote><p>我们建立了两个object: 一个是Jeff,一个是MaoMao,他们有各自的属性,分数PERSON类和ANIMAL类,他们之间的关系是Pet of.所以我们对他们有一个框框表示,也就是symbolic的表示<br>除此之外,我们会也会保存一个对应的分布表示,比如说对于Object: Jeff,来说,他的信息以及关系的建立与“Jeff is handsome, he has a cute cat named MaoMao”这整个句子有关,所以我们举个例子,把这个句子的单词换成word vectors然后过了个LSTM获得隐藏表示$h_t$我们把这个作为他的distributed representation,也就是所谓的object embedding!</p><h3 id="整个Object-Memory就由所有Object的一个Symbolic-Graph-Representation-和每个object对应的-distributed-object-embedding-表示"><a href="#整个Object-Memory就由所有Object的一个Symbolic-Graph-Representation-和每个object对应的-distributed-object-embedding-表示" class="headerlink" title="整个Object Memory就由所有Object的一个Symbolic Graph Representation 和每个object对应的 distributed object embedding 表示"></a>整个Object Memory就由所有Object的一个Symbolic Graph Representation 和每个object对应的 distributed object embedding 表示</h3><h3 id="Parsing出ontology的两种模式"><a href="#Parsing出ontology的两种模式" class="headerlink" title="Parsing出ontology的两种模式"></a>Parsing出ontology的两种模式</h3><ul><li>stationary: 现在你parsing出来的本体图一定会是最终本体图的一个部分,你只是暂时缺少信息而已</li><li>dynamic: 现有的parsing出来的本体图可能会最终被改变<br>这两种模式的选择同时还依赖着本体ontology的定义.比如这句话,如果Person:Tom和Item:Car之间的关系是sell,那么就是stationary,如果是Ownership那么就是dynamic<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/52987697.jpg" alt=""></li></ul><h2 id="Details-of-Inline-Memory"><a href="#Details-of-Inline-Memory" class="headerlink" title="Details of Inline Memory"></a>Details of Inline Memory</h2><p>之前我们看到,Inline Memory就是一个buffer,内涵需要被处理的序列,可能包含了Distributed和Symbolic的表示,下面举一些例子,Inline Memory里面是什么.</p><ul><li>Distributed: Context Dependent Word Embeddings, Hidden state of NNs</li><li>Symbolic: 通过Sequence Labeling等方法,识别、归纳出的符号表示. 比如: Apple:fruit<br>Parsing的时候Reader也是可以去写入Inline Memory的,我们把这个过程叫做”Notes-taking”,如果是distributed的话,这个就等价于interactive attention,如果reader返回的信号是离散的,symbolic的,我们可以理解为输出了一个action动作.</li></ul><h2 id="Details-of-Reader"><a href="#Details-of-Reader" class="headerlink" title="Details of Reader"></a>Details of Reader</h2><p>Reasoner的结构我们之前说过了,我复制一波</p><ul><li>Reader 读取器从头到尾去读取|写入这个inline memory,有可能不止读一次,通过交互去利用inline memory的信息去更新(读取|写入)Carry-On Memory的信息.<ul><li>Reader相当于OONP的控制中心, 包括一个Neural Net Controller(包含一个policy net),这里的控制系统其实和神经图灵机NTM的控制器有一些相似,决定是否写入读取Matrix Memory,inline memory,同时, Policy Net可以给出当前时间合适的action,用来更新object memory的结构,也可以更新inline memory,最终会加入到action history中,后面会举具体的例子, </li><li>同时reader中间还有三个symbolic processors,分别是symbolic matching, symb olic reasoner, symbolic analyzer, 可以利用Object Memory, inline memory, action history, policy net中symbolic的表示来计算,我们之后得看看具体的例子.</li><li><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/78964408.jpg" alt=""></li></ul></li></ul><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/4775618.jpg" alt=""><br>对于t时刻来说reasoner的运作的步骤:</p><ul><li>Step1: Input: Action History -&gt; Symbolic Analyzer -&gt; 获得能指导后续action选择的特征.</li><li>Step2: $s_t=$NeuralNetController.get(MatrixMemory) 控制器从Matrix Memory中获得向量表示</li><li>Step3: $x_t^{(s)}=$NeuralNetController.get(InlineMemory.symbolic), $x_t^{(d)}=$NeuralNetController.get(InlineMemory.distributed)</li><li>Step4: NeuralNetController.fuse($s_t, x_t^{(d)}, x_t^{(s)}$)</li><li>Step5: 根据$x_t^{(s)}$去筛选object的candidate,这些candidate应该从Object Memory里面来?然后让这些candidate去meet $x_t^{(d)}$,应该是某个classifier训练出来之后,feed in $x_t^{(d)}$去选择candidates.</li><li>Step6: 根据$x_t^{(s)}$去筛选object的candidate,这些candidate应该从Object Memory里面来?然后让这些candidate去meet Step4的结果,和step5类似</li><li>Step7: 利用policy net去根据step6/step5的结果(应该是前面确定了object),去output出一个action</li><li>Step8: Apply action,然后更新Object Memory, Inline Memory, matrix Memory</li><li>Step9: 将Object Memory扔进Symbolic Reasoner 去做逻辑推断.得到一些暂时的结论,比如做一次information retrieval</li></ul><blockquote><p>着尼玛也太复杂了,搞我心态</p></blockquote><h2 id="OONP-Actions"><a href="#OONP-Actions" class="headerlink" title="OONP Actions"></a>OONP Actions</h2><p>我们来看看OONP有哪些actions.(被Policy-Net output出来的action)</p><ul><li>New-Assign: instantiate一个object,或者将手中的信息更新到某个已经又的object中</li><li>Update.X: 决定更新哪一个内部属性X</li><li>Update2what: 决定把这个内部属性更新成什么<br>经常会出现的: New-Assign一个某个类型的空object</li></ul><h3 id="New-assign有三种"><a href="#New-assign有三种" class="headerlink" title="New assign有三种"></a>New assign有三种</h3><ul><li>生成新的object $p(c, new|S_t)$ c是class类别</li><li>把现有的信息$S_t$填入已经有的object中 $p(c,k | S_t)$</li><li>什么都不做 $p(none | S_t)$<br>是否生成新的object主要由两个决定:</li><li>现有的信息$S_t$不能被已经有的object囊括</li><li>语法、语义的信息显示需要有新的object了<br>我们利用scores来计算所有的可能性:<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/21139338.jpg" alt=""><br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/72257414.jpg" alt=""><br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/8940273.jpg" alt=""><br>每一种颜色代表一个class的type.我们一共有6个object,所以会有六种可能$p(c, k|S_t)$,一共有三个类型,所以我们可能会产生三种新的object$p(c, new|S_t)$, 还有一种是什么都不做,所以一共考虑10种可能性.这些概率都是由policynet来决定</li></ul><h3 id="Updating-objects"><a href="#Updating-objects" class="headerlink" title="Updating objects"></a>Updating objects</h3><p>New Assign过后,Updata.X 会选择去update external link还是update internal property,然后Update2what会进一步决定如何更新</p><h3 id="关于action的例子"><a href="#关于action的例子" class="headerlink" title="关于action的例子"></a>关于action的例子</h3><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/42525372.jpg" alt=""><br>我们逐步建立了我们的ontology图,然后我们就可以用这个图进行推理、提取信息啦</p><h2 id="Symbolic扮演了很大的作用"><a href="#Symbolic扮演了很大的作用" class="headerlink" title="Symbolic扮演了很大的作用"></a>Symbolic扮演了很大的作用</h2><ul><li>利用分析action history给Reader提供信息比如:“The system just New an object with Person-class five words ago”-这个是Symbolic Analizer</li><li>利用object memory的符号表示:本体图来更新一些信息,比如 小红拿着苹果+小红去了厨房=》苹果也在厨房</li><li>之前提到的New Assign,我们决定使用是否新建对象的时候,需要比对现有的信息和object,这个比对需要symbolic的帮助下完成: Symbolic Matching</li></ul><h1 id="OONP-的-Learning"><a href="#OONP-的-Learning" class="headerlink" title="OONP 的 Learning"></a>OONP 的 Learning</h1><h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><p>参数是所有的distributed representation 以及后续需要处理它们产生的参数,标注“right action at each time step”,训练模型去maximize所有的这个decision的truth的likelihood<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/7911857.jpg" alt=""></p><h2 id="reinforcement-learning"><a href="#reinforcement-learning" class="headerlink" title="reinforcement learning"></a>reinforcement learning</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/46004951.jpg" alt=""><br>policy gradient的方法去学:<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/54569091.jpg" alt=""></p><h2 id="都用上"><a href="#都用上" class="headerlink" title="都用上"></a>都用上</h2><p>对于一些确定的任务,static ontology, 用SL不错<br>如果一些无法确定的任务,不能reverse engineer作为监督的我们需要用RL<br>combine 他们也非常简单<br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/58341177.jpg" alt=""></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="BABI"><a href="#BABI" class="headerlink" title="BABI"></a>BABI</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/60663467.jpg" alt=""></p><h2 id="Police-Report"><a href="#Police-Report" class="headerlink" title="Police Report"></a>Police Report</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/60153941.jpg" alt=""><br><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/54681811.jpg" alt=""></p><h2 id="Court-Judgement"><a href="#Court-Judgement" class="headerlink" title="Court Judgement"></a>Court Judgement</h2><p><img src="http://image-jeff.oss-cn-hangzhou.aliyuncs.com/18-10-25/11465547.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Object-Oriented-Neural-Programming&quot;&gt;&lt;a href=&quot;#Object-Oriented-Neural-Programming&quot; class=&quot;headerlink&quot; title=&quot;Object Oriented Neural P
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/NLP/Paper/Note/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>2018-10-17组会takeaway</title>
    <link href="https://jeffchy.github.io/2018/10/17/2018-10-17%E7%BB%84%E4%BC%9Atakeaway/"/>
    <id>https://jeffchy.github.io/2018/10/17/2018-10-17组会takeaway/</id>
    <published>2018-10-17T11:58:34.000Z</published>
    <updated>2018-10-17T12:42:59.883Z</updated>
    
    <content type="html"><![CDATA[<h1 id="极简takeaway"><a href="#极简takeaway" class="headerlink" title="极简takeaway"></a>极简takeaway</h1><h2 id="Multitask-Learning-的用法"><a href="#Multitask-Learning-的用法" class="headerlink" title="Multitask Learning 的用法"></a>Multitask Learning 的用法</h2><ul><li>Predict What You Want 比如NER中的name对OOV非常敏感,所以我们可以先predict是否是name,这也就force模型学习到识别name的信息,然后再分别做</li><li>Reverse What You Dislike, 比如你有一个任务,Domain Adaption,你想要转换到不同的domain,所以你不希望区分domain的特征,所以在这个任务上加了一个classifier但是梯度反过来<img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/20164286.jpg" alt=""></li><li>用作data agumentation的trick.<img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/55329766.jpg" alt=""></li><li>添加任务使得神经网络更难.比如NMT中额外加了个任务预测parse tree,发现提升了NMT的效果.</li></ul><h2 id="Constrained-Decoder"><a href="#Constrained-Decoder" class="headerlink" title="Constrained Decoder"></a>Constrained Decoder</h2><p>用有限状态机做decoder,利用别的更容易得到的信息做decoder,也算一种multitask,mark Johnson的<br>Guided Open Vocabulary Image Captiong with Constrained Beamn Search<br>还有Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;极简takeaway&quot;&gt;&lt;a href=&quot;#极简takeaway&quot; class=&quot;headerlink&quot; title=&quot;极简takeaway&quot;&gt;&lt;/a&gt;极简takeaway&lt;/h1&gt;&lt;h2 id=&quot;Multitask-Learning-的用法&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="Group Meeting" scheme="https://jeffchy.github.io/tags/Group-Meeting/"/>
    
  </entry>
  
  <entry>
    <title>Self Organizing Map</title>
    <link href="https://jeffchy.github.io/2018/10/17/Self-Orgnizing-Map/"/>
    <id>https://jeffchy.github.io/2018/10/17/Self-Orgnizing-Map/</id>
    <published>2018-10-17T05:22:27.000Z</published>
    <updated>2018-10-17T06:44:07.395Z</updated>
    
    <content type="html"><![CDATA[<p>今天我们来学习一下Self organizing map,自组织映射.本文主要先参考Youtube的一些tutorial<br><a href="https://www.youtube.com/watch?v=H9H6s-x-0YE" target="_blank" rel="external">https://www.youtube.com/watch?v=H9H6s-x-0YE</a> [How SOM Works]<br><a href="https://www.youtube.com/watch?v=_Euwc9fWBJw" target="_blank" rel="external">https://www.youtube.com/watch?v=_Euwc9fWBJw</a> [How SOM Learn]</p><h1 id="SOM的输入输出"><a href="#SOM的输入输出" class="headerlink" title="SOM的输入输出"></a>SOM的输入输出</h1><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/3748111.jpg" alt=""><br>如图所示:</p><ul><li>Input<ul><li>$X=x_1 \cdots x_n$,每个$x_i$是一个feature的向量,每个entry可以理解成是一个attribute</li><li>Neural的数目</li><li>Epoch</li><li>Learning Rate</li><li>因为是无监督的一种人工神经网络所以我们不需要labelY</li></ul></li><li>Output<ul><li>每个$x$的类别 </li></ul></li></ul><h1 id="SOM-如何学习"><a href="#SOM-如何学习" class="headerlink" title="SOM 如何学习"></a>SOM 如何学习</h1><h2 id="传统的人工神经网络"><a href="#传统的人工神经网络" class="headerlink" title="传统的人工神经网络"></a>传统的人工神经网络</h2><p>如下图,假设输入X有三个column(feature),传统的ANN的weights是乘上x的某个feature,然后最后加起来<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/81541287.jpg" alt=""><br>也就是$\sum_i w_ix_i$</p><h2 id="SOM有很大的区别"><a href="#SOM有很大的区别" class="headerlink" title="SOM有很大的区别"></a>SOM有很大的区别</h2><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/7001971.jpg" alt=""><br>SOM的weights不是乘上feature用的了,而变成了Node本身的attributes.可以理解为,一个Ghost Data Point!或者说想象出的datapoint.</p><h2 id="Ghosts-Node-Neurons-与每个具体的datapoint的欧式距离评估并且互相竞争"><a href="#Ghosts-Node-Neurons-与每个具体的datapoint的欧式距离评估并且互相竞争" class="headerlink" title="Ghosts Node (Neurons)与每个具体的datapoint的欧式距离评估并且互相竞争"></a>Ghosts Node (Neurons)与每个具体的datapoint的欧式距离评估并且互相竞争</h2><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/27314565.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/23398176.jpg" alt=""><br>距离最小的neuron我们将它称为BMU(best matching unit)</p><ul><li>首先对每个neuron initialize feature</li><li>给定某个input $x_i$,$X$的某一行(列为feature),我们找出那个BMU</li><li>然后我们更新这个BMU<strong>以及它附近的neuron</strong>的权值.使得这一部分点更贴近这个数据</li><li><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/91384068.jpg" alt="图from wikipedia"></li></ul><h2 id="如果有多个BMU-挨个儿drag"><a href="#如果有多个BMU-挨个儿drag" class="headerlink" title="如果有多个BMU,挨个儿drag"></a>如果有多个BMU,挨个儿drag</h2><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/40857344.jpg" alt=""><br>每次radius的大小会越来越小.最后会变成这样<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/29158314.jpg" alt=""></p><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/73266070.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天我们来学习一下Self organizing map,自组织映射.本文主要先参考Youtube的一些tutorial&lt;br&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=H9H6s-x-0YE&quot; target=&quot;_blank&quot; rel
      
    
    </summary>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/Note/"/>
    
    
      <category term="Algorithm" scheme="https://jeffchy.github.io/tags/Algorithm/"/>
    
      <category term="Machine Learning" scheme="https://jeffchy.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>如何更好地建模数字:ACL18 Evaluating and Improving their Ability to Predict Numbers</title>
    <link href="https://jeffchy.github.io/2018/10/16/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-ACL18-Evaluating-and-Improving-their-Ability-to-Predict-Numbers/"/>
    <id>https://jeffchy.github.io/2018/10/16/论文解读-ACL18-Evaluating-and-Improving-their-Ability-to-Predict-Numbers/</id>
    <published>2018-10-16T07:17:58.000Z</published>
    <updated>2018-10-16T16:01:34.025Z</updated>
    
    <content type="html"><![CDATA[<p>今天说一下这篇ACL18的文章<br><a href="https://arxiv.org/abs/1805.08154" target="_blank" rel="external">https://arxiv.org/abs/1805.08154</a><br>Numeracy for Language Models:<br>Evaluating and Improving their Ability to Predict Numbers</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="现在表示数字的方法不合理"><a href="#现在表示数字的方法不合理" class="headerlink" title="现在表示数字的方法不合理"></a>现在表示数字的方法不合理</h2><p>本文讨论了language model中如何更好地predict数字的问题.language model能够建模一个单词sequence在文本下出现的概率,如果语法越符合规范,越符合实际,那么这个sequence的概率应该越高.<br><strong>realistic and grammatical</strong>,现有的方法不能很好地建模数字:</p><ul><li>新的数字出现out of vocabulary的概率比单词要高很多,很多单词都会被归为UNKNOWN</li><li>现有很多方法,比如GloVe中的embedding,数字只是很少的一部分,只有3.79%左右,所以很多现有模型并没有考虑数字的影响,因为本身微弱.但是同时在有些数据集中,比如儿童杂志、诊所报告中,数字出现的非常多</li><li><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/48112826.jpg" alt=""><h2 id="本文的贡献"><a href="#本文的贡献" class="headerlink" title="本文的贡献"></a>本文的贡献</h2></li><li>本文主要探索并且提出了一些方案去建模数字,比如<ul><li>digit-by-digit composition</li><li>memorisation</li><li>提出了一个深度学习模型,基于连续Probabiligy density function</li></ul></li><li>本文提出了对out of vocabulary的问题的一些处理方法</li><li>本文在clinical/scientific数据集上跑实验,并且发现<ul><li>把数字和单词分开处理会提升LM的困惑度</li><li>不同的建模数字的strategy对不同的上下文环境(数据集)的效果是不同的</li><li>连续概率密度函数可以提升LM的预测准确度</li></ul></li></ul><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>其实主要就是language model.language model:<br>$X = x_1, x_2, \cdots, x_n$, 利用chain-rule: $P(x_1,x_2,\cdots,x_n)=\prod_{i=1}^n P(x_i|x_1,\cdots,x_{i-1})$, 通常我们的做法是学习一个embedding,$E=R^{|V| \times D}$,将每个出现过的单词(属于vocabulary),用一个$D$维的向量表示.然后每个词用一个onehot表示去提取这个向量,然后输入模型中.</p><h2 id="char-embedding"><a href="#char-embedding" class="headerlink" title="char embedding"></a>char embedding</h2><p>除了单词之外,用char做embedding可以capture每个单词的前缀后缀这样的信息.比如love,就可以通过char level的embedding,通常是过一个RNN $e^{chars}=RNN(l,o,v,e)$,用hidden layer来encode词的每个字母的信息(数字的每个位同理).<br>这个在本为中只针对nuneral!</p><h2 id="Output-of-RNN"><a href="#Output-of-RNN" class="headerlink" title="Output of RNN"></a>Output of RNN</h2><p>$p(s_t|h_t)=softmax(\phi(s_t))$, $\phi$是score function, $h_t$代表t时刻的hidden layer</p><h2 id="Training-and-Evaluation"><a href="#Training-and-Evaluation" class="headerlink" title="Training and Evaluation"></a>Training and Evaluation</h2><p>Training $H_{train}=-\frac{1}{N}\sum_{s \in train} logp(s_t | s_{&lt;t})$<br>Evaluation主要用困惑度,其实就是,我们希望我们的模型预测出test set中这句话的概率越大越好.<br>$exp(H_{test})$</p><h1 id="Modeling-the-Numerals"><a href="#Modeling-the-Numerals" class="headerlink" title="Modeling the Numerals."></a>Modeling the Numerals.</h1><h2 id="Softmax-Model-and-Variants"><a href="#Softmax-Model-and-Variants" class="headerlink" title="Softmax Model and Variants"></a>Softmax Model and Variants</h2><h3 id="Basic-softmax"><a href="#Basic-softmax" class="headerlink" title="Basic softmax"></a>Basic softmax</h3><p>也就是传统的方法.这里的score function $\phi(s_t)=h_t^T e_{s_t}^{token}=h_t^T E_{out} w_{s_t}$这里的$E_{out}$其实Embedding Matrix, 而$e_{s_t}$是其中的一行.所以这里的$E_{out} \in R^{D\times |V|}$.其实就是原始的方法,这种方法假设,你一开始就知道所有的词,并且存成vocabulary,因此你遇见没有见过的新的词或者新的数字的话,你得用special word, 比如’UNKword’,’UNKnumeral’来表示他们.</p><h3 id="Softmax-with-Digit-Based-Embeddings"><a href="#Softmax-with-Digit-Based-Embeddings" class="headerlink" title="Softmax with Digit-Based Embeddings"></a>Softmax with Digit-Based Embeddings</h3><p>这里就是在原来的基础之上加上了char-level RNN,也是已经有了的工作了.不过把他们的表示直接加上而不是concat,这样也是可以的,关于相加和concat,挺玄学的,直觉上觉得concat保留了更多的信息,但是相加可能也会有很好的效果.至于为什么我就不太清楚了,欢迎指正.<br>所以我们的score function变成:</p><script type="math/tex; mode=display">\phi(s_t)=h_t^T e_{s_t}^{token}+h_t^T e_{t}^{chars}=h_t^T E_{out}w_{s_t}+h_t^T E^{RNN}_{out}w_{s_t}</script><p>这里的$e^{chars}$就是前面提到的char embedding了.</p><p>$E^{RNN}_{out}$的组成我们需要说一下:<br><strong>对于所有的in-vocabulary的numeral,我们有对应的char-embedding,对于in-vocabulary的word,我们还是给他原本的token embedding $e_{s_t}$,对于OOV的numeral和char embedding,该怎么样还是怎么样,我们还是要用UNK来替代他</strong></p><blockquote><p>这里我的疑问就是,为什么作者选择用char-level只对numeral做encoding,对word却还是用原来的token embedding呢?为什么不能用同样的strategy?是因为这样的区别印证了作者开头说的:把word和numeral分开处理效果更好吗?而且,对于新的OOV的数字,为什么不能直接同样地做char-level encoding呢?</p></blockquote><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>作者使用了层级softmax,就是先用一个softmax classifier,判断你是接下来生成数字还是单词,判断完成之后,每个分支继续softmax,比如对于生成数字的分支,我们的softmax相当于:$p(s_t|c_t=numeral, h_t)$,这两个分支之间是不共享任何参数的,所以我们彻底将word和numeral分割了开来.</p><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/78335419.jpg" alt=""><br>C={word,numeral}</p><h3 id="所以利用Hierarchical-Softmax-作者之后的工作就集中在如何表示Number这一块-也就是说如何建模-p-s-t-c-t-numeral-h-t-这个distribution"><a href="#所以利用Hierarchical-Softmax-作者之后的工作就集中在如何表示Number这一块-也就是说如何建模-p-s-t-c-t-numeral-h-t-这个distribution" class="headerlink" title="所以利用Hierarchical Softmax,作者之后的工作就集中在如何表示Number这一块,也就是说如何建模$p(s_t|c_t=numeral, h_t)$这个distribution"></a>所以利用Hierarchical Softmax,作者之后的工作就集中在如何表示Number这一块,也就是说如何建模$p(s_t|c_t=numeral, h_t)$这个distribution</h3><h2 id="Digit-RNN-Model"><a href="#Digit-RNN-Model" class="headerlink" title="Digit-RNN Model"></a>Digit-RNN Model</h2><p>emmm其实就是char level的RNN,只不过我们把之前token level得到的$h_t$feed进这个char level RNN而已,这样就在保留了上文的信息的同时,也用RNN来给数字提取了一个表示.这样我们就不需要用UNKnumeral这样的symbol了,对于任何新的词,我们都可以用RNN和之前的hidden state得到它的一个比较好的encoding,来predict它的概率.</p><h2 id="MoG"><a href="#MoG" class="headerlink" title="MoG"></a>MoG</h2><p>我们需要的是predict numeral的概率,作者想到用Mixture of Gaussian去计算.</p><script type="math/tex; mode=display">q(v)=\sum_{k=1}^{K} \pi_k N_k(v; \mu_k, \sigma_k^2)</script><script type="math/tex; mode=display">\pi_k = softmax(B^T h_t)</script><p>我们根据的hidden state来计算每个gaussian component的weight.对于一个连续的随机变量,对于它等于某个值的概率都是0,所以我们需要用cdf的差值近似出这个pdf.这个近似的精度和这个数字的十位数小数精度有关,这是也我认为这个模型有趣的地方.</p><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/5781206.jpg" alt=""><br>作者将一个数字的概率拆解成,留几位小数点的概率乘上已知小数点位数,值是多少的概率.</p><script type="math/tex; mode=display">p(s)=p(v,r)=p(r)\tilde{Q}(v|r)</script><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/78862107.jpg" alt=""></p><h2 id="Combination"><a href="#Combination" class="headerlink" title="Combination"></a>Combination</h2><p>作者又很骚地将三个模型combine了起来…相当于又加了一个hierarchy的softmax,首先根据hidden state预测使用哪种strategy,{h-softmax, d-RNN, MoG},然后扔进不同的模型中…最后求一个weighted sum.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/94549504.jpg" alt=""><br>其实和ensemble learning的思想是非常像的,不过ensemble learning的普通的bagging是取average.这个会自动选择weights.</p><h2 id="Perplexity-evaluation-APP"><a href="#Perplexity-evaluation-APP" class="headerlink" title="Perplexity evaluation APP"></a>Perplexity evaluation APP</h2><p>作者使用了adjust perplexity,因为perplexity对 OOV 的词非常敏感.<br>所以作者penalize了OOV的词的概率<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/17773389.jpg" alt=""></p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/40441429.jpg" alt=""></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>这里稍微提一下,作者用了EM算法无监督初始化高斯分布的$\sigma,\mu$<br>还有这个MoG模型没有办法直接学习到embedding,因为他会predict概率.而没有表示的过程.<br>这个是softmax学到的number embedding<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-16/99781036.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-17/98342249.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天说一下这篇ACL18的文章&lt;br&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.08154&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1805.08154&lt;/a&gt;&lt;br&gt;Nume
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Attention Is All You Need浅析</title>
    <link href="https://jeffchy.github.io/2018/10/14/Attention-Is-All-You-Need%E6%B5%85%E6%9E%90/"/>
    <id>https://jeffchy.github.io/2018/10/14/Attention-Is-All-You-Need浅析/</id>
    <published>2018-10-14T15:01:49.000Z</published>
    <updated>2018-10-15T15:18:13.050Z</updated>
    
    <content type="html"><![CDATA[<h1 id="不知道取什么的一级标题"><a href="#不知道取什么的一级标题" class="headerlink" title="不知道取什么的一级标题"></a>不知道取什么的一级标题</h1><p>今天还说说这一篇神作了,主要的参考资料是</p><ul><li>原paper: <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external">https://arxiv.org/abs/1706.03762</a></li><li>剑林大哥博客里的中文浅析(我觉得看paperweekly公众号的朋友应该都会知道他): <a href="https://kexue.fm/archives/4765" target="_blank" rel="external">https://kexue.fm/archives/4765</a></li><li>刘贺的博客 <a href="https://www.52coding.com.cn/index.php?/Articles/single/66" target="_blank" rel="external">https://www.52coding.com.cn/index.php?/Articles/single/66</a></li><li>Pytorch开源实现:<a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/" target="_blank" rel="external">https://github.com/jadore801120/attention-is-all-you-need-pytorch/</a></li><li>在一篇论文里面(BERT)里看到的,作者推荐的英文tutorial,Good english tutorial and implementation, for English speakers:) from harvard nlp -&gt; refer to.<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="external">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li></ul><h1 id="笔者的学习顺序"><a href="#笔者的学习顺序" class="headerlink" title="笔者的学习顺序"></a>笔者的学习顺序</h1><ol><li>笔者先看了剑林大哥和刘贺大哥的博客</li><li>笔者准备去读一遍论文</li><li>笔者准备去看哈佛的pytorch tutorial,正好最近需要用pytorch,学一下</li></ol><h1 id="论文提要"><a href="#论文提要" class="headerlink" title="论文提要"></a>论文提要</h1><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>作者先理了理目前解决<strong>sequence modeling and transduction problems</strong>的主流模型</p><ul><li>Recurrent models(RNN, LSTM, GRU), 利用hidden state或者加上cell state来传递序列的顺序信息,这种递归的结构<strong>无法并行</strong>,而且Recurrent Model没有办法很好的建模全局的结构信息.因为本质与Markov Process相似.</li><li>Attention mechanisms使得模型可以更好的建模依赖关系,但是通常我们把它和RNN一起用.</li><li>所以本作提出模型<strong>Tranformer</strong>,避免了RNN,完全基于attention机制,使得模型能够更好地建模输入和输出的全局依赖.而且可以并行.</li></ul><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>对于<strong>sequence transduction problems</strong>,目前用得最多的还是seq2seq的结构,我们训练一个Encoder $E$和一个Decoder $D$,通过Encoder,我们将一串Input $x={x_1,x_2,\cdots,x_n}$(比如英文句子的embedding表示)编码成$z={z_1, \cdots, z_n}$, 然后将$z$输入一个Decoder,得到target language (比如Chinese) $y={y_1, y_2, \cdots, y_m}$,以前的Encoder-Decoder结构主要用的是Recurrent Neural Network, 而本文的是将RNN单元替换掉,本质还是遵循着EncoderDecoder的基本思想.</p><h3 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/68227395.jpg" alt=""></p><h3 id="Encoder长这样"><a href="#Encoder长这样" class="headerlink" title="Encoder长这样"></a>Encoder长这样</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/67768606.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/75659670.jpg" alt=""></p><script type="math/tex; mode=display"> L1 = LayerNorm\{multiHeadSelfAttendLayer(input(x)+posiInput(x))+[input(x)+posiInput(x)]\}</script><script type="math/tex; mode=display"> L2 = LayerNorm\{ fc(L1)+L1 \}</script><p>这里模型使用了multi head self attention layer, 并且运用了 residual connection来保留原始信息的传播,然后每一层作layer-norm. 简单说下layer norm,别和batch norm搞混了.<br>layer norm: 注意layer norm是对layer的input作normalize,而batchnorm是对每个batch的x的某一个维度作normalize:)<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/24798913.jpg" alt=""></p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder也是非常类似的,除了加上了一层mask的multihead self attention layer,对这个我的理解是,decoding的时候,你需要防止attention去看那些padding补零的entry,所以要去除这个影响.</p><h3 id="真正的encoder-decoder是上面的单元的6次stack-以提升表达能力"><a href="#真正的encoder-decoder是上面的单元的6次stack-以提升表达能力" class="headerlink" title="真正的encoder decoder是上面的单元的6次stack.以提升表达能力"></a>真正的encoder decoder是上面的单元的6次stack.以提升表达能力</h3><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>现在的核心问题是解决什么是文章中的Attention,呢个Multi head self attention 到底是什么.</p><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>是Multi-head Attentionde的比本组成单元,长下图这样.其实和点乘计算相似度的attention是相同的.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/41878014.jpg" alt=""></p><ul><li>$Q$代表一个个query$q_t$组成的矩阵(比如一个词的embedding) </li><li>$K$代表Key,$k_t$组成的矩阵.</li><li>$V$代表Value,$v_t$组成的矩阵.</li><li>解释一下,attention是根据<strong>相似度训练出来的attention weights</strong>,对目标<strong>input或者output</strong>作加权求和.确定重点关注哪一个部分.所以我们先根据$<q_t, k_t="">$的matmul,得到query与每个key的相似度,然后用一个softmax得到对应的attention weights,也就是key对应的value的每个部分的关注度应该是多少. </q_t,></li><li>举个例子Attention在NMT的应用中,(这一段参考<a href="https://www.52coding.com.cn/index.php?/Articles/single/66" target="_blank" rel="external">https://www.52coding.com.cn/index.php?/Articles/single/66</a> 同样是一篇好文).<blockquote><p>可以理解为，比如刚翻译完主语，接下来想要找谓语，【找谓语】这个信息就是 query，然后 key 是源句子的编码，通过 query 和 key 计算出 attention weight （应该关注的谓语的位置），最后和 value （源句子编码）计算加权和。</p></blockquote></li></ul><p>我们的attention:)<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/20529845.jpg" alt=""><br>这样的矩阵运算是搞笑的,而这样的定义是有普适性的. $\frac{1}{\sqrt{d_k}}$用作normalizing,使得softmax更加soft</p><h3 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/41012200.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/35256709.jpg" alt=""><br>看图,很清晰,multihead attention就是先Linear()一下QKV,这些投影的weights是学习到的,使得模型有更强的表达能力.然后将这些输入scaled dot-product attention,然后concat起来</p><h3 id="中间的position-wise-的Feed-forward小模块"><a href="#中间的position-wise-的Feed-forward小模块" class="headerlink" title="中间的position wise 的Feed forward小模块"></a>中间的position wise 的Feed forward小模块</h3><p>其实就是<br>$Linear(ReLU(Linear(x)))$<br>但是谷歌又风骚地把它命名为$FFN$<br>可以被理解成两个一维的卷积,per layer</p><h3 id="为了保留位置信息-position-embedding作为输入是必须的"><a href="#为了保留位置信息-position-embedding作为输入是必须的" class="headerlink" title="为了保留位置信息,position embedding作为输入是必须的"></a>为了保留位置信息,position embedding作为输入是必须的</h3><p>因为模型中如果没有position embedding作为输入,如果改变两个单词的顺序,模型学到的东西是不会变的.<br>文章中直接用了这样的embedding for position<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/63416539.jpg" alt=""></p><h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>文章中用的attention主要是self-attention,也就是说QKV分别等于$query(X),X,X$,query(X)表示找寻与X有关系的单词.我们可以认为这样的self-attention能在encoding部分学到词与词之间的依赖关系.<img src="http://oj4pv4f25.bkt.clouddn.com/18-10-15/60719083.jpg" alt=""></p><h3 id="整个模型大概就是这样-可以说非常巧妙地使用-并且强化了attention地机制"><a href="#整个模型大概就是这样-可以说非常巧妙地使用-并且强化了attention地机制" class="headerlink" title="整个模型大概就是这样.可以说非常巧妙地使用,并且强化了attention地机制."></a>整个模型大概就是这样.可以说非常巧妙地使用,并且强化了attention地机制.</h3><h2 id="再说说代码-这里代码贴的是pytorch开源实现"><a href="#再说说代码-这里代码贴的是pytorch开源实现" class="headerlink" title="再说说代码 这里代码贴的是pytorch开源实现"></a>再说说代码 这里代码贴的是pytorch开源实现</h2><p><a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/" target="_blank" rel="external">https://github.com/jadore801120/attention-is-all-you-need-pytorch/</a></p><h3 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="string">''' Define the Layers '''</span></div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">from</span> transformer.SubLayers <span class="keyword">import</span> MultiHeadAttention, PositionwiseFeedForward</div><div class="line"></div><div class="line">__author__ = <span class="string">"Yu-Hsiang Huang"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' Compose with two layers '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super(EncoderLayer, self).__init__()</div><div class="line">        self.slf_attn = MultiHeadAttention(</div><div class="line">            n_head, d_model, d_k, d_v, dropout=dropout)</div><div class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, enc_input, non_pad_mask=None, slf_attn_mask=None)</span>:</span></div><div class="line">        enc_output, enc_slf_attn = self.slf_attn(</div><div class="line">            enc_input, enc_input, enc_input, mask=slf_attn_mask)</div><div class="line">        enc_output *= non_pad_mask</div><div class="line"></div><div class="line">        enc_output = self.pos_ffn(enc_output)</div><div class="line">        enc_output *= non_pad_mask</div><div class="line"></div><div class="line">        <span class="keyword">return</span> enc_output, enc_slf_attn</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' Compose with three layers '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_inner, n_head, d_k, d_v, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super(DecoderLayer, self).__init__()</div><div class="line">        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</div><div class="line">        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</div><div class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, dec_input, enc_output, non_pad_mask=None, slf_attn_mask=None, dec_enc_attn_mask=None)</span>:</span></div><div class="line">        dec_output, dec_slf_attn = self.slf_attn(</div><div class="line">            dec_input, dec_input, dec_input, mask=slf_attn_mask)</div><div class="line">        dec_output *= non_pad_mask</div><div class="line"></div><div class="line">        dec_output, dec_enc_attn = self.enc_attn(</div><div class="line">            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)</div><div class="line">        dec_output *= non_pad_mask</div><div class="line"></div><div class="line">        dec_output = self.pos_ffn(dec_output)</div><div class="line">        dec_output *= non_pad_mask</div><div class="line"></div><div class="line">        <span class="keyword">return</span> dec_output, dec_slf_attn, dec_enc_attn</div></pre></td></tr></table></figure><h3 id="Sublayers-Multihead-attention-PositionwiseFeedforward"><a href="#Sublayers-Multihead-attention-PositionwiseFeedforward" class="headerlink" title="Sublayers Multihead-attention, PositionwiseFeedforward"></a>Sublayers Multihead-attention, PositionwiseFeedforward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Define the sublayers in encoder/decoder layer '''</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> transformer.Modules <span class="keyword">import</span> ScaledDotProductAttention</div><div class="line"></div><div class="line">__author__ = <span class="string">"Yu-Hsiang Huang"</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' Multi-Head Attention module '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_head, d_model, d_k, d_v, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super().__init__()</div><div class="line"></div><div class="line">        self.n_head = n_head</div><div class="line">        self.d_k = d_k</div><div class="line">        self.d_v = d_v</div><div class="line"></div><div class="line">        self.w_qs = nn.Linear(d_model, n_head * d_k)</div><div class="line">        self.w_ks = nn.Linear(d_model, n_head * d_k)</div><div class="line">        self.w_vs = nn.Linear(d_model, n_head * d_v)</div><div class="line">        nn.init.normal_(self.w_qs.weight, mean=<span class="number">0</span>, std=np.sqrt(<span class="number">2.0</span> / (d_model + d_k)))</div><div class="line">        nn.init.normal_(self.w_ks.weight, mean=<span class="number">0</span>, std=np.sqrt(<span class="number">2.0</span> / (d_model + d_k)))</div><div class="line">        nn.init.normal_(self.w_vs.weight, mean=<span class="number">0</span>, std=np.sqrt(<span class="number">2.0</span> / (d_model + d_v)))</div><div class="line"></div><div class="line">        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, <span class="number">0.5</span>))</div><div class="line">        self.layer_norm = nn.LayerNorm(d_model)</div><div class="line"></div><div class="line">        self.fc = nn.Linear(n_head * d_v, d_model)</div><div class="line">        nn.init.xavier_normal_(self.fc.weight)</div><div class="line"></div><div class="line">        self.dropout = nn.Dropout(dropout)</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, q, k, v, mask=None)</span>:</span></div><div class="line"></div><div class="line">        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head</div><div class="line"></div><div class="line">        sz_b, len_q, _ = q.size()</div><div class="line">        sz_b, len_k, _ = k.size()</div><div class="line">        sz_b, len_v, _ = v.size()</div><div class="line"></div><div class="line">        residual = q</div><div class="line"></div><div class="line">        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)</div><div class="line">        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)</div><div class="line">        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)</div><div class="line"></div><div class="line">        q = q.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(<span class="number">-1</span>, len_q, d_k) <span class="comment"># (n*b) x lq x dk</span></div><div class="line">        k = k.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(<span class="number">-1</span>, len_k, d_k) <span class="comment"># (n*b) x lk x dk</span></div><div class="line">        v = v.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(<span class="number">-1</span>, len_v, d_v) <span class="comment"># (n*b) x lv x dv</span></div><div class="line"></div><div class="line">        mask = mask.repeat(n_head, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># (n*b) x .. x ..</span></div><div class="line">        output, attn = self.attention(q, k, v, mask=mask)</div><div class="line"></div><div class="line">        output = output.view(n_head, sz_b, len_q, d_v)</div><div class="line">        output = output.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>).contiguous().view(sz_b, len_q, <span class="number">-1</span>) <span class="comment"># b x lq x (n*dv)</span></div><div class="line"></div><div class="line">        output = self.dropout(self.fc(output))</div><div class="line">        output = self.layer_norm(output + residual)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> output, attn</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">''' A two-feed-forward-layer module '''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_in, d_hid, dropout=<span class="number">0.1</span>)</span>:</span></div><div class="line">        super().__init__()</div><div class="line">        self.w_1 = nn.Conv1d(d_in, d_hid, <span class="number">1</span>) <span class="comment"># position-wise</span></div><div class="line">        self.w_2 = nn.Conv1d(d_hid, d_in, <span class="number">1</span>) <span class="comment"># position-wise</span></div><div class="line">        self.layer_norm = nn.LayerNorm(d_in)</div><div class="line">        self.dropout = nn.Dropout(dropout)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        residual = x</div><div class="line">        output = x.transpose(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">        output = self.w_2(F.relu(self.w_1(output)))</div><div class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">        output = self.dropout(output)</div><div class="line">        output = self.layer_norm(output + residual)</div><div class="line">        <span class="keyword">return</span> output</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;不知道取什么的一级标题&quot;&gt;&lt;a href=&quot;#不知道取什么的一级标题&quot; class=&quot;headerlink&quot; title=&quot;不知道取什么的一级标题&quot;&gt;&lt;/a&gt;不知道取什么的一级标题&lt;/h1&gt;&lt;p&gt;今天还说说这一篇神作了,主要的参考资料是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/NLP/Paper/Note/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>简单说说BERT模型:Pre-training of Deep Bidirectional Transformers for Language Understanding </title>
    <link href="https://jeffchy.github.io/2018/10/13/%E7%AE%80%E5%8D%95%E8%AF%B4%E8%AF%B4BERT%E6%A8%A1%E5%9E%8B-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/"/>
    <id>https://jeffchy.github.io/2018/10/13/简单说说BERT模型-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/</id>
    <published>2018-10-13T12:36:50.000Z</published>
    <updated>2018-10-13T13:55:24.858Z</updated>
    
    <content type="html"><![CDATA[<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>最近朋友圈又被<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="external">BERT</a>刷屏了,ELMO刷屏之后还没多久,谷歌就提出了吊打一切的BERT,用它可以获得更好的单词在文本中的隐藏表示,在11大NLP的任务中达到了变态的结果,太不给机会了.</p><h2 id="BERT的中心思想和碎碎念"><a href="#BERT的中心思想和碎碎念" class="headerlink" title="BERT的中心思想和碎碎念"></a>BERT的中心思想和碎碎念</h2><p>BERT是Bidirectional Encoder Representations from Transformers的缩写,产生了一个pretrained的Encoder,可以很好的建模单词的上下文信息,获得一个单词的隐藏表示.和ELMO类似的地方就是,它算是一个Encoder,我们需要在corpus上跑一下这个模型,(Bidirectional Transformer),得到每一个单词在上下文中的hidden representation.并不是像word vector一样即插即用.但是它的优势是:</p><ul><li>相对其他这样的模型来讲还是比较快速的</li><li>效果变态好</li><li>finetuning的过程也并不复杂</li></ul><h2 id="一些文章推荐"><a href="#一些文章推荐" class="headerlink" title="一些文章推荐"></a>一些文章推荐</h2><p>BERT的详细介绍很多推送已经说过了,重复一下好像意义不大,给一个传送门吧:<br><a href="https://zhuanlan.zhihu.com/p/46656854" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/46656854</a> 机器之心[中文]<br><a href="https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/" target="_blank" rel="external">https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/</a> Reddit上第一作者的帖子,非常简明[英文,需要f q]</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>可以看到,模型本身就是Bidirectional-transformer.如果你了解Transformer模型的话,那么这个模型其实就很好理解了,没什么可以说的.论文中也直接跳过,refer to到了Attention is all you need这一篇神作. 如果你不太清楚transformer模型,之后我也会介绍一下.<br>整个BERT模型的设计原则就是, Language Model的训练其实制约了word representation, 从左到右、或者结合了从右到左这样的顺序性隐藏在了之前的模型中.这篇文章利用双向的transformer,(最左边的那个),可以真正地将词和context直接而密集地链接起来.<br><img src="http://oj4pv4f25.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-13%20%E4%B8%8B%E5%8D%889.54.46.png" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-13%20%E4%B8%8B%E5%8D%889.21.20.png" alt=""></p><h2 id="模型训练的任务"><a href="#模型训练的任务" class="headerlink" title="模型训练的任务"></a>模型训练的任务</h2><p>作者在两个任务上训练.</p><ol><li><p>Masked LM 随机扔掉一些词,然后预测他们</p><blockquote><p>Input:<br>the man [MASK1] to [MASK2] store<br>Label:<br>[MASK1] = went; [MASK2] = store</p></blockquote></li><li><p>Next Sentence Prediction 打乱一些句子,构造数据集,预测句子A的后面是否是句子B, true or false  </p></li></ol><ul><li>Input:the man went to the store [SEP] he bought a gallon of milk<br>Label: IsNext</li><li>Input: the man went to the store [SEP] penguins are flightless birds<br>Label: NotNext</li></ul><p>通过这两个任务的训练得到的Bidirectional Transformer, 成为了一个足够全面,强力的,能建模token level(词和词之间), sequence level (句子和句子之间)信息的Encoder.</p><h2 id="作者在各种任务上做了测试"><a href="#作者在各种任务上做了测试" class="headerlink" title="作者在各种任务上做了测试"></a>作者在各种任务上做了测试</h2><p>吊打一切.jpg<br>真的是吊打一切,只能这么形容了…<br>详情看paper :)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;BERT&quot;&gt;&lt;a href=&quot;#BERT&quot; class=&quot;headerlink&quot; title=&quot;BERT&quot;&gt;&lt;/a&gt;BERT&lt;/h2&gt;&lt;p&gt;最近朋友圈又被&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot; target=&quot;
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Natual Language Processing" scheme="https://jeffchy.github.io/tags/Natual-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>Quick Review On Forward-Backward Algorithm</title>
    <link href="https://jeffchy.github.io/2018/10/12/Quick-Review-On-Forward-Backward-Algorithm/"/>
    <id>https://jeffchy.github.io/2018/10/12/Quick-Review-On-Forward-Backward-Algorithm/</id>
    <published>2018-10-12T13:14:25.000Z</published>
    <updated>2018-10-13T12:37:30.521Z</updated>
    
    <content type="html"><![CDATA[<p>I strongly recommend these short vedio lecture :) from <strong>mathematicalmonk</strong> <a href="https://www.youtube.com/user/mathematicalmonk" target="_blank" rel="external">https://www.youtube.com/user/mathematicalmonk</a><br>He made many really wonderful videos</p><p>I’m such a lazy guy :), I hate typing so much latex, but 我是一个勤劳的搬运工</p><h3 id="Forward-Backward-Algorithm-Intro"><a href="#Forward-Backward-Algorithm-Intro" class="headerlink" title="Forward-Backward Algorithm Intro"></a>Forward-Backward Algorithm Intro</h3><p><a href="https://www.youtube.com/watch?v=7zDARfKVm7s" target="_blank" rel="external">https://www.youtube.com/watch?v=7zDARfKVm7s</a></p><h3 id="Forward-Algorithm"><a href="#Forward-Algorithm" class="headerlink" title="Forward Algorithm"></a>Forward Algorithm</h3><p><a href="https://www.youtube.com/watch?v=M7afek1nEKM" target="_blank" rel="external">https://www.youtube.com/watch?v=M7afek1nEKM</a><br><a href="https://www.youtube.com/watch?v=MPmrFu4jFk4" target="_blank" rel="external">https://www.youtube.com/watch?v=MPmrFu4jFk4</a></p><h3 id="Backward-Algorithm"><a href="#Backward-Algorithm" class="headerlink" title="Backward Algorithm"></a>Backward Algorithm</h3><p><a href="https://www.youtube.com/watch?v=jwYuki9GgJo" target="_blank" rel="external">https://www.youtube.com/watch?v=jwYuki9GgJo</a></p><h3 id="log-sum-exp-trick"><a href="#log-sum-exp-trick" class="headerlink" title="log-sum-exp trick"></a>log-sum-exp trick</h3><p><a href="https://www.youtube.com/watch?v=-RVM21Voo7Q&amp;index=105&amp;list=PLD0F06AA0D2E8FFBA&amp;t=0s" target="_blank" rel="external">https://www.youtube.com/watch?v=-RVM21Voo7Q&amp;index=105&amp;list=PLD0F06AA0D2E8FFBA&amp;t=0s</a></p><h3 id="Viterbi-algorithm"><a href="#Viterbi-algorithm" class="headerlink" title="Viterbi algorithm"></a>Viterbi algorithm</h3><p><a href="https://www.youtube.com/watch?v=RwwfUICZLsA&amp;index=106&amp;list=PLD0F06AA0D2E8FFBA&amp;t=34s" target="_blank" rel="external">https://www.youtube.com/watch?v=RwwfUICZLsA&amp;index=106&amp;list=PLD0F06AA0D2E8FFBA&amp;t=34s</a><br><a href="https://www.youtube.com/watch?v=t3JIk3Jgifs&amp;index=107&amp;list=PLD0F06AA0D2E8FFBA&amp;t=0s" target="_blank" rel="external">https://www.youtube.com/watch?v=t3JIk3Jgifs&amp;index=107&amp;list=PLD0F06AA0D2E8FFBA&amp;t=0s</a></p><p>Watch these videos, and it’s done.<br>Subscribe! Mathematicalmonk!</p><h2 id="Code-Application-Pytorch-Tutorial-Bi-directional-CRF-tagger"><a href="#Code-Application-Pytorch-Tutorial-Bi-directional-CRF-tagger" class="headerlink" title="Code Application: Pytorch Tutorial: Bi-directional CRF tagger :)"></a>Code Application: Pytorch Tutorial: Bi-directional CRF tagger :)</h2><p><a href="https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html" target="_blank" rel="external">https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;I strongly recommend these short vedio lecture :) from &lt;strong&gt;mathematicalmonk&lt;/strong&gt; &lt;a href=&quot;https://www.youtube.com/user/mathematic
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/NLP/Note/"/>
    
    
      <category term="Probability Theory" scheme="https://jeffchy.github.io/tags/Probability-Theory/"/>
    
      <category term="Algorithm" scheme="https://jeffchy.github.io/tags/Algorithm/"/>
    
      <category term="Natual Language Processing" scheme="https://jeffchy.github.io/tags/Natual-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>Brown Clustering</title>
    <link href="https://jeffchy.github.io/2018/10/09/Brown-Clustering/"/>
    <id>https://jeffchy.github.io/2018/10/09/Brown-Clustering/</id>
    <published>2018-10-09T06:45:59.000Z</published>
    <updated>2018-10-09T08:22:50.597Z</updated>
    
    <content type="html"><![CDATA[<p>我还是用回中文吧. 上一篇我们介绍了RNNG,其中为了减少softmax选择vocabulary的复杂度的时候,使用了Brown Clustering,这是一个古老但是非常有效的算法,今天我们就看看Brown Clustering是什么吧.</p><p><strong>本篇主要基于M.Collins的Video Lecture <a href="https://www.youtube.com/watch?v=xGfQMrYoIx4&amp;list=PLO9y7hOkmmSEAqCc0wrNBrsoJMTmIN98M" target="_blank" rel="external">https://www.youtube.com/watch?v=xGfQMrYoIx4&amp;list=PLO9y7hOkmmSEAqCc0wrNBrsoJMTmIN98M</a></strong></p><h1 id="Intruduction-什么是Brown-Clustering"><a href="#Intruduction-什么是Brown-Clustering" class="headerlink" title="Intruduction 什么是Brown Clustering"></a>Intruduction 什么是Brown Clustering</h1><p>Brown Clustering可以将单词归为不同的类,比如<br>K1: Monday, Thursday, Weekends, …<br>K2: Water, Gas, Coal, …</p><p>Brown Clustering还可以将单词层级化<br>比如途中的 boy,girl,是一类,都是人,再加上apple,pear,形成了一类比如名词.我们可以用0,1表示这样的类别关系<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-9/48409628.jpg" alt=""></p><h1 id="我们如何Clustering"><a href="#我们如何Clustering" class="headerlink" title="我们如何Clustering"></a>我们如何Clustering</h1><p>Intuition - similar words - similar left and right words (context)<br>比如Monday, Tuesday,他们是类似的,那么应用他们的上下文也通常比较类似. 比如I go to school on Monday / Tuesday…</p><h2 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h2><ul><li>$V$ 是数据集中的所有vocabulary</li><li>$C$ Cluster,是一个把单词映射到k个类别的函数, $V \to \{1,2,3,\cdots,k\}$, 比如 $C(the)=C(a)=1, C(one)=2$</li><li>与HMM非常相似,我们定义evidence $e(w_i|C(w_i))$比如$e(the|1)$ 反映了已知现在类别是1的情况下the出现的概率</li><li>同样我们定义 $q(C(w_i)|C(w_{i-1}))$,表示已知前一个词的类别,那现在这个单词所属的类别的分布是什么</li><li>所以我们的整个数据的产生概率为 $p(w_1,\cdots,w_n) = \prod_{i=1}^n e(w_i|C(w_i))q(C(w_i)|C(w_{i-1}))$ emm其实就是likelihood</li><li>然后写成log likelihood: $log p(w_1,\cdots,w_n) = \sum_{i=1}^n log e(w_i|C(w_i))q(C(w_i)|C(w_{i-1}))$ </li><li>一个例子 <img src="http://oj4pv4f25.bkt.clouddn.com/18-10-9/87807444.jpg" alt=""></li></ul><h2 id="小小总结一下"><a href="#小小总结一下" class="headerlink" title="小小总结一下"></a>小小总结一下</h2><p>Brown CLustering可以表示为</p><ul><li>输入<ul><li>$V$ vocabulary</li><li>$X$ corpus</li></ul></li><li>输出<ul><li>一个函数映射$C: V \to {1 \cdots k}$</li><li>e分布的参数 $e(v|c)$ 对于所有的 $v \in V, c \in \{1,\cdots,k\}$</li><li>q分布的$q(c|c’)$ 对于所有的 $c \in \{1,\cdots,k\}\  c’ \in \{1,\cdots,k\}$</li></ul></li></ul><h2 id="如何找到这些参数"><a href="#如何找到这些参数" class="headerlink" title="如何找到这些参数"></a>如何找到这些参数</h2><p>我们用我们前面derive的式子来衡量$C$<br>第二个等号使用互信息得到<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-9/87288170.jpg" alt=""></p><h2 id="算法1"><a href="#算法1" class="headerlink" title="算法1"></a>算法1</h2><p>我们有$|V|$个vocabulary,每个自成一类，我们的目标是找到$k$个cluster,所以我们greedy地merge $|V|-k$次<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-9/3293337.jpg" alt=""><br>通过枚举所有的pair,太慢了</p><h2 id="算法2"><a href="#算法2" class="headerlink" title="算法2"></a>算法2</h2><p>我们找出最常见的M=1000个单词，每个单词给一个类别。然后我们对剩下的单词中最常见的number $M+1$的单词一个类别，所以现在有$M+1$个类别，然后选出最合适的pair进行merge，变为$M$类，多所有VOCABULARY进行重复，最后我们会把所有的词归到M类上，完成以后再做M-1次merge即可得到层级表示。<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-9/86185324.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我还是用回中文吧. 上一篇我们介绍了RNNG,其中为了减少softmax选择vocabulary的复杂度的时候,使用了Brown Clustering,这是一个古老但是非常有效的算法,今天我们就看看Brown Clustering是什么吧.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;本
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/NLP/Note/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Probability Theory" scheme="https://jeffchy.github.io/tags/Probability-Theory/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>论文解读:Recurrent Neural Network Grammars</title>
    <link href="https://jeffchy.github.io/2018/10/08/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0:%20Recurrent-Neural-Network-Grammars/"/>
    <id>https://jeffchy.github.io/2018/10/08/论文笔记: Recurrent-Neural-Network-Grammars/</id>
    <published>2018-10-08T03:43:13.000Z</published>
    <updated>2018-10-08T16:03:05.371Z</updated>
    
    <content type="html"><![CDATA[<p>今天主要来说说 NAACL16 CMU的一篇, RNNG, Recurrent Neural Network Grammars. 这篇文章提出了RNNG,可以用作Parsing和language Modeling,并且实验结果表明在EN和ZH两个语言上达到了state-of-the-art.<br>韦阳dalao在知乎上也分享过,另外推荐他的一系列知乎专栏 - 特别是这篇 <a href="https://zhuanlan.zhihu.com/p/45527481" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/45527481</a><br>搬运完毕,我开始说论文了. 可能还是大部分用英文吧.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>RNN, A sequential deep learning models solved many problems effectively these days, such as machine translation, image captioning, chatbots…But is language a sequential stuff?</p><blockquote><p>Despite these impressive results, sequential models are a priori inappropriate models of natural language, since re- lationships among words are largely organized in terms of latent nested structures rather than sequen- tial surface order (Chomsky, 1957).</p></blockquote><p>My tutor also believes that natural language cannot just be analyzed and learned by a deep learning sequential structure, because it actually contains more complex latent structure.</p><p>This paper proposed RNNG, a new generative probabilistic model of sentences, which can model nested, hierarchical relationships, This grammar generate the parse tree from top-down just like transition-based parsing of CFG, but each action(decition also parameterized using RNNs, condition on the action and generating process’s history) so it also relax context-free assumption.</p><p>They give two variants:</p><ul><li>Generative Model for parsing and language modeling</li><li>Discrimitive Model for parsing (using the techniques of importance sampling)</li></ul><h1 id="RNN-Grammars"><a href="#RNN-Grammars" class="headerlink" title="RNN Grammars"></a>RNN Grammars</h1><p>$(N, \Sigma, \Theta)$ refers to (finite set of nonterminals, finite set of terminals which is disjoint from $\Sigma$, neural network parameters) respectively.</p><p>Note that, the key difference between RNNG and PCFG is: we do not need the production rules, or we say grammar rules. But we encode it to the “action” of transiand learn it automaticly.</p><h1 id="Top-down-Discriminative-Parsing-of-RNNG"><a href="#Top-down-Discriminative-Parsing-of-RNNG" class="headerlink" title="Top-down Discriminative Parsing of RNNG"></a>Top-down Discriminative Parsing of RNNG</h1><p>Parsing: given corpus of sentences $X=\{x_1, \cdots, x_n\}$ get it’s parse tree $T=\{t_1, \cdots, t_n\}$<br>We want to model $P(T|X)$</p><p>As we said, the RNNG is based on top-down version of transition-based parsing.</p><ul><li>We have stack Buffer $B$ which contains <strong>Unprocessed terminal symbols(words,..)</strong></li><li>We have stack $S$, which contains <strong>{terminal symbols, “open” nonterminal symbols, completed constituents(all its sub-constituents are found)}</strong></li><li>We have a corpus which makes up the terminal sets and nonterminal sets $(N, \Sigma)$</li><li>We have following actions $A$<ul><li>$NT(X)$ move a nonterminal to the top of stack, thus make it “open”</li><li>$SHIFT$ removes the terminal $x$ from buffer $B$ and push it to the top of stack $S$</li><li>$REDUCE$ “close” the nonterminals, more detaily, keep poping the terminals until we get an “open” nonterminals, and make a complete constituents</li></ul></li></ul><p>Lets see an example. We initialize the Buffer with all the terminals(words), and stack with empty, than we use $NT(S)$ to push the root nonterminal to the top of stack, than we follow the actions to get a parse tree.<br>The Figure.1 shows the detail of three actions, Figure.2 shows an parsing example, with some of my notes.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-8/76108219.jpg" alt=""></p><h2 id="When-to-stop"><a href="#When-to-stop" class="headerlink" title="When to stop?"></a>When to stop?</h2><p>When there’s 1 single constituent (the parse tree) in the stack, and no word in the buffer, we complete.</p><h2 id="Constraints-on-parser-transitions"><a href="#Constraints-on-parser-transitions" class="headerlink" title="Constraints on parser transitions."></a>Constraints on parser transitions.</h2><p>Denote $n$ as number of “open” nonterminals on the stack</p><ul><li>$NT(X)$ operation is appliable if $B$ is not empty and $n &lt; 100$,<ul><li>If $B$ is empty, and we introduce a new open nonterminal, we are not able to close it and make a complete constituent</li><li>If $n &gt;= 100$ to limit the nonterminal sizes of a parse tree. (This one I’m not quite sure, if you know the reason, please give me some comments.)</li></ul></li><li>$SHIFT$ operations can only be applied if $B$ is not empty and $n \le 1$ <ul><li>If $B$ is empty we cannot shift elements in it to the stack</li><li>If $n = 0$, there’s only no open nonterminal, so introducing new word will not be able to include have parent</li></ul></li><li>$REDUCE$ operation can only be applied if the top of the stack is not an open nonterminal symbol. because if the open nonterminal is on the top of stack, you need at least 1 nonterminal/constituents to be its child.</li><li>$REDUCE$ can only be applied if $n \le 2$ or the buffer is empty<ul><li>because if $n = 1$, it can only be $\ \ (S\ \ $ thus if buffer is not empty, it is not possible<br>$A_D(B,S,n)$</li></ul></li></ul><h1 id="Top-down-Generative-Parsing-of-RNNG"><a href="#Top-down-Generative-Parsing-of-RNNG" class="headerlink" title="Top-down Generative Parsing of RNNG"></a>Top-down Generative Parsing of RNNG</h1><p>We want to model $P(Tree,X)$, so we need to generate the parse trees, and terminals (words) at the same time, So there’s no Buffer anymore, but output buffer $T$, and we replace $SHIFT$ operation with $GEN(x)$, which generate a terminal $x$ from the vocabulary $\Sigma$, and then add it to the top of stack (just lick $SHIFT$), and we stochastically selected the actions according to the conditional distribution that depends on the current output buffer and stack(I think now it should be the stack while the paper said we samples depends on (T ,B), but B buffer does not exist anymore)?<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-8/49703311.jpg" alt=""></p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints"></a>Constraints</h2><ul><li>$GEN(x)$ operation can only be applied if $n \le 1$, just like $SHIFT$</li><li>$REDUCE$ operation can only be applied if top of stack is not an open nonterminal, and $n \le 1$, $n \le 1$ here’s $n$ can be equals to 1 because when $n=1$ and reduce, we do not have input buffer, so by stopping $GEN(x)$ operation, we can end the process legally.</li></ul><h2 id="We-denote-the-set-of-actions-A-G-T-S-n"><a href="#We-denote-the-set-of-actions-A-G-T-S-n" class="headerlink" title="We denote the set of actions $A_G(T,S,n)$"></a>We denote the set of actions $A_G(T,S,n)$</h2><h1 id="Generative-Model-Detail"><a href="#Generative-Model-Detail" class="headerlink" title="Generative Model Detail"></a>Generative Model Detail</h1><p>Let’s denote: $y$ as the parse tree, $x$ as the terminals, we want to model $P(x,y)$, but how to model is the key problem.<br>As we are parsing from top to down, we futher denote $u_t$ as the embedding of the current parsing state, and $r_a$ as the action embedding, the sentence can be generate using the following formular using chain rule(language model)<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-8/50397204.jpg" alt=""></p><p>How we compute $u_t$? the current state is related to the history of $S,T,A$, stack, output buffer, actions, we denote them as $o_t,s_t,h_t$ respectively, by concat them, taking a linear transform, and add a nonlinear, we can represent $u_t$, see the following figure for details.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-8/39438407.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-8/19123141.jpg" alt=""></p><h2 id="Represent-elements-in-stack"><a href="#Represent-elements-in-stack" class="headerlink" title="Represent elements in stack"></a>Represent elements in stack</h2><p>We use a bi-LSTM to encode the open non-terminals, terminals, closed constituents. see the figure:<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-8/4624452.jpg" alt=""><br>We add the parent non-terminal to the head and tail.</p><h2 id="word-generation"><a href="#word-generation" class="headerlink" title="word generation"></a>word generation</h2><p>When generate a word, we first predict the action, $GEN$, and if the action is $GEN$, we predict which word $x$ it generates. To reduce the time complexity, we use a class-factored softmax, we splits the vocabulary to several classes, we first predict which class we should use, then for each class we predict the words in it. They cluster their words using brown clustering. <a href="https://en.wikipedia.org/wiki/Brown_clustering" target="_blank" rel="external">https://en.wikipedia.org/wiki/Brown_clustering</a><br>So we can reduce the time complexity of predicting a word from $O(|\Sigma|)$ to $O(\sqrt|\Sigma|)$ by split the vocabulary to $\sqrt|\Sigma|$ classes</p><h2 id="Convert-to-the-discrimitive-parsing-model"><a href="#Convert-to-the-discrimitive-parsing-model" class="headerlink" title="Convert to the discrimitive parsing model"></a>Convert to the discrimitive parsing model</h2><p>Converting is easy, change the embedding of output buffer $T_t$ to the embedding of input buffer, and retrain and max the conditional likelihood, given input string.</p><h1 id="Inference-the-Generative-model-via-importance-sampling"><a href="#Inference-the-Generative-model-via-importance-sampling" class="headerlink" title="Inference the Generative model via importance sampling"></a>Inference the Generative model via importance sampling</h1><p>We modeled $P(x,y)$, if we want to get a language model, we need to compute $P(x) = \sum_y P(x,y)$ by marginalize all possible parse trees $y$, however, the genarative process is not bounded, so number of parse trees will be large, it’s intractable to enumerate them all.</p><p>Similarly, to use the generative model for parsing, we need to select the best parse tree, so $y = argmax_y P(x,y)$, also intractable.</p><p>So we solve it by importance sampling.</p><h2 id="importance-sampling"><a href="#importance-sampling" class="headerlink" title="importance sampling"></a>importance sampling</h2><p>We define a conditional proposal distribution $q(y|x)$, with following property:</p><ul><li>$p(x,y)&gt;0 =&gt; q(y|x)&gt;0$</li><li>$y \sim q(y|x)$ can be obtained easily</li><li>$q(y|x)$ are known</li></ul><p><strong>The Discrimitive trained parser satisfy all these property.</strong><br>So we sample from the discrimitive parser.<br>We define $w(x,y) = p(x,y)/q(y|x)$<br>Thus the $p(x)$ can be obtained by:<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-8/42461349.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-8/8518009.jpg" alt=""></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-10-9/54224601.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天主要来说说 NAACL16 CMU的一篇, RNNG, Recurrent Neural Network Grammars. 这篇文章提出了RNNG,可以用作Parsing和language Modeling,并且实验结果表明在EN和ZH两个语言上达到了state-of
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
      <category term="Note" scheme="https://jeffchy.github.io/categories/NLP/Paper/Note/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Deep Learning" scheme="https://jeffchy.github.io/tags/Deep-Learning/"/>
    
      <category term="Natural Language Parsing" scheme="https://jeffchy.github.io/tags/Natural-Language-Parsing/"/>
    
  </entry>
  
  <entry>
    <title>ELMo-论文解读</title>
    <link href="https://jeffchy.github.io/2018/09/25/ELMo-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <id>https://jeffchy.github.io/2018/09/25/ELMo-论文解读/</id>
    <published>2018-09-25T07:02:54.000Z</published>
    <updated>2018-10-08T04:03:15.449Z</updated>
    
    <content type="html"><![CDATA[<p>今天来讲一讲这一篇论文啦. Deep contextualized word representations, 也就是最近火的不行的ELMo. <a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1802.05365.pdf</a><br>文章提出了一种新型的word embedding,使用它可以提高NLP很多任务的精度.<br>扔一篇很好的中文知乎介绍先, <a href="https://zhuanlan.zhihu.com/p/38254332" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/38254332</a> (抛玉引砖?)<br>这篇文章讲得相当好…我不知道我能讲啥了.<br>我尽量补充一些吧</p><p>首先默认大家看了前面的那个知乎专栏,其实里面有一个比较容易让人疑惑的点,那就是ELMo到底是怎么用的.<br>下面引用一篇博客的内容,我也是如此理解的.</p><blockquote><p>具体来讲如何使用ElMo产生的表征呢？对于一个supervised NLP任务，可以分以下三步:<br>1.产生pre-trained biLM模型。模型由两层bi-LSTM组成，之间用residual connection连接起来。<br>2.在任务语料上(注意是语料，忽略label)fine-tuning上一步得到的biLM模型。可以把这一步看为biLM的domain transfer。<br>3.利用ELMo的word embedding来对任务进行训练。通常的做法是把它们作为输入加到已有的模型中，一般能够明显的提高原模型的表现。</p></blockquote><hr><p>本文来自 triplemeng 的CSDN 博客 ，全文地址请点击：<a href="https://blog.csdn.net/triplemeng/article/details/82380202?utm_source=copy" target="_blank" rel="external">https://blog.csdn.net/triplemeng/article/details/82380202?utm_source=copy</a> </p><p>而加到已知模型中的方法就如知乎专栏那一篇所说</p><blockquote><p>（1）直接将ELMo词向量 ELMo_k 与普通的词向量 x_k拼接（concat）[ $x_k$;$ELMo_k$ ]。<br> （2) 直接将ELMo词向量ELMo_k 与隐层输出向量 h_k 拼接[ $h_k$;$ELMo_k$ ]，在SNLI,SQuAD上都有提升。</p></blockquote><p>finetune应该还是比较慢的…之前的finetune,或者说对不同的语料来说,每个词之间的contextual information是不一样的,所以在此之前还是最好过一遍BiLSTM模型,得到针对这个语料库的隐藏表示,而这个部分是非常慢的.即便不finetune,我们也要根据我们拿来的pretrained的BiLSTM跑一下整个语料库,一句一句跑(因为我们需要得到hidden layer),然后把所有的词的hidden layer和embedding dump下来存起来.整个encoding的过程还是有点慢的…反正会比pretrained wordvec 要慢不少咯</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天来讲一讲这一篇论文啦. Deep contextualized word representations, 也就是最近火的不行的ELMo. &lt;a href=&quot;https://arxiv.org/pdf/1802.05365.pdf&quot; target=&quot;_blank&quot; re
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Probability Theory" scheme="https://jeffchy.github.io/tags/Probability-Theory/"/>
    
      <category term="Embedding" scheme="https://jeffchy.github.io/tags/Embedding/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：End-to-End Sequence Labeling via Bi-directional LSTM-CNN-CRF</title>
    <link href="https://jeffchy.github.io/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9AEnd-to-End-Sequence-Labeling-via-Bi-directional-LSTM-CNN-CRF/"/>
    <id>https://jeffchy.github.io/2018/09/24/论文笔记：End-to-End-Sequence-Labeling-via-Bi-directional-LSTM-CNN-CRF/</id>
    <published>2018-09-24T06:57:37.000Z</published>
    <updated>2018-10-08T04:03:03.434Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文摘要-End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNN-CRF"><a href="#论文摘要-End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNN-CRF" class="headerlink" title="论文摘要 End-to-end Sequence Labeling via Bi-directional LSTM-CNN-CRF"></a>论文摘要 End-to-end Sequence Labeling via Bi-directional LSTM-CNN-CRF</h1><p>今天来将一些 End-to-end Sequence Labeling via Bi-directional LSTM-CNN-CRF 这篇文章吧，这是一篇CMU的工作。简单来说就是用一个完全End-to-end的模型来解决sequence labeling的问题，在NER和POStagging的问题上做了测试，达到了state-of-the-art，亮点在于不需要繁琐的feature engineering了。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在Introduction中，作者就大概阐述了整个End-to-end model的architecture。</p><blockquote><p>We first use convolutional<br>neural networks (CNNs) (LeCun et al.,<br>1989) to encode character-level information of a<br>word into its character-level representation. Then<br>we combine character- and word-level representations<br>and feed them into bi-directional LSTM<br>(BLSTM) to model context information of each<br>word. On top of BLSTM, we use a sequential<br>CRF to jointly decode labels for the whole sentence.</p></blockquote><p>用CNN去得到char-level representation，然后结合char-level和word-level（word vectors）输入一个Bi-LSTM在建模上下文信息，最后我们再用一个sequential的CRF去decode label。</p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><h3 id="CNN-for-char-level-representation"><a href="#CNN-for-char-level-representation" class="headerlink" title="CNN for char-level representation"></a>CNN for char-level representation</h3><p>为什么我们需要CNN来encode char-level的信息？因为char-level可以比较好的表示一些词的一些构词特性。比如一些前缀后缀，pre-，post-，un-，im，或者ing、ed等等。</p><p>基本的结构和图像的有</p><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-9-24/56895944.jpg" alt=""></p><p>分享一个github里面开源的Keras实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_build_model</span><span class="params">(self)</span>:</span></div><div class="line">       <span class="string">"""</span></div><div class="line"><span class="string">       Build and compile the Character Level CNN model</span></div><div class="line"><span class="string">       Returns: None</span></div><div class="line"><span class="string">       """</span></div><div class="line">       <span class="comment"># Input layer</span></div><div class="line">       inputs = Input(shape=(self.input_size,), name=<span class="string">'sent_input'</span>, dtype=<span class="string">'int64'</span>)</div><div class="line">       <span class="comment"># Embedding layers</span></div><div class="line">       x = Embedding(self.alphabet_size + <span class="number">1</span>, self.embedding_size, input_length=self.input_size)(inputs)</div><div class="line">       <span class="comment"># Convolution layers</span></div><div class="line">       <span class="keyword">for</span> cl <span class="keyword">in</span> self.conv_layers:</div><div class="line">           x = Convolution1D(cl[<span class="number">0</span>], cl[<span class="number">1</span>])(x)</div><div class="line">           x = ThresholdedReLU(self.threshold)(x)</div><div class="line">           <span class="keyword">if</span> cl[<span class="number">2</span>] != <span class="number">-1</span>:</div><div class="line">               x = MaxPooling1D(cl[<span class="number">2</span>])(x)</div><div class="line">       x = Flatten()(x)</div><div class="line">       <span class="comment"># Fully connected layers</span></div><div class="line">       <span class="keyword">for</span> fl <span class="keyword">in</span> self.fully_connected_layers:</div><div class="line">           x = Dense(fl)(x)</div><div class="line">           x = ThresholdedReLU(self.threshold)(x)</div><div class="line">           x = Dropout(self.dropout_p)(x)</div><div class="line">       <span class="comment"># Output layer</span></div><div class="line">       predictions = Dense(self.num_of_classes, activation=<span class="string">'softmax'</span>)(x)</div><div class="line">       <span class="comment"># Build and compile model</span></div><div class="line">       model = Model(inputs=inputs, outputs=predictions)</div><div class="line">       model.compile(optimizer=self.optimizer, loss=self.loss)</div><div class="line">       self.model = model</div><div class="line">       print(<span class="string">"CharCNNZhang model built: "</span>)</div><div class="line">       self.model.summary()</div></pre></td></tr></table></figure><p>参考<a href="https://github.com/chaitjo/character-level-cnn/blob/master/models/char_cnn_zhang.py" target="_blank" rel="external">https://github.com/chaitjo/character-level-cnn/blob/master/models/char_cnn_zhang.py</a></p><p>CharCNN的其他的一些原理以及技术分享</p><p><a href="https://blog.csdn.net/liuchonge/article/details/70947995" target="_blank" rel="external">https://blog.csdn.net/liuchonge/article/details/70947995</a></p><h3 id="BI-LSTM-to-Encode-Context-Information"><a href="#BI-LSTM-to-Encode-Context-Information" class="headerlink" title="BI-LSTM to Encode Context Information"></a>BI-LSTM to Encode Context Information</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-9-24/50364748.jpg" alt=""></p><p>LSTM unit不需要过多介绍了，简单来说通过构造一个gradient的highway以及与forget、output、input gate之间的互动来解决一部分vanishing gradient的问题。</p><p>至于Bidirectional RNN，<a href="https://www.youtube.com/watch?v=uRFegQXnY54" target="_blank" rel="external">https://www.youtube.com/watch?v=uRFegQXnY54</a></p><p>这个有介绍。因为LSTM只能通过过去predict未来，但是有的时候我们也需要知道句子后面的context，这个时候用Bi-directional RNN会有不错的效果。</p><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-9-24/74031804.jpg" alt=""></p><p>Bi-directional RNN 用人话来讲就是，除了forward的一个LSTM，再加一个backward的LSTM，神经网络的forward pass就是上图中紫色的$a^{<1>},a^{<2>},a^{<3>}，a^{<4>}$，然后绿色的$a^{<4>},a^{<3>},a^{<2>}，a^{<1>}$，backward pass就是反过来。最后每一个time slot都会得到两个hidden state，一个紫色的一个绿色的，一般来说把他们的表示concat然后输入一个activation function(比如softmax)或者当做feature输入别的判别函数即可。</1></2></3></4></4></3></2></1></p><h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><p>我们融合了char、word-level的representation，再加上了context information，通过Bi-LSTM得到了一个隐藏层的表示。接下来我们将这个表示输入一个CRF进行predict。<br>我们令$z = \{z_1, \cdots, z_n \}$表示我们之前影藏层得到的vector（feature vector）<br>我们用$y = \{ y_1, \cdots, y_n \}$表示我们生成的对应的每一个label<br>我们用$Y(z)$来表示可能的label的集合。<br>所以我们的条件概率可以这样表示$P(y | z; W,b)$<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-9-24/39346110.jpg" alt=""><br>最后训练的目标就是得到最大的Maximum Likelihood<br>$L(W,b) = \sum_i log p(y | z; W,b)$<br>最后在做Decoding的时候我们只要search label sequence $y^{\star}$，是的他条件概率最大就可以了！<br>$y^{\star} = argmax_{y \in Y(z)} p(y | z; W, b)$</p><h3 id="最后整体的模型"><a href="#最后整体的模型" class="headerlink" title="最后整体的模型"></a>最后整体的模型</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-9-24/77056616.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;论文摘要-End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNN-CRF&quot;&gt;&lt;a href=&quot;#论文摘要-End-to-end-Sequence-Labeling-via-Bi-directional-LS
      
    
    </summary>
    
      <category term="NLP" scheme="https://jeffchy.github.io/categories/NLP/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/categories/NLP/Paper/"/>
    
    
      <category term="Natural Language Processing" scheme="https://jeffchy.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Paper" scheme="https://jeffchy.github.io/tags/Paper/"/>
    
      <category term="Probability Theory" scheme="https://jeffchy.github.io/tags/Probability-Theory/"/>
    
      <category term="Music Generation" scheme="https://jeffchy.github.io/tags/Music-Generation/"/>
    
  </entry>
  
</feed>
