<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />










  <meta name="baidu-site-verification" content="dXyzlo70j3" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Personal portal for Sharing Computer Science Technology and some petty things in my life">
<meta property="og:type" content="website">
<meta property="og:title" content="Jeff-Chiang">
<meta property="og:url" content="https://jeffchy.github.io/index.html">
<meta property="og:site_name" content="Jeff-Chiang">
<meta property="og:description" content="Personal portal for Sharing Computer Science Technology and some petty things in my life">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jeff-Chiang">
<meta name="twitter:description" content="Personal portal for Sharing Computer Science Technology and some petty things in my life">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://jeffchy.github.io/"/>





  <title>Jeff-Chiang</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-123760763-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d6324a63a8be2ad81d8f3e82174037f4";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jeff-Chiang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Tech and Life</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/09/11/2018-7-2018-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/11/2018-7-2018-9/" itemprop="url">2018.7-2018.9</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-11T15:54:40+08:00">
                2018-09-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/09/11/2018-7-2018-9/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/09/11/2018-7-2018-9/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="我的非典型暑假以及非典型保研"><a href="#我的非典型暑假以及非典型保研" class="headerlink" title="我的非典型暑假以及非典型保研"></a>我的非典型暑假以及非典型保研</h1><p>今天没有总结的心情,之后再补上好吧…</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/09/10/2017-7-2018-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/10/2017-7-2018-7/" itemprop="url">2018.7-2018.9</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-10T15:54:40+08:00">
                2018-09-10
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/09/10/2017-7-2018-7/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/09/10/2017-7-2018-7/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>因为一个小小的契机,想着也该总结一下大三了.</p>
<h1 id="我的大三"><a href="#我的大三" class="headerlink" title="我的大三"></a>我的大三</h1><p>大三是关键的一年,这个在大三没开始的时候我就知道了,<br>学长学姐们唠唠叨叨说:</p>
<ul>
<li>“大三了,还是收心学习吧”</li>
<li>“大三你就该做好决定了,考研?推免?出国?工作?”</li>
<li>“大三再不去实验室就晚了哦”</li>
<li>“大三千万不能挂科啊”</li>
<li>“大三的GPA是最关键的“</li>
<li>“大三的专业课很难啊”<br>上面几条tips,对,也不对.<br>现在我也算是学长了,该轮到我唠唠叨叨了.<blockquote>
<p>总结一个大三的不完全攻略挺难的,说自己的经历却很简单.<br>我是个懒人,害怕说错话,就说说自己好了</p>
</blockquote>
</li>
</ul>
<h1 id="大二暑假"><a href="#大二暑假" class="headerlink" title="大二暑假"></a>大二暑假</h1><p>我决定从大二暑假开始.因为这段实习经历让我想明白了一些事情.<br>大二我机缘巧合,去了一家叫做boonray的初创企业实习,做web开发.<br><a href="http://www.boonray.com/" target="_blank" rel="external">http://www.boonray.com/</a> 这个酷酷的前端,还有所谓的boonray cloud躺在我搭的后端里面.<br>为了这个项目我大概写了3k行python,1k行JS,html很多,不计,几乎是我一个人.<br>一个多月后,我拿了几千块和一个老板送的无人机滚回去上学.</p>
<blockquote>
<p>于是我有了我第一个申请了软著的大型项目[虽然是公司名义],以及第一个实习经历.</p>
</blockquote>
<p>哇哦,请起来好棒棒!我学到了什么呢?</p>
<ul>
<li>自那以后,我再也不想做web开发了.因为我发现,我一个多月累死累活码出来的webapp,没有CTO的博士学历和他掌握的计算机视觉能力值钱.不是说搞web不好,只是我觉得如果我读大学,读研,读出来之后搞普通的开发,我学到的很多东西都用不上,那么我特么读大学干什么.我工位旁边的IOS小哥连很多基本的数学知识都不知道,不照样好得很么.</li>
<li>我想搞AI了,我想读人工智能方向的研究生.</li>
</ul>
<blockquote>
<p>我始终认为,这个经历最关键的不是”working experience”,也不是项目的经验,更不是钱,而是让我确定了比”我读研还是出国?”这种,更大的问题的内心答案.所以我的大三的一些选择,包括最终保研,都顺理成章了.</p>
</blockquote>
<h1 id="大三上"><a href="#大三上" class="headerlink" title="大三上"></a>大三上</h1><p><strong><em>因为我想搞AI,所以:</em></strong></p>
<ul>
<li>我大三上特意选了计算机视觉I,以及随机过程.</li>
<li>我必须想办法读研,而且是AI相关方向的.因为AI就要这么高的门槛</li>
<li>因为我们学校AI方向的教授挺强的,我想好好提一下绩点,时刻保留我本校保研的可能性.</li>
<li>我当时啥也不会,头铁参加了阿里的电话面试,面的岗是AI的算法.被秒拒(我头太铁了吧,菜的真实)</li>
<li>我又参加了一次hackathon,并且用AI的方式解决问题,我带队C位获奖了,Techchrunch上海宝马智能出行项目第二名,虽然我当时可以说还是啥也不懂.</li>
<li>我买了周志华的《机器学习》三天打鱼两天晒网得看.</li>
<li>我有在准备托福.</li>
</ul>
<p>然后我大三上就平平淡淡地这么过去了.<br>啊?等等,就没了?<br>嗯,就没了.<br>你的决定呢,出国?保研?读研?<br>我没想好,你急个啥?…</p>
<h1 id="大三下的前一半"><a href="#大三下的前一半" class="headerlink" title="大三下的前一半"></a>大三下的前一半</h1><p><strong><em>因为我想搞AI,所以:</em></strong></p>
<ul>
<li>我大三下选了<strong><em>人工智能, 优化与机器学习</em></strong></li>
<li>我<strong><em>主动联系导师进了自然语言处理组</em></strong></li>
<li>我<strong><em>自学了CS231n</em></strong></li>
<li>我的组会从刚开始完全听不懂,现在慢慢能听懂了.</li>
<li>我比较认真得对待了我的两个AI的项目.</li>
<li>我跟一个学长做了一个小项目,发EMNLP失败,但是学到了不少.</li>
<li>虽然我才大三,但是我希望能找到,<strong><em>能够动手的AI算法岗作为暑期实习</em></strong></li>
</ul>
<h1 id="大三下的后一半-我也开始急了"><a href="#大三下的后一半-我也开始急了" class="headerlink" title="大三下的后一半,我也开始急了"></a>大三下的后一半,我也开始急了</h1><p>为啥急了呢.<br>基友A决定了,出国!<br>基友B被退学了,然后花钱出国!…<br>基友C决定了,考研!<br>女同学A无敌了,哈佛交流,英语全搞定,出国!<br>女同学B绩点不行,但是志向是拿到游戏公司offer,工作!<br>身边人都决定了就剩下你,你急不急?<br>急死老夫了好吧…<br>所以你就该自然而然下决定了</p>
<h2 id="决定了-保研吧"><a href="#决定了-保研吧" class="headerlink" title="决定了,保研吧?"></a>决定了,保研吧?</h2><ul>
<li>因为老美top学校AI专业出国难上加难,超级无敌难,再加上之前进的组顶会成果也很不错,学风严谨,一点也不划,导师也挺牛的.学长们的出路都非常不错.我出国的意愿开始动摇,本校保研该组的意愿不断上升.</li>
<li>因此我必须提高我的专业课绩点,因为保研要用</li>
<li>我不比赛了,收心学习.</li>
<li>幸不辱命,我这学期4门专业课满绩啦</li>
</ul>
<h3 id="大二暑假和整个大三-我想明白了我想要做什么-并且决定保研-同时平时的一些积累也给我成功保研打下了基础-时间很快就来到这个暑假了-2018-7-2018-9-有必要另开一篇博客说说-我的非典型暑假以及我的超级非典型保研之路"><a href="#大二暑假和整个大三-我想明白了我想要做什么-并且决定保研-同时平时的一些积累也给我成功保研打下了基础-时间很快就来到这个暑假了-2018-7-2018-9-有必要另开一篇博客说说-我的非典型暑假以及我的超级非典型保研之路" class="headerlink" title="大二暑假和整个大三,我想明白了我想要做什么,并且决定保研,同时平时的一些积累也给我成功保研打下了基础,时间很快就来到这个暑假了,2018.7-2018.9,有必要另开一篇博客说说,我的非典型暑假以及我的超级非典型保研之路"></a>大二暑假和整个大三,我想明白了我想要做什么,并且决定保研,同时平时的一些积累也给我成功保研打下了基础,时间很快就来到这个暑假了,2018.7-2018.9,有必要另开一篇博客说说,我的非典型暑假以及我的超级非典型保研之路</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/25/XiaoIce-Image2poetry/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/25/XiaoIce-Image2poetry/" itemprop="url">XiaoIce Image to Poetry 论文解读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-25T14:57:37+08:00">
                2018-08-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/25/XiaoIce-Image2poetry/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/25/XiaoIce-Image2poetry/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>今天继续介绍好玩的小冰系列, 简要介绍一下模型.论文的地址是:<br><a href="https://arxiv.org/abs/1808.03090" target="_blank" rel="external">Image Inspired Poetry Generation in XiaoIce</a><br>再啰嗦一下,说这篇文章的原因是笔者最近在做Image-Captioning,但是却觉得End2end的模型的表现一般,对训练数据的质量以及数量比较敏感.在寻思着有没有更好的方法.</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>这篇文章的主要目的是通过图片生成<strong>现代诗</strong><br>使用的方法简单来说有以下几步</p>
<ul>
<li>keyword and sentiment extraction from image</li>
<li>keywords expands to poetry-like words</li>
<li>using RNN to generate verse</li>
</ul>
<h2 id="Keyword-extraction"><a href="#Keyword-extraction" class="headerlink" title="Keyword extraction"></a>Keyword extraction</h2><p>第一步是对一张图片做keyword extraction,用的是很常用的做法,用一个pretrained的CNN(如VGG16),我们得到一个能够表示图片feature的vector,比如4096维,通常的做法是我们接着跟一个fully-connected layer做一个分类,然后用一个softmax把数值转化为概率,这是一个非常常用的做法,就不细说,所以假设<br>$I$是给定的图片,我们有一个category,$C$,我们的目标是求出$P(C|I)$,其中针对名次Noun和Adj两种磁性的词分别做了两个神经网络来train并且predict.</p>
<h2 id="Sentence-Model"><a href="#Sentence-Model" class="headerlink" title="Sentence Model"></a>Sentence Model</h2><p>我们通过RNN来构造一个Language Mode,这个模型叫RNNLM<br>$w_i = argmax_{w} P(w|w_{1:i-1})$</p>
<h2 id="Recursive-Generation"><a href="#Recursive-Generation" class="headerlink" title="Recursive Generation"></a>Recursive Generation</h2><p>我们做了keword extraction之后,我们需要通过一个keyword作为seed,生成一个句子,但是如果之后forward的RNNLM的话,我们只能往后生成,但是<strong>我们的关键词不一定在句首啊</strong>,所以解决办法也很简单,弄一个反过来的language model<br>$w_i = argmax_{w} P(w|w_{n:i+1})$然后分别向前向后生成就完事啦<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-26/79659149.jpg" alt=""></p>
<h2 id="Generation-With-Previous-Line"><a href="#Generation-With-Previous-Line" class="headerlink" title="Generation With Previous Line"></a>Generation With Previous Line</h2><p>我们在生成的时候把之前的一行的信息也Encode了并且作为input<br>$w_i = argmax_{w} P(w|w_{1:i, l_{j-1}})$</p>
<h2 id="Hierarchy-Poem-Model"><a href="#Hierarchy-Poem-Model" class="headerlink" title="Hierarchy Poem Model"></a>Hierarchy Poem Model</h2><p>$w_i = argmax_{w} P(w|w_{1:i, l_{1:j-1}})$<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-26/50361494.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/24/XiaoIce-Band-论文解读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/24/XiaoIce-Band-论文解读/" itemprop="url">XiaoIce Band 论文解读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-24T10:33:22+08:00">
                2018-08-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/24/XiaoIce-Band-论文解读/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/24/XiaoIce-Band-论文解读/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近小冰一波论文疯狂刷屏,是深度学习的工程应用的良好例子,也看起来非常有趣.这些论文解决的问题都不简单,通过看他们应该可以受到一些解决问题上的启发.今天看的论文是KDD 2018上的Best Student Paper,<br><a href="https://dl.acm.org/authorize.cfm?key=N665888" target="_blank" rel="external">“XiaoIce Band: A Melody and Arrangement Generation Framework for Pop Music”</a>,再牢骚两句,觉得这论文有趣其实还是因为身边有一个想做音乐生成的朋友,同时在AI的课上有一个小组的项目使用LSTM做了简单的音乐生成(其实挺基础的),没有融合乐理的很多constraint,她们只用了midi,当成单轨,当时我就在底下提问了,多轨的效果试过么?这篇文章训练了多个音轨,同时用了乐理作为约束,我觉得和我当时想的一样,这才是music generation该有的样子hhhh,这个任务想必也挺困难的,我们简要看一下它是怎么做的吧😄<br>笔者想做的是NLP,但是看这篇文章应该也有不少启发.</p>
<h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h3><ul>
<li>这篇文章考虑了 <strong><em>和弦走向</em></strong>, 做音乐的朋友们会知道,写歌中和弦走向是很重要的一部分.什么 4536啊6415啊这些,现在的流行歌曲大都会采用一种好听的和弦的走向.</li>
<li>这篇文章考虑了 multi-track, 多轨,以及之间的和谐</li>
<li>这篇文章考虑了节奏和旋律的和谐 针对单轨  </li>
<li>这篇文章考虑了编曲 针对多轨</li>
</ul>
<p>所以本文提出了两个模型</p>
<ul>
<li>Chord based Rhythm and Melody Cross-Generation Model (CRMCG)</li>
<li>Multi-Instrument Co-Arrangement Model (MICA)<br>分别做旋律节奏(根据和弦走向)的生成和编曲</li>
</ul>
<h3 id="整体模型结构-PIPILELINE"><a href="#整体模型结构-PIPILELINE" class="headerlink" title="整体模型结构 PIPILELINE"></a>整体模型结构 PIPILELINE</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/39386920.jpg" alt=""></p>
<ul>
<li>数据预处理, 把训练的歌曲提取出必要的元素</li>
<li>单轨生成模型CRMCG</li>
<li>多轨生成模型MICA</li>
<li>生成音乐</li>
</ul>
<h2 id="CRMCG"><a href="#CRMCG" class="headerlink" title="CRMCG"></a>CRMCG</h2><p>CRMCG是单轨生成模型,它的主要逻辑是这样的,是一个seq2seq</p>
<ul>
<li>INPUT: 人为给定和弦 $C = \{ c_1, c_2, \cdots, c_N \}$</li>
<li>OUTPUT: 生成对应小节(一个和弦走向的和弦通常对应一个小节)  $P = \{ p_1, p_2, \cdots, p_N \}$</li>
<li>因此每个小节的旋律$M_i$都和$C$有关</li>
</ul>
<h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/38082880.jpg" alt=""></p>
<h3 id="Chord"><a href="#Chord" class="headerlink" title="Chord"></a>Chord</h3><p>我们对输入的和弦序列$C$做Embedding,并且输入GRU,得到每个序列中和弦的隐藏层表示.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/4830501.jpg" alt=""><br>用这个来encode 我们的和弦输入</p>
<h3 id="Rhythm-amp-Melody-Encoder"><a href="#Rhythm-amp-Melody-Encoder" class="headerlink" title="Rhythm &amp; Melody Encoder"></a>Rhythm &amp; Melody Encoder</h3><p>这个的生成也类似,首先拿到embedding vector<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/64572589.jpg" alt=""><br>然后,通过一个GRU来得到<strong>每个小节中的对应Rhythm序列和Melody序列的</strong>encoding表示<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/94700231.jpg" alt=""></p>
<h3 id="Rhythm-Decoder"><a href="#Rhythm-Decoder" class="headerlink" title="Rhythm Decoder"></a>Rhythm Decoder</h3><p>Encoder讲完了,接着开始讲decoder, Rhythm与Chord没什么关系,但是由于要与旋律和谐,它与前一时刻的自己和Melody是有关的,所以我们把之前得到的隐藏层concat到一起(这是非常常用的合并信息的操作),然后过一个Dense Layer,然后再过一个 Relu 的Activation, 最后输入另外一个GRU作为decoder,接一个Softmax表示成生成Rhythm的概率分布,condition on已知的小节信息.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/49026522.jpg" alt=""></p>
<h3 id="Melody-Decoder"><a href="#Melody-Decoder" class="headerlink" title="Melody Decoder"></a>Melody Decoder</h3><p>Melody的Decoder与Rhythm的类似,区别就是我们需要考虑chord进来,因为她们是相关的<br>这一时刻的Melody与现在时刻的Rhythm,Chord以及前一时刻的自己有关<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/15181689.jpg" alt=""><br>然后类似:<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/62267076.jpg" alt=""></p>
<h3 id="Summary-of-CRMCG"><a href="#Summary-of-CRMCG" class="headerlink" title="Summary of CRMCG"></a>Summary of CRMCG</h3><p>这张图还是总结地很好的.<br>CRMCG是一个标准的的Encoder-Decoder模型,在一个有趣场景中的使用例子,值得学习.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/47136008.jpg" alt=""><br>Encoder-Decoder的基本框架<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/50085942.jpg" alt=""></p>
<h2 id="MICA-这个模型笔者也没有完全看懂-只能稍作解释-最好自己看文辩证理解一下…"><a href="#MICA-这个模型笔者也没有完全看懂-只能稍作解释-最好自己看文辩证理解一下…" class="headerlink" title="MICA 这个模型笔者也没有完全看懂,只能稍作解释,最好自己看文辩证理解一下…."></a>MICA 这个模型笔者也没有完全看懂,只能稍作解释,最好自己看文辩证理解一下….</h2><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/18460744.jpg" alt=""><br>这篇文章最强也是最难的地方是MICA的建模.如何把这样一个问题建模成end2end模型,非常有技术含量…<br>每个音轨,我们通过CRMCG生成的单轨信息(拿的是input的hidden state?),然后利用attention机制建模tracks之间的相互关系,然后修改了GRU门控单元joint地生成.<br>什么事attention机制?看看这个<a href="https://zhuanlan.zhihu.com/p/31547842" target="_blank" rel="external">知乎链接</a>,会有一个比较直观的了解.</p>
<p>原本的GRU<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/4063736.jpg" alt=""></p>
<h3 id="Attention-Cell-建模track之间的关系-模型图中竖着的方框"><a href="#Attention-Cell-建模track之间的关系-模型图中竖着的方框" class="headerlink" title="Attention Cell 建模track之间的关系[模型图中竖着的方框]"></a>Attention Cell 建模track之间的关系[模型图中竖着的方框]</h3><p>Attention<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/98664645.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/9770713.jpg" alt=""></p>
<h3 id="MLP-Cell-建模某个track-的-hidden-state在-整个音乐-中的重要性"><a href="#MLP-Cell-建模某个track-的-hidden-state在-整个音乐-中的重要性" class="headerlink" title="MLP Cell 建模某个track 的 hidden state在[整个音乐]中的重要性"></a>MLP Cell 建模某个track 的 hidden state在[整个音乐]中的重要性</h3><p>我的理解就是每一次都把这个时间段所有task的隐藏层考虑进来,并且通过一个$W^i,b^i$(最后一行)的参数来建模每个track对整体音乐的重要性<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/74231115.jpg" alt=""></p>
<h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>loss 其实是 conditional log-likelyhood<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-24/8433169.jpg" alt=""></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>四个字:很能折腾….<br>它的处理多个track协同训练的方法也许值得借鉴?<br>同时音乐的生成部分也可圈可点,作为一个encoder decoder的生动例子…</p>
<p>但是我总觉得这个模型可能只有微软能调出来…</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/22/word2vec-introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/22/word2vec-introduction/" itemprop="url">word2vec-introduction</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-22T11:21:54+08:00">
                2018-08-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Note/" itemprop="url" rel="index">
                    <span itemprop="name">Note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/22/word2vec-introduction/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/22/word2vec-introduction/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>This Note is based on the Ng’s <strong>sequential model</strong> class in <a href="deeplearning.ai">deeplearning.ai</a>, along with some of my understandings.<br>Most of the images below are screenshots of the Ng’s class, I strongly recommend taking this sequential model class, it’s a pretty well introduction.</p>
<h2 id="Word-representation"><a href="#Word-representation" class="headerlink" title="Word representation"></a>Word representation</h2><h3 id="One-hot"><a href="#One-hot" class="headerlink" title="One hot"></a>One hot</h3><p>one traditional way to represent word is through one-hot vectors.<br>If you have 100k vocabulary, your 1-hot vector size will be dim 1x100k, with the specific represented word setting to 1 and all the other elements set to 0s. This is an example:<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/28049823.jpg" alt=""></p>
<p>What will be the drawbacks of one-hot vectors?</p>
<ul>
<li>computational and memory expensive when vocabulary size is really large</li>
<li>word vectors are independent with each other, $v_1={China}, v_2={America}$, they are both country, so they are <em>correlated</em>, but the one-hot vectors says that $v_1 \cdot v_2 = 0$, they are perpendicular, thus independent and uncorrelated, so one-hot vector is easy to get but we lose some information.</li>
</ul>
<h3 id="feature-representation-word2vec"><a href="#feature-representation-word2vec" class="headerlink" title="feature representation - word2vec"></a>feature representation - word2vec</h3><p>So to solve the problems, we want a vector representation like the following<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/986348.jpg" alt=""><br>Each word vector is 128 dim, in continous vector space, a word can be represented by it, and the similarity of word can be encoded into it.<br>And we can do operations on the learned representations like this:</p>
<blockquote>
<p>$v_{queen} - v_{king} == v_{woman} - v_{man}$</p>
</blockquote>
<p>We often use a embedding matrix, it multiply a one-hot vector, can returns a current embedding<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/1375136.jpg" alt=""></p>
<p>How can we do this?</p>
<h3 id="How-we-use-word-embeddings"><a href="#How-we-use-word-embeddings" class="headerlink" title="How we use word embeddings?"></a>How we use word embeddings?</h3><p>Using word embedding is like transfer learning, we train it on big corpus unsupervisedly, and apply it onto a particular problem (usually smaller sized), as the prior knowledge.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/2581072.jpg" alt=""></p>
<blockquote>
<p>Intuition:<br>We predict target word by given context words, and the by-product of it are word embedding $P(t|c)$</p>
</blockquote>
<h2 id="How-to-do-learn-word-embedding"><a href="#How-to-do-learn-word-embedding" class="headerlink" title="How to do learn word embedding"></a>How to do learn word embedding</h2><h3 id="V1-use-previous-few-words-by-product-of-learning-language-model"><a href="#V1-use-previous-few-words-by-product-of-learning-language-model" class="headerlink" title="V1 use previous few words (by-product of learning language model)"></a>V1 use previous few words (by-product of learning language model)</h3><p>given sentences, retreive the corresponding word embeddings from embedding matrix $E$, and feed the feched word vectors into a fully conneted hidden layer, and then feed into a softmax with parameter matrix, (first map the vector dim into vocabulary size, than take softmax, equivalent to a fully connected layer and a softmax operation), after softmax, we can get the word by argmax, and the embedding matrix weights learned can be used as word embeddings.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/72304825.jpg" alt=""></p>
<h3 id="other-context-center-pair-to-learn-word-embedding"><a href="#other-context-center-pair-to-learn-word-embedding" class="headerlink" title="other (context / center) pair to learn word embedding"></a>other (context / center) pair to learn word embedding</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/42202177.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/61894626.jpg" alt=""></p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul>
<li>one-hot vectors</li>
<li>$E$ embedding matrix</li>
<li>retrived embedding vectors $e$</li>
<li>softmax layer (fully connected hidden layer + softmax operation), map to size of $|V|$</li>
<li>NLL loss</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/20/Image-Data-Augmentation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/20/Image-Data-Augmentation/" itemprop="url">Image Data Augmentation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-20T18:12:34+08:00">
                2018-08-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Code/" itemprop="url" rel="index">
                    <span itemprop="name">Code</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/20/Image-Data-Augmentation/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/20/Image-Data-Augmentation/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Image-Data-Augmentation-in-Python"><a href="#Image-Data-Augmentation-in-Python" class="headerlink" title="Image Data Augmentation in Python"></a>Image Data Augmentation in Python</h3><p>因为深度学习需要较多的数据进行训练,所以在图像数据集比较少的时候,我们可以通过对已知图像进行变换来增加数据集的大小.这是深度学习中很重要的一个trick. 笔者最近也正好需要进行一次数据增广,用简单的例子给大家说明一下如何使用</p>
<h2 id="Augmentor"><a href="#Augmentor" class="headerlink" title="Augmentor"></a>Augmentor</h2><p>推荐使用python library: <a href="https://github.com/mdbloice/Augmentor" target="_blank" rel="external">Augmentor</a></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install Augmentor</div></pre></td></tr></table></figure>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># import lib</span></div><div class="line"><span class="keyword">import</span> Augmentor</div><div class="line"></div><div class="line"><span class="comment"># set origin data root dir, contains the data</span></div><div class="line"><span class="comment"># images that you want to augment</span></div><div class="line">p = Augmentor.Pipeline(<span class="string">"../falldown_augment/"</span>)</div><div class="line"></div><div class="line"><span class="comment"># set the transforms</span></div><div class="line">p.random_distortion(probability=<span class="number">1</span>, grid_width=<span class="number">4</span>, grid_height=<span class="number">4</span>, magnitude=<span class="number">8</span>)</div><div class="line">p.random_color(<span class="number">0.6</span>, <span class="number">0.1</span>, <span class="number">0.7</span>)</div><div class="line">p.rotate(probability=<span class="number">0.7</span>, max_left_rotation=<span class="number">10</span>, max_right_rotation=<span class="number">10</span>)</div><div class="line">p.zoom(probability=<span class="number">0.5</span>, min_factor=<span class="number">1.1</span>, max_factor=<span class="number">1.5</span>)</div><div class="line">p.flip_left_right(probability=<span class="number">0.5</span>)</div><div class="line"></div><div class="line"><span class="comment"># sample due to the config</span></div><div class="line">p.sample(<span class="number">500</span>)</div></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>用以上代码可以将77张图片增广成500张,根据我们设置的变换以及相应的概率.<br>这个库代码简洁,功能也比较全,支持多线程,可以和keras/pytorch进行交互,(融入成generator)<br>总的来说功能够用也好用,详细的大家看他的<a href="http://augmentor.readthedocs.io/" target="_blank" rel="external">doc</a>或者github主页吧.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/10/Unsupervised-Neural-Dependency-Parsing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/10/Unsupervised-Neural-Dependency-Parsing/" itemprop="url">Neural Unsupervided Dependency Parsing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-10T23:31:21+08:00">
                2018-08-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/10/Unsupervised-Neural-Dependency-Parsing/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/10/Unsupervised-Neural-Dependency-Parsing/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Unsupervised-Neural-Dependency-Parsing-Y-Jiang-et-al"><a href="#Unsupervised-Neural-Dependency-Parsing-Y-Jiang-et-al" class="headerlink" title="Unsupervised Neural Dependency Parsing[Y. Jiang et. al]"></a>Unsupervised Neural Dependency Parsing[Y. Jiang et. al]</h2><p>今天介绍的论文是 Unsupervised Neural Dependency Parsing <a href="https://aclweb.org/anthology/D16-1073" target="_blank" rel="external">Y. Jiang et. al</a>的中文提要</p>
<h3 id="什么是无Unsupervised-Dependency-Parsing？这篇文章的主要工作"><a href="#什么是无Unsupervised-Dependency-Parsing？这篇文章的主要工作" class="headerlink" title="什么是无Unsupervised Dependency Parsing？这篇文章的主要工作"></a>什么是无Unsupervised Dependency Parsing？这篇文章的主要工作</h3><p><a href="/2018/08/05/DMV/">Unsupervised Dependency Parsing</a><br>再简要总结一下Unsupervided Dependency Parsing的特点</p>
<ul>
<li>输入是带有POS的句子，但是不带有具体的parse tree</li>
<li>通常使用基于<a href="/2018/08/04/EM/">EM</a>算法的方法来估计每个”grammar rules”的参数(概率)</li>
<li>传统模型通常使用特征(features)和归纳偏置(inductive bias)来将一些有用的先验信息incorporate进模型中，比如grammar rules的correlation使得模型达到更好的效果。通常是一些手动design的featue和特别的prior distribution。</li>
<li>这篇文章创新地使用了神经网络来自动地在预测 grammars rules 的参数，基于对POS tags的离散表示，而这些离散表示是自动地从corpus中间学习得到的。这些离散表示可以很好地将POS之间的correlation 进去。这里有一些类似word2vec模型？</li>
<li>最终这篇文章的模型达到了非常好的效果</li>
</ul>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li>之前大多做Unsupervised Dependency Parsing（以下简称UDP）的工作多是基于<a href="2018/08/05/DMV/">DMV</a>,在DMV的基础上添加多种inductive bias,或者handcrafted features来将POS tags 之间的相关性correlation encode进模型之中。</li>
<li>但是本文做事使用了神经网络来做两件事情<ul>
<li>学习得到每个POS tags的离散表示，生成POS embedding</li>
<li>根据POS embedding 预测 每个 grammar rules 的概率（更新概率）</li>
</ul>
</li>
<li>因此我们学习到的embeddings可以自动地encode我们词之间的correlation，而不需要hand crafted feature.</li>
<li>同时作者提到，POS的相关性导致的需要smoothing的问题已经被神经网络自动地解决了<ul>
<li>Smoothing是什么？smoothing通常被用来解决监督学习中某些数据非常稀疏的问题。smooth在无监督学习中也有用处，<a href="http://www.aclweb.org/anthology/N09-1012" target="_blank" rel="external">Headden et.al 2009 NAACL</a>,利用分部之间的线性插值来缓解</li>
</ul>
</li>
</ul>
<h2 id="DMV"><a href="#DMV" class="headerlink" title="DMV"></a>DMV</h2><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>DMV是一个UDP的生成模型，具体细节可以参考这一篇文章<a href="/2018/08/05/DMV/">DMV</a><br>先说一些notations</p>
<ul>
<li>$dec$ STOP or COND 是否继续生成parse tree</li>
<li>$h$ head POS</li>
<li>$dir$ LEFT or RIGHT</li>
<li>$c$ child POS</li>
<li>$val$ boolean | whether current head POS already has a child in current direction<br>简单来说呢就是，这个模型有三种类型的rules:</li>
<li><strong>CHILD</strong> $P_{CHILD}(c|h,dir,val)$ 已知head和val,往给定方向生成儿子的概率</li>
<li><strong>DECISION</strong> $P_{DECISION}(dec|h,dir,val)$ …是否继续生成的概率</li>
<li><strong>ROOT</strong> $P_{ROOT}(c|root)$ …root生成某个儿子的概率</li>
</ul>
<h3 id="Drawbacks"><a href="#Drawbacks" class="headerlink" title="Drawbacks"></a>Drawbacks</h3><p>DMV模型还有不少缺点</p>
<ul>
<li>模型过度简化 <a href="http://www.aclweb.org/anthology/N09-1012" target="_blank" rel="external">Headden et.al 2009 NAACL</a>使用 <a href="https://blog.csdn.net/u013515273/article/details/78273342" target="_blank" rel="external">lexicalization</a> 来解决</li>
<li><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-8/47430962.jpg" alt=""></li>
<li><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-8/76592145.jpg" alt=""></li>
</ul>
<h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><ul>
<li>EM 算法来 learning DMV model (Inside-Outside)<ul>
<li>E-step: Learn expected counts - 求出后验概率 $p(y | \theta, x^{(i)})$</li>
<li>M-step: Normalize parameters - <img src="http://oj4pv4f25.bkt.clouddn.com/18-8-9/60137963.jpg" alt=""></li>
</ul>
</li>
<li>但是EM算法是有缺陷的<ul>
<li>比如 non-convex 容易收敛到local optimal</li>
<li>initialization matters</li>
</ul>
</li>
</ul>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>接下来就是最关键的 Model 了。<br><strong>我们希望用神经网络来替代M-step中的新参数估计阶段</strong></p>
<h3 id="E-step"><a href="#E-step" class="headerlink" title="E-step"></a>E-step</h3><p>E-step保持不变，和原本的算法类似，我们可以通过计算inside-outside score来获得我们的expected counts这我们计算出了三种类型的rules在每个句子中$x \in X$中的出现的expected counts</p>
<ul>
<li>$e_c(x_i)$ CHILD rule 在该句子中的expected counts, $p_c$ CHILD rule在整个corpus中出现的分布parameter(概率)</li>
<li>$e_d(x_i)$ DECISION rule 在该句子中的expected counts, $p_d$ CHILD rule在整个corpus中出现的分布parameter(概率)</li>
<li>$e_r(x_i)$ ROOT rule 在该句子中的expected counts, $p_r$ CHILD rule在整个corpus中出现的分布parameter(概率)<br>我们通过<a href="/2018/08/05/Inside-Outside-Algorithm/">Insode-outside</a>计算出了这些量，也就是所谓的expected count</li>
</ul>
<h3 id="原本的M-step"><a href="#原本的M-step" class="headerlink" title="原本的M-step"></a>原本的M-step</h3><p>原本的M-step其实是优化一个expected log likelihood，找的最优的参数,只不过我们面临的通常是一个 multinomial 的分布，所以我们可以通过数学方法证明，参数的更新其实就是一个normalize。如图：如果对证明方法感兴趣的话，可以去看这个M. Collins大佬 <a href="http://www.cs.columbia.edu/~mcollins/em.pdf" target="_blank" rel="external">note</a> 的最后一部分<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-9/60137963.jpg" alt=""><br>我们这篇文章的$ELL(\theta)$便是</p>
<script type="math/tex; mode=display">ELL(\theta)=\sum_{\alpha=1}^N(\sum_c e_c(x_i)\log{p_c}+\sum_d e_d(x_i)\log{p_d}+\sum_r e_r(x_i)\log{p_r})</script><p>这里这个式子的最优解按照往常normalize一下即可<br>但是这里我们不直接normalize,而是通过训练一个神经网络来学会通过POS,direction,valence等信息来predict出这个rule在这个corpus中的概率!所以这个神经网络的objective function可以就看做事这个$ELL(\theta)$，不过这里面$\theta$的含义就加上了神经网络的权重参数了。而之前的各种correlation我们通过了POS的embedding融合进进了模型中间，所以作者认为这个神经网络可以被看做是在M-step的优化过程中加上了一个regulation项，这个regulation项encode进了POS之间的correlation.</p>
<h2 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h2><p>我们用一个神经网络来建模每个rule的概率的predict,拿child rule来举例</p>
<ul>
<li>$P_{CHILD}(c|h,dir,val)$</li>
<li>神经网络的输入就是 $h,dir,val$</li>
<li>输出就是$P_{CHILD}(c|h,dir,val)$<h3 id="Detail"><a href="#Detail" class="headerlink" title="Detail"></a>Detail</h3>需要能够将离散的POS tag输入神经网络进行计算，我们需要把它转化为离散表示，和word2vec的目的是一致的，我们也许可以叫他POS2Vec。</li>
<li>我们记所有的POS tag的集合为$T$</li>
<li>所有的head tag as a d-dim vector $v_h \in R^d$</li>
<li>val as a $d’$-dim vector</li>
<li>input layer: $[(d+d’) \times 1]$ vector。<ul>
<li>我们把 val 和某一个要预测的 head pos tag concat起来作为输入 $[v_h;v_{val}]$</li>
</ul>
</li>
<li>hidden layer: <ul>
<li>我们把输入的vector通过一个矩阵$W_{dir}, [d_h \times (d+d’)]$的transform。<ul>
<li>注意这里的$W_{dir} \in W_{left}, W_{right}$这两个是分开独立的，对不同的方向我们有一个不同的神经网络。</li>
</ul>
</li>
<li>然后通过一个Relu的non-linear</li>
<li>hidden layer output: $f = ReLU(W_{dir}[v_h;v_{val}])$ <ul>
<li>$[d_h \times 1]$ vector</li>
</ul>
</li>
</ul>
</li>
<li>softmax layer:<ul>
<li>经过了隐层的变换之后我们需要把这个vector map到概率！</li>
<li>我们的Embedding matrix隆重登场了！$W^{|T| \times d_h}$</li>
<li>然后我们过一个softmax就有了我们最终的概率结果！</li>
<li>$[p_{c_1}, \cdots, p_{c_{|T|}}] = Softmax(W^Tf)$</li>
<li>output: $[|T| \times 1]$ vector!<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-9/53439112.jpg" alt=""></li>
</ul>
</li>
</ul>
<h3 id="emmmm"><a href="#emmmm" class="headerlink" title="emmmm"></a>emmmm</h3><p>一些别的注意点</p>
<ul>
<li>Decision rule 的学习和 Child rule 使用一个结构，只不过output改变一下</li>
<li>$W_{dir}$和在Decision/Child中是共享参数的</li>
<li>embedding size $d_h$ 是个很重要的hyper parameter</li>
</ul>
<h3 id="Learning-Procedure"><a href="#Learning-Procedure" class="headerlink" title="Learning Procedure"></a>Learning Procedure</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-10/51173510.jpg" alt=""></p>
<h2 id="results"><a href="#results" class="headerlink" title="results"></a>results</h2><p>效果自然是不错滴<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-10/89507397.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-10/98180433.jpg" alt=""></p>
<h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><p>这篇文章的一个优势是用神经网络来替代DMV中的M-step。<br>在效果很不错的基础上<br>用一个很简单的网络结构解决了通常需要很复杂模型才能解决的问题<br>这一点是非常有意义的。<br>同时作者比较细心得讨论并且可视化了神经网络学习到的一些结果，<br>很好地证明了这篇工作Smooth correlate POS的Motivation得到了解决。（关于这部分不难，直接看原文吧）</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/10/Correlate-and-dependent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/10/Correlate-and-dependent/" itemprop="url">Correlation and Dependence</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-10T15:31:21+08:00">
                2018-08-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/10/Correlate-and-dependent/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/10/Correlate-and-dependent/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Correlation"><a href="#Correlation" class="headerlink" title="Correlation"></a>Correlation</h3><ul>
<li>两个随机变量$X,Y$相关通常指的是<strong>线性相关</strong>，直观上理解就是，一个变大另一个也相应变大（或反向）</li>
<li>$Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}} = 0$表示线性无关</li>
<li>线性无关 $Corr(X,Y)=0$不推出独立，因为可能会有非线性的关系</li>
</ul>
<h3 id="Dependence"><a href="#Dependence" class="headerlink" title="Dependence"></a>Dependence</h3><ul>
<li>两个随机变量$X,Y$如果独立</li>
<li>$P(X)P(Y)=P(X,Y), \ \ P(X|Y)=P(X)$</li>
<li>独立推出无关</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/08/Inductive-Bias/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/08/Inductive-Bias/" itemprop="url">Inductive Bias</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-08T23:51:21+08:00">
                2018-08-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning-Theory/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning Theory</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning-Theory/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/08/Inductive-Bias/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/08/Inductive-Bias/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h3><p>Inductive Bias 归纳偏置是机器学习中的一个重要常见的概念<br>摘自Wikipedia</p>
<blockquote>
<p>当学习器去预测其未遇到过的输入的结果时，会做一些假设（Mitchell, 1980）。而学习算法中的归纳偏置则是这些假设的集合。</p>
</blockquote>
<h3 id="机器学习中常见的归纳偏置列表："><a href="#机器学习中常见的归纳偏置列表：" class="headerlink" title="机器学习中常见的归纳偏置列表："></a>机器学习中常见的归纳偏置列表：</h3><ul>
<li>最大条件独立性（conditional independence）：如果假说能转成贝叶斯模型架构，则试着使用最大化条件独立性。这是用于朴素贝叶斯分类器（Naive Bayes classifier）的偏置。</li>
<li>最小交叉验证误差：当试图在假说中做选择时，挑选那个具有最低交叉验证误差的假说，虽然交叉验证看起来可能无关偏置，但天下没有免费的午餐理论显示交叉验证已是偏置的。</li>
<li>最大边界：当要在两个类别间画一道分界线时，试图去最大化边界的宽度。这是用于支持向量机的偏置。这个假设是不同的类别是由宽界线来区分。</li>
<li>最小描述长度（Minimum description length）：当构成一个假设时，试图去最小化其假设的描述长度。假设越简单，越可能为真的。见奥卡姆剃刀。</li>
<li>最少特征数（Minimum features）：除非有充分的证据显示一个特征是有效用的，否则它应当被删除。这是特征选择（feature selection）算法背后所使用的假设。</li>
<li>最近邻居：假设在特征空间（feature space）中一小区域内大部分的样本是同属一类。给一个未知类别的样本，猜测它与它最紧接的大部分邻居是同属一类。这是用于最近邻居法的偏置。这个假设是相近的样本应倾向同属于一类别。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/08/Prior-Distribution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/08/Prior-Distribution/" itemprop="url">Prior & Posterial</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-08T23:31:21+08:00">
                2018-08-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Probability-Theory/" itemprop="url" rel="index">
                    <span itemprop="name">Probability Theory</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Probability-Theory/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/08/Prior-Distribution/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/08/Prior-Distribution/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h3><ul>
<li>$\theta$ parameters of distribution</li>
<li>$X$ data R.V</li>
<li>$P(\theta)$ Prior, Prior distribution when not seeing data</li>
<li>$P(\theta | X)$ posterior distribution when seen data</li>
<li>$P(X | \theta)$ Likelihood</li>
<li>$P(X)$ data distribution</li>
</ul>
<h3 id="Prior-amp-Posterior"><a href="#Prior-amp-Posterior" class="headerlink" title="Prior &amp; Posterior"></a>Prior &amp; Posterior</h3><script type="math/tex; mode=display">P(\theta | X) = \frac{P(\theta)P(X|\theta)}{P(X)}</script><script type="math/tex; mode=display">P(X) = \int_\theta P(X|\theta)P(\theta)d \theta</script>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Jeff Chiang" />
          <p class="site-author-name" itemprop="name">Jeff Chiang</p>
           
              <p class="site-description motion-element" itemprop="description">Personal portal for Sharing Computer Science Technology and some petty things in my life</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">52</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">34</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/jeffchy" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="jiangchy@shanghaitech.edu.cn" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                    
                      E-Mail
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/jeffchiang/" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      微博
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jeff Chiang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jeffchy-github-io.disqus.com/count.js" async></script>
    

    

  




	





  










  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
