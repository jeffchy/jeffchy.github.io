<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />










  <meta name="baidu-site-verification" content="dXyzlo70j3" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Personal portal for Sharing Computer Science Technology and some petty things in my life">
<meta property="og:type" content="website">
<meta property="og:title" content="Jeff-Chiang">
<meta property="og:url" content="https://jeffchy.github.io/page/2/index.html">
<meta property="og:site_name" content="Jeff-Chiang">
<meta property="og:description" content="Personal portal for Sharing Computer Science Technology and some petty things in my life">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jeff-Chiang">
<meta name="twitter:description" content="Personal portal for Sharing Computer Science Technology and some petty things in my life">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://jeffchy.github.io/page/2/"/>





  <title>Jeff-Chiang</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-123760763-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d6324a63a8be2ad81d8f3e82174037f4";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jeff-Chiang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Tech and Life</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/22/word2vec-introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/22/word2vec-introduction/" itemprop="url">word2vec-introduction</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-22T11:21:54+08:00">
                2018-08-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Note/" itemprop="url" rel="index">
                    <span itemprop="name">Note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/22/word2vec-introduction/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/22/word2vec-introduction/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>This Note is based on the Ng’s <strong>sequential model</strong> class in <a href="deeplearning.ai">deeplearning.ai</a>, along with some of my understandings.<br>Most of the images below are screenshots of the Ng’s class, I strongly recommend taking this sequential model class, it’s a pretty well introduction.</p>
<h2 id="Word-representation"><a href="#Word-representation" class="headerlink" title="Word representation"></a>Word representation</h2><h3 id="One-hot"><a href="#One-hot" class="headerlink" title="One hot"></a>One hot</h3><p>one traditional way to represent word is through one-hot vectors.<br>If you have 100k vocabulary, your 1-hot vector size will be dim 1x100k, with the specific represented word setting to 1 and all the other elements set to 0s. This is an example:<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/28049823.jpg" alt=""></p>
<p>What will be the drawbacks of one-hot vectors?</p>
<ul>
<li>computational and memory expensive when vocabulary size is really large</li>
<li>word vectors are independent with each other, $v_1={China}, v_2={America}$, they are both country, so they are <em>correlated</em>, but the one-hot vectors says that $v_1 \cdot v_2 = 0$, they are perpendicular, thus independent and uncorrelated, so one-hot vector is easy to get but we lose some information.</li>
</ul>
<h3 id="feature-representation-word2vec"><a href="#feature-representation-word2vec" class="headerlink" title="feature representation - word2vec"></a>feature representation - word2vec</h3><p>So to solve the problems, we want a vector representation like the following<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/986348.jpg" alt=""><br>Each word vector is 128 dim, in continous vector space, a word can be represented by it, and the similarity of word can be encoded into it.<br>And we can do operations on the learned representations like this:</p>
<blockquote>
<p>$v_{queen} - v_{king} == v_{woman} - v_{man}$</p>
</blockquote>
<p>We often use a embedding matrix, it multiply a one-hot vector, can returns a current embedding<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/1375136.jpg" alt=""></p>
<p>How can we do this?</p>
<h3 id="How-we-use-word-embeddings"><a href="#How-we-use-word-embeddings" class="headerlink" title="How we use word embeddings?"></a>How we use word embeddings?</h3><p>Using word embedding is like transfer learning, we train it on big corpus unsupervisedly, and apply it onto a particular problem (usually smaller sized), as the prior knowledge.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/2581072.jpg" alt=""></p>
<blockquote>
<p>Intuition:<br>We predict target word by given context words, and the by-product of it are word embedding $P(t|c)$</p>
</blockquote>
<h2 id="How-to-do-learn-word-embedding"><a href="#How-to-do-learn-word-embedding" class="headerlink" title="How to do learn word embedding"></a>How to do learn word embedding</h2><h3 id="V1-use-previous-few-words-by-product-of-learning-language-model"><a href="#V1-use-previous-few-words-by-product-of-learning-language-model" class="headerlink" title="V1 use previous few words (by-product of learning language model)"></a>V1 use previous few words (by-product of learning language model)</h3><p>given sentences, retreive the corresponding word embeddings from embedding matrix $E$, and feed the feched word vectors into a fully conneted hidden layer, and then feed into a softmax with parameter matrix, (first map the vector dim into vocabulary size, than take softmax, equivalent to a fully connected layer and a softmax operation), after softmax, we can get the word by argmax, and the embedding matrix weights learned can be used as word embeddings.<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/72304825.jpg" alt=""></p>
<h3 id="other-context-center-pair-to-learn-word-embedding"><a href="#other-context-center-pair-to-learn-word-embedding" class="headerlink" title="other (context / center) pair to learn word embedding"></a>other (context / center) pair to learn word embedding</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/42202177.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-22/61894626.jpg" alt=""></p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul>
<li>one-hot vectors</li>
<li>$E$ embedding matrix</li>
<li>retrived embedding vectors $e$</li>
<li>softmax layer (fully connected hidden layer + softmax operation), map to size of $|V|$</li>
<li>NLL loss</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/20/Image-Data-Augmentation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/20/Image-Data-Augmentation/" itemprop="url">Image Data Augmentation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-20T18:12:34+08:00">
                2018-08-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Code/" itemprop="url" rel="index">
                    <span itemprop="name">Code</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/20/Image-Data-Augmentation/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/20/Image-Data-Augmentation/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Image-Data-Augmentation-in-Python"><a href="#Image-Data-Augmentation-in-Python" class="headerlink" title="Image Data Augmentation in Python"></a>Image Data Augmentation in Python</h3><p>因为深度学习需要较多的数据进行训练,所以在图像数据集比较少的时候,我们可以通过对已知图像进行变换来增加数据集的大小.这是深度学习中很重要的一个trick. 笔者最近也正好需要进行一次数据增广,用简单的例子给大家说明一下如何使用</p>
<h2 id="Augmentor"><a href="#Augmentor" class="headerlink" title="Augmentor"></a>Augmentor</h2><p>推荐使用python library: <a href="https://github.com/mdbloice/Augmentor" target="_blank" rel="external">Augmentor</a></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install Augmentor</div></pre></td></tr></table></figure>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># import lib</span></div><div class="line"><span class="keyword">import</span> Augmentor</div><div class="line"></div><div class="line"><span class="comment"># set origin data root dir, contains the data</span></div><div class="line"><span class="comment"># images that you want to augment</span></div><div class="line">p = Augmentor.Pipeline(<span class="string">"../falldown_augment/"</span>)</div><div class="line"></div><div class="line"><span class="comment"># set the transforms</span></div><div class="line">p.random_distortion(probability=<span class="number">1</span>, grid_width=<span class="number">4</span>, grid_height=<span class="number">4</span>, magnitude=<span class="number">8</span>)</div><div class="line">p.random_color(<span class="number">0.6</span>, <span class="number">0.1</span>, <span class="number">0.7</span>)</div><div class="line">p.rotate(probability=<span class="number">0.7</span>, max_left_rotation=<span class="number">10</span>, max_right_rotation=<span class="number">10</span>)</div><div class="line">p.zoom(probability=<span class="number">0.5</span>, min_factor=<span class="number">1.1</span>, max_factor=<span class="number">1.5</span>)</div><div class="line">p.flip_left_right(probability=<span class="number">0.5</span>)</div><div class="line"></div><div class="line"><span class="comment"># sample due to the config</span></div><div class="line">p.sample(<span class="number">500</span>)</div></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>用以上代码可以将77张图片增广成500张,根据我们设置的变换以及相应的概率.<br>这个库代码简洁,功能也比较全,支持多线程,可以和keras/pytorch进行交互,(融入成generator)<br>总的来说功能够用也好用,详细的大家看他的<a href="http://augmentor.readthedocs.io/" target="_blank" rel="external">doc</a>或者github主页吧.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/10/Unsupervised-Neural-Dependency-Parsing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/10/Unsupervised-Neural-Dependency-Parsing/" itemprop="url">Neural Unsupervided Dependency Parsing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-10T23:31:21+08:00">
                2018-08-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/10/Unsupervised-Neural-Dependency-Parsing/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/10/Unsupervised-Neural-Dependency-Parsing/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Unsupervised-Neural-Dependency-Parsing-Y-Jiang-et-al"><a href="#Unsupervised-Neural-Dependency-Parsing-Y-Jiang-et-al" class="headerlink" title="Unsupervised Neural Dependency Parsing[Y. Jiang et. al]"></a>Unsupervised Neural Dependency Parsing[Y. Jiang et. al]</h2><p>今天介绍的论文是 Unsupervised Neural Dependency Parsing <a href="https://aclweb.org/anthology/D16-1073" target="_blank" rel="external">Y. Jiang et. al</a>的中文提要</p>
<h3 id="什么是无Unsupervised-Dependency-Parsing？这篇文章的主要工作"><a href="#什么是无Unsupervised-Dependency-Parsing？这篇文章的主要工作" class="headerlink" title="什么是无Unsupervised Dependency Parsing？这篇文章的主要工作"></a>什么是无Unsupervised Dependency Parsing？这篇文章的主要工作</h3><p><a href="/2018/08/05/DMV/">Unsupervised Dependency Parsing</a><br>再简要总结一下Unsupervided Dependency Parsing的特点</p>
<ul>
<li>输入是带有POS的句子，但是不带有具体的parse tree</li>
<li>通常使用基于<a href="/2018/08/04/EM/">EM</a>算法的方法来估计每个”grammar rules”的参数(概率)</li>
<li>传统模型通常使用特征(features)和归纳偏置(inductive bias)来将一些有用的先验信息incorporate进模型中，比如grammar rules的correlation使得模型达到更好的效果。通常是一些手动design的featue和特别的prior distribution。</li>
<li>这篇文章创新地使用了神经网络来自动地在预测 grammars rules 的参数，基于对POS tags的离散表示，而这些离散表示是自动地从corpus中间学习得到的。这些离散表示可以很好地将POS之间的correlation 进去。这里有一些类似word2vec模型？</li>
<li>最终这篇文章的模型达到了非常好的效果</li>
</ul>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li>之前大多做Unsupervised Dependency Parsing（以下简称UDP）的工作多是基于<a href="2018/08/05/DMV/">DMV</a>,在DMV的基础上添加多种inductive bias,或者handcrafted features来将POS tags 之间的相关性correlation encode进模型之中。</li>
<li>但是本文做事使用了神经网络来做两件事情<ul>
<li>学习得到每个POS tags的离散表示，生成POS embedding</li>
<li>根据POS embedding 预测 每个 grammar rules 的概率（更新概率）</li>
</ul>
</li>
<li>因此我们学习到的embeddings可以自动地encode我们词之间的correlation，而不需要hand crafted feature.</li>
<li>同时作者提到，POS的相关性导致的需要smoothing的问题已经被神经网络自动地解决了<ul>
<li>Smoothing是什么？smoothing通常被用来解决监督学习中某些数据非常稀疏的问题。smooth在无监督学习中也有用处，<a href="http://www.aclweb.org/anthology/N09-1012" target="_blank" rel="external">Headden et.al 2009 NAACL</a>,利用分部之间的线性插值来缓解</li>
</ul>
</li>
</ul>
<h2 id="DMV"><a href="#DMV" class="headerlink" title="DMV"></a>DMV</h2><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>DMV是一个UDP的生成模型，具体细节可以参考这一篇文章<a href="/2018/08/05/DMV/">DMV</a><br>先说一些notations</p>
<ul>
<li>$dec$ STOP or COND 是否继续生成parse tree</li>
<li>$h$ head POS</li>
<li>$dir$ LEFT or RIGHT</li>
<li>$c$ child POS</li>
<li>$val$ boolean | whether current head POS already has a child in current direction<br>简单来说呢就是，这个模型有三种类型的rules:</li>
<li><strong>CHILD</strong> $P_{CHILD}(c|h,dir,val)$ 已知head和val,往给定方向生成儿子的概率</li>
<li><strong>DECISION</strong> $P_{DECISION}(dec|h,dir,val)$ …是否继续生成的概率</li>
<li><strong>ROOT</strong> $P_{ROOT}(c|root)$ …root生成某个儿子的概率</li>
</ul>
<h3 id="Drawbacks"><a href="#Drawbacks" class="headerlink" title="Drawbacks"></a>Drawbacks</h3><p>DMV模型还有不少缺点</p>
<ul>
<li>模型过度简化 <a href="http://www.aclweb.org/anthology/N09-1012" target="_blank" rel="external">Headden et.al 2009 NAACL</a>使用 <a href="https://blog.csdn.net/u013515273/article/details/78273342" target="_blank" rel="external">lexicalization</a> 来解决</li>
<li><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-8/47430962.jpg" alt=""></li>
<li><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-8/76592145.jpg" alt=""></li>
</ul>
<h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><ul>
<li>EM 算法来 learning DMV model (Inside-Outside)<ul>
<li>E-step: Learn expected counts - 求出后验概率 $p(y | \theta, x^{(i)})$</li>
<li>M-step: Normalize parameters - <img src="http://oj4pv4f25.bkt.clouddn.com/18-8-9/60137963.jpg" alt=""></li>
</ul>
</li>
<li>但是EM算法是有缺陷的<ul>
<li>比如 non-convex 容易收敛到local optimal</li>
<li>initialization matters</li>
</ul>
</li>
</ul>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>接下来就是最关键的 Model 了。<br><strong>我们希望用神经网络来替代M-step中的新参数估计阶段</strong></p>
<h3 id="E-step"><a href="#E-step" class="headerlink" title="E-step"></a>E-step</h3><p>E-step保持不变，和原本的算法类似，我们可以通过计算inside-outside score来获得我们的expected counts这我们计算出了三种类型的rules在每个句子中$x \in X$中的出现的expected counts</p>
<ul>
<li>$e_c(x_i)$ CHILD rule 在该句子中的expected counts, $p_c$ CHILD rule在整个corpus中出现的分布parameter(概率)</li>
<li>$e_d(x_i)$ DECISION rule 在该句子中的expected counts, $p_d$ CHILD rule在整个corpus中出现的分布parameter(概率)</li>
<li>$e_r(x_i)$ ROOT rule 在该句子中的expected counts, $p_r$ CHILD rule在整个corpus中出现的分布parameter(概率)<br>我们通过<a href="/2018/08/05/Inside-Outside-Algorithm/">Insode-outside</a>计算出了这些量，也就是所谓的expected count</li>
</ul>
<h3 id="原本的M-step"><a href="#原本的M-step" class="headerlink" title="原本的M-step"></a>原本的M-step</h3><p>原本的M-step其实是优化一个expected log likelihood，找的最优的参数,只不过我们面临的通常是一个 multinomial 的分布，所以我们可以通过数学方法证明，参数的更新其实就是一个normalize。如图：如果对证明方法感兴趣的话，可以去看这个M. Collins大佬 <a href="http://www.cs.columbia.edu/~mcollins/em.pdf" target="_blank" rel="external">note</a> 的最后一部分<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-9/60137963.jpg" alt=""><br>我们这篇文章的$ELL(\theta)$便是</p>
<script type="math/tex; mode=display">ELL(\theta)=\sum_{\alpha=1}^N(\sum_c e_c(x_i)\log{p_c}+\sum_d e_d(x_i)\log{p_d}+\sum_r e_r(x_i)\log{p_r})</script><p>这里这个式子的最优解按照往常normalize一下即可<br>但是这里我们不直接normalize,而是通过训练一个神经网络来学会通过POS,direction,valence等信息来predict出这个rule在这个corpus中的概率!所以这个神经网络的objective function可以就看做事这个$ELL(\theta)$，不过这里面$\theta$的含义就加上了神经网络的权重参数了。而之前的各种correlation我们通过了POS的embedding融合进进了模型中间，所以作者认为这个神经网络可以被看做是在M-step的优化过程中加上了一个regulation项，这个regulation项encode进了POS之间的correlation.</p>
<h2 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h2><p>我们用一个神经网络来建模每个rule的概率的predict,拿child rule来举例</p>
<ul>
<li>$P_{CHILD}(c|h,dir,val)$</li>
<li>神经网络的输入就是 $h,dir,val$</li>
<li>输出就是$P_{CHILD}(c|h,dir,val)$<h3 id="Detail"><a href="#Detail" class="headerlink" title="Detail"></a>Detail</h3>需要能够将离散的POS tag输入神经网络进行计算，我们需要把它转化为离散表示，和word2vec的目的是一致的，我们也许可以叫他POS2Vec。</li>
<li>我们记所有的POS tag的集合为$T$</li>
<li>所有的head tag as a d-dim vector $v_h \in R^d$</li>
<li>val as a $d’$-dim vector</li>
<li>input layer: $[(d+d’) \times 1]$ vector。<ul>
<li>我们把 val 和某一个要预测的 head pos tag concat起来作为输入 $[v_h;v_{val}]$</li>
</ul>
</li>
<li>hidden layer: <ul>
<li>我们把输入的vector通过一个矩阵$W_{dir}, [d_h \times (d+d’)]$的transform。<ul>
<li>注意这里的$W_{dir} \in W_{left}, W_{right}$这两个是分开独立的，对不同的方向我们有一个不同的神经网络。</li>
</ul>
</li>
<li>然后通过一个Relu的non-linear</li>
<li>hidden layer output: $f = ReLU(W_{dir}[v_h;v_{val}])$ <ul>
<li>$[d_h \times 1]$ vector</li>
</ul>
</li>
</ul>
</li>
<li>softmax layer:<ul>
<li>经过了隐层的变换之后我们需要把这个vector map到概率！</li>
<li>我们的Embedding matrix隆重登场了！$W^{|T| \times d_h}$</li>
<li>然后我们过一个softmax就有了我们最终的概率结果！</li>
<li>$[p_{c_1}, \cdots, p_{c_{|T|}}] = Softmax(W^Tf)$</li>
<li>output: $[|T| \times 1]$ vector!<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-9/53439112.jpg" alt=""></li>
</ul>
</li>
</ul>
<h3 id="emmmm"><a href="#emmmm" class="headerlink" title="emmmm"></a>emmmm</h3><p>一些别的注意点</p>
<ul>
<li>Decision rule 的学习和 Child rule 使用一个结构，只不过output改变一下</li>
<li>$W_{dir}$和在Decision/Child中是共享参数的</li>
<li>embedding size $d_h$ 是个很重要的hyper parameter</li>
</ul>
<h3 id="Learning-Procedure"><a href="#Learning-Procedure" class="headerlink" title="Learning Procedure"></a>Learning Procedure</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-10/51173510.jpg" alt=""></p>
<h2 id="results"><a href="#results" class="headerlink" title="results"></a>results</h2><p>效果自然是不错滴<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-10/89507397.jpg" alt=""><br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-10/98180433.jpg" alt=""></p>
<h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><p>这篇文章的一个优势是用神经网络来替代DMV中的M-step。<br>在效果很不错的基础上<br>用一个很简单的网络结构解决了通常需要很复杂模型才能解决的问题<br>这一点是非常有意义的。<br>同时作者比较细心得讨论并且可视化了神经网络学习到的一些结果，<br>很好地证明了这篇工作Smooth correlate POS的Motivation得到了解决。（关于这部分不难，直接看原文吧）</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/10/Correlate-and-dependent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/10/Correlate-and-dependent/" itemprop="url">Correlation and Dependence</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-10T15:31:21+08:00">
                2018-08-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/10/Correlate-and-dependent/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/10/Correlate-and-dependent/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Correlation"><a href="#Correlation" class="headerlink" title="Correlation"></a>Correlation</h3><ul>
<li>两个随机变量$X,Y$相关通常指的是<strong>线性相关</strong>，直观上理解就是，一个变大另一个也相应变大（或反向）</li>
<li>$Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}} = 0$表示线性无关</li>
<li>线性无关 $Corr(X,Y)=0$不推出独立，因为可能会有非线性的关系</li>
</ul>
<h3 id="Dependence"><a href="#Dependence" class="headerlink" title="Dependence"></a>Dependence</h3><ul>
<li>两个随机变量$X,Y$如果独立</li>
<li>$P(X)P(Y)=P(X,Y), \ \ P(X|Y)=P(X)$</li>
<li>独立推出无关</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/08/Inductive-Bias/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/08/Inductive-Bias/" itemprop="url">Inductive Bias</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-08T23:51:21+08:00">
                2018-08-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning-Theory/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning Theory</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning-Theory/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/08/Inductive-Bias/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/08/Inductive-Bias/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h3><p>Inductive Bias 归纳偏置是机器学习中的一个重要常见的概念<br>摘自Wikipedia</p>
<blockquote>
<p>当学习器去预测其未遇到过的输入的结果时，会做一些假设（Mitchell, 1980）。而学习算法中的归纳偏置则是这些假设的集合。</p>
</blockquote>
<h3 id="机器学习中常见的归纳偏置列表："><a href="#机器学习中常见的归纳偏置列表：" class="headerlink" title="机器学习中常见的归纳偏置列表："></a>机器学习中常见的归纳偏置列表：</h3><ul>
<li>最大条件独立性（conditional independence）：如果假说能转成贝叶斯模型架构，则试着使用最大化条件独立性。这是用于朴素贝叶斯分类器（Naive Bayes classifier）的偏置。</li>
<li>最小交叉验证误差：当试图在假说中做选择时，挑选那个具有最低交叉验证误差的假说，虽然交叉验证看起来可能无关偏置，但天下没有免费的午餐理论显示交叉验证已是偏置的。</li>
<li>最大边界：当要在两个类别间画一道分界线时，试图去最大化边界的宽度。这是用于支持向量机的偏置。这个假设是不同的类别是由宽界线来区分。</li>
<li>最小描述长度（Minimum description length）：当构成一个假设时，试图去最小化其假设的描述长度。假设越简单，越可能为真的。见奥卡姆剃刀。</li>
<li>最少特征数（Minimum features）：除非有充分的证据显示一个特征是有效用的，否则它应当被删除。这是特征选择（feature selection）算法背后所使用的假设。</li>
<li>最近邻居：假设在特征空间（feature space）中一小区域内大部分的样本是同属一类。给一个未知类别的样本，猜测它与它最紧接的大部分邻居是同属一类。这是用于最近邻居法的偏置。这个假设是相近的样本应倾向同属于一类别。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/08/Prior-Distribution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/08/Prior-Distribution/" itemprop="url">Prior & Posterial</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-08T23:31:21+08:00">
                2018-08-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Probability-Theory/" itemprop="url" rel="index">
                    <span itemprop="name">Probability Theory</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Probability-Theory/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/08/Prior-Distribution/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/08/Prior-Distribution/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h3><ul>
<li>$\theta$ parameters of distribution</li>
<li>$X$ data R.V</li>
<li>$P(\theta)$ Prior, Prior distribution when not seeing data</li>
<li>$P(\theta | X)$ posterior distribution when seen data</li>
<li>$P(X | \theta)$ Likelihood</li>
<li>$P(X)$ data distribution</li>
</ul>
<h3 id="Prior-amp-Posterior"><a href="#Prior-amp-Posterior" class="headerlink" title="Prior &amp; Posterior"></a>Prior &amp; Posterior</h3><script type="math/tex; mode=display">P(\theta | X) = \frac{P(\theta)P(X|\theta)}{P(X)}</script><script type="math/tex; mode=display">P(X) = \int_\theta P(X|\theta)P(\theta)d \theta</script>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/07/GenDis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/07/GenDis/" itemprop="url">Generative Model and Discriminative Model</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-07T12:31:21+08:00">
                2018-08-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Note/" itemprop="url" rel="index">
                    <span itemprop="name">Note</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Note/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/07/GenDis/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/07/GenDis/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>余今日偶得屠师爷天书《Unsupervised Dependency Parsing本录》，有两词无法参透。谷歌搜索罢，略知一二，故在今日饭前稍作记录。</p>
<h3 id="壹"><a href="#壹" class="headerlink" title="壹"></a>壹</h3><p>生成方法、判别方法乃是机器学习只根本方法，重中之重，余之前所学多为判别方法，今开始对生成方法多加关注，与各位同道分享一二。</p>
<p>Notation: $X$ data, $Y$ label</p>
<h3 id="贰-Generative-Approach"><a href="#贰-Generative-Approach" class="headerlink" title="贰 Generative Approach"></a>贰 Generative Approach</h3><ul>
<li>learn $P(X,Y)$</li>
<li>marginailze to $P(X|Y)$ to classify</li>
<li>reflect the <strong>similarity</strong> of data</li>
<li>can be used to learn latent variable</li>
<li>Naive Bayes、EM based GMM、DMV</li>
</ul>
<h3 id="叁-Discriminative-Approach"><a href="#叁-Discriminative-Approach" class="headerlink" title="叁 Discriminative Approach"></a>叁 Discriminative Approach</h3><ul>
<li>learn $P(X|Y)$ drirect from <strong>features</strong></li>
<li>reflect the <strong>distinction</strong> of data margin</li>
<li>can not be used to learn latent variable</li>
<li>KNN, regression, SVM, CRF</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/06/DMV/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/06/DMV/" itemprop="url">Dependency Model With Valence</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-06T22:31:21+08:00">
                2018-08-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Note/" itemprop="url" rel="index">
                    <span itemprop="name">Note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/06/DMV/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/06/DMV/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇Notes主要介绍一下什么事Dependency Model With Valence<br>Some Materials in this note are taken from Prof. Kewei Tu’s Lecture slice 17 in Shanghaitech SIST CS181 course</p>
<h2 id="Dependency-Grammars"><a href="#Dependency-Grammars" class="headerlink" title="Dependency Grammars"></a>Dependency Grammars</h2><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/89653079.jpg" alt=""><br>Dependency Grammar与Context Free Grammar有着很大的联系，DG通常是由一个POS tag指向了另外一个，$A \to B$, $head \to dependent$. 被指向的dependent通常是head的“modifier”。<br>$A \to B =&gt; B\ modify \ A =&gt; A\ depend\ on\ B$</p>
<h2 id="Unsupervised-Dependency-Parser"><a href="#Unsupervised-Dependency-Parser" class="headerlink" title="Unsupervised Dependency Parser"></a>Unsupervised Dependency Parser</h2><p>无监督学习 DG 成为了一个难以解决的问题。无监督学习DG的设定通常是给定句子的corpus,并且句子是带有Part Of Speeach Tags的。<br>说到无监督，那么我们不得不提到<a href="/2018/08/04/EM/">EM</a>算法了，同时由于DG和CFG的相似性，建议在看DMV这一篇之前先去看一下<a href="/2018/08/05/Inside-Outside-Algorithm/">inside-outside Algorithm</a><br>DMV(Dependency Model With Valence)是 <a href="https://people.eecs.berkeley.edu/~klein/papers/acl04-factored_induction.ps" target="_blank" rel="external">Klein and Manning, 2004</a>的一篇杰出工作. 之后的大多无监督依存句法分析的工作通常都是DMV算法的改进。</p>
<h2 id="DMV"><a href="#DMV" class="headerlink" title="DMV"></a>DMV</h2><h3 id="Genarative-Point-Of-View"><a href="#Genarative-Point-Of-View" class="headerlink" title="Genarative Point Of View"></a>Genarative Point Of View</h3><p>DMV是一种生成模型。假设DMV模型的参数已经训练好，那么我们如何生成一刻DG parse tree呢？<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/63286428.jpg" alt=""><br>我们先往左边生成,对node(head) sample可能的dependent,如果是一个valid的dependet,我们就递归得进入这个node,如果我们sample到了一个stop sign，我们就结束这一条路线，返回上一个node,这个时候我们该尝试往右边sample，同样，如果sample到了dependent，那么就继续进入下一级递归，如果sample到了stop sign,那么再往上一级进行递归<br>[matiarial from (<a href="https://ufal.mff.cuni.cz/~marecek/papers/2012_pondelni_seminar.ppt" target="_blank" rel="external">https://ufal.mff.cuni.cz/~marecek/papers/2012_pondelni_seminar.ppt</a>)]<br>我们再看一个更具体一点的例子：<br><img src="http://03.imgmini.eastday.com/mobile/20170919/20170919095515_4baf0af3c783c320329e672290980538_3.jpeg" alt=""><br>[matiarial from (<a href="http://mini.eastday.com/a/170919095515198-2.html" target="_blank" rel="external">http://mini.eastday.com/a/170919095515198-2.html</a>)]</p>
<h3 id="Probability-Model-of-DMV"><a href="#Probability-Model-of-DMV" class="headerlink" title="Probability Model of DMV"></a>Probability Model of DMV</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-6/864191.jpg" alt=""><br>$p(y^{(0)}| S, \theta )$表示从$ROOT$开始生成root at position 0的parse tree y的概率</p>
<h3 id="Running-Example"><a href="#Running-Example" class="headerlink" title="Running Example"></a>Running Example</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-6/58920928.jpg" alt=""><br>解析：一开始ROOT只能往右边走，通过sample attach了VB,乘上sample的概率然后再乘上$y$在position 2上，给定了$VB, \theta$继续生成的概率。之后继续递归。…</p>
<h3 id="根据上面的式子，我们知道！"><a href="#根据上面的式子，我们知道！" class="headerlink" title="根据上面的式子，我们知道！"></a>根据上面的式子，我们知道！</h3><p>只要有了所有的parameters，也就是 $\theta_c, \theta_s$或者说每一个rule（比如：VB—&gt;NN)的的概率，我们就相当于得到了DMV这个生成模型的分布，那么我们就可以根据这个分布进行生成了(parsing)。但是我们一开始是没有parsing过的data的，我们只有POS的data,而没有整颗parse tree，所以又到了我们EM算法出场的时候了。是不是和CFG的Inside-outside很像？</p>
<h3 id="回顾一下inside-outside-这里可以原封不动地套用"><a href="#回顾一下inside-outside-这里可以原封不动地套用" class="headerlink" title="回顾一下inside-outside, 这里可以原封不动地套用"></a>回顾一下inside-outside, 这里可以原封不动地套用</h3><ul>
<li>我们没有parse tree 的标注(annotation)，所以我么希望通过EM算法进行无监督学习</li>
<li>EM Algorithm<ul>
<li>Initialize parameters (每个grammar rules $r$在corpus$X$中出现的概率)</li>
<li>E-step: 计算Expected Count (grammar rules $r$在corpus$X$中出现的次数的期望)<ul>
<li>计算Inside Score (DP)</li>
<li>计算Outside Score (DP)</li>
<li>得到用Inside/Outside Score表示Expected Count</li>
</ul>
</li>
<li>M-step: Update Probabilities Use expected counts</li>
<li>跳到E-step, 迭代</li>
</ul>
</li>
<li>最终我们得到了每个grammar </li>
</ul>
<h3 id="Dependency-Grammar的Inside-Outside-Score怎么计算呢？"><a href="#Dependency-Grammar的Inside-Outside-Score怎么计算呢？" class="headerlink" title="Dependency Grammar的Inside-Outside Score怎么计算呢？"></a>Dependency Grammar的Inside-Outside Score怎么计算呢？</h3><p>注意了！DG和CFG是有着非常非常紧密地联系的！他们可以互相转化！<br>DG: $w_i \to w_j$ =&gt; CFG: $w_i \to w_i w_j$,就像本篇文章第一张图那样。<br>就拿上面的那一张图来举例子吧！<br>$ P(VBZ&lt;-VBZ| X, \theta) [DG]= P(VBZ \to VBG, VBZ| X, \theta)[CFG]$ 所有的$p,d,q$加起来。<br>回忆一下这个图。<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/49723064.jpg" alt=""></p>
<h3 id="根据DG和CFG之间的转化关系我们可以用同样的Inside-Outside算法来计算出expected-count了"><a href="#根据DG和CFG之间的转化关系我们可以用同样的Inside-Outside算法来计算出expected-count了" class="headerlink" title="根据DG和CFG之间的转化关系我们可以用同样的Inside-Outside算法来计算出expected count了"></a>根据DG和CFG之间的转化关系我们可以用同样的Inside-Outside算法来计算出expected count了</h3><p>之后我们再更新EM algorithm的参数就行了！<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-6/85653980.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/05/Inside-Outside-Algorithm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/05/Inside-Outside-Algorithm/" itemprop="url">Inside-Outside-Algorithm</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-05T13:31:21+08:00">
                2018-08-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Note/" itemprop="url" rel="index">
                    <span itemprop="name">Note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/05/Inside-Outside-Algorithm/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/05/Inside-Outside-Algorithm/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这是M.Collins的Notes中文笔记,内容跟着Notes走,但是笔者会融合一些笔者平时积累的其他资料,Notes原版在M.Collins大神的主页.<br><a href="http://www.cs.columbia.edu/~mcollins/" target="_blank" rel="external">http://www.cs.columbia.edu/~mcollins/</a></p>
<h3 id="Inside-Outside-Algorithm"><a href="#Inside-Outside-Algorithm" class="headerlink" title="Inside Outside Algorithm"></a>Inside Outside Algorithm</h3><p>Inside Outside 算法是 PCFG grammar induction (Unsupervised Learning)中的重要算法。Collins大佬的notes中的<a href="/2018/08/04/EM/">EM</a>算法是本notes的基础。<br>Inside Outside 算法被誉为是NLP中最难的算法，如果彻底搞懂NLP就没什么好怕的了！</p>
<h3 id="什么是PCFG"><a href="#什么是PCFG" class="headerlink" title="什么是PCFG"></a>什么是PCFG</h3><p>Probability Context-free Grammars</p>
<ul>
<li>$N$ is finite sets of non-terminals</li>
<li>$\sum$ is finite sets of ternminals</li>
<li>$R$ is finite set of rules, the grammar is in Chomsky normal form, $A \to BC , A \to x$</li>
<li>$S \in N$ is a distinguished start symbol</li>
</ul>
<p>对于每一条rule，我们定义rule spanning，如下。一个parse tree可以被定义为多个rule production的势函数的乘积。</p>
<ul>
<li>$(A \to B,C,i,k,j)$表示non-ternminal $A$ span 出了 $w_i \cdots w_j$,  $B$ span 出了 $w_i \cdots w_k$, $C$ span 出了 $w_k \cdots w_j$</li>
<li>$(A \to i)$ 表示 non-terminal $A$ span 出了最底层的word i</li>
<li>举一个例子<img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/92426943.jpg" alt=""></li>
<li>上面两种rules 都有其对应的 potential function $\phi(r)$, 所以一整棵树的势函数就是所有rule的势函数的乘积<img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/74502801.jpg" alt=""></li>
<li>通常我们对于每一种rule我们可以直接定义它的势函数为，看见这一条rule的概率，或者说这一条rule在这个corpus中出现的概率。所以我们将$\phi(A \to BC,i,k,j)=q(A\to BC)$,这个概率就是我们针对每个grammar rule assign的参数</li>
</ul>
<h3 id="Inside-Outside-算法用作grammar-learning的Input-Output是什么"><a href="#Inside-Outside-算法用作grammar-learning的Input-Output是什么" class="headerlink" title="Inside-Outside 算法用作grammar learning的Input Output是什么"></a>Inside-Outside 算法用作grammar learning的Input Output是什么</h3><p>Input</p>
<ul>
<li>很多个句子的集合 $X$, 每一个句子$x \in X$, 我们有$x_1 \cdots x_n$, $x_i$是词</li>
<li>一个CFG $(N,\sum, R,S)$ in Chomsky Normal Form，注意!这里的每个$r \in R$是没有概率的，也就是这里不是PCFG,如果我们知道每个gramma rules的概率的话，我们就是一个完整的PCFG的设定了！我们就可以用 <strong>CYK</strong> 算法，在$O(n^3G)$动态规划得把整棵树parse出来。</li>
</ul>
<p>Output</p>
<ul>
<li>每个grammar rules的概率，也就是我们对每个语法的$\phi$</li>
</ul>
<h2 id="Inside-Outside-算法的基本设定就讲完了，下面我们就讲一下细节"><a href="#Inside-Outside-算法的基本设定就讲完了，下面我们就讲一下细节" class="headerlink" title="Inside Outside 算法的基本设定就讲完了，下面我们就讲一下细节"></a>Inside Outside 算法的基本设定就讲完了，下面我们就讲一下细节</h2><p>接下来的资料是根据 prof. Kewei Tu 在Shanghaitech CS181课程中的slice #17进行概括<br>这个更加图文并茂一些</p>
<h3 id="EM算法的设定"><a href="#EM算法的设定" class="headerlink" title="EM算法的设定"></a>EM算法的设定</h3><p>看了前面的input,output是不是很熟悉呢？因为我们拿到是一个POS sequence而不是整个parse-tree,也就是说我们没有办法通过统计的方法来通过corpus得到每个grammar rules的参数（也就是概率）了，我们也因此没有办法用 CYK 算法进行parsing了。所以这个时候就该用EM算法了。</p>
<h3 id="sketch-of-Inside-outside-人话版本"><a href="#sketch-of-Inside-outside-人话版本" class="headerlink" title="sketch of Inside-outside 人话版本"></a>sketch of Inside-outside 人话版本</h3><ul>
<li>随机初始化 grammar rules 的概率</li>
<li>利用这个概率计算出每个 grammar rules 的 <strong>expected counts</strong></li>
<li>利用我们的expected counts 我们就能重新更新我们 grammar rules 的概率了！</li>
<li>进入step2, 反复迭代。</li>
</ul>
<h3 id="sketch-of-Inside-outside-数学版本"><a href="#sketch-of-Inside-outside-数学版本" class="headerlink" title="sketch of Inside-outside 数学版本"></a>sketch of Inside-outside 数学版本</h3><ul>
<li>随机初始化 grammar rules 的概率 $\theta^0$</li>
<li>E-step: 利用这个概率计算出每个 grammar rules 的 <strong>expected counts</strong> $C(N^j\to N^r N^s \ used | X, \theta^t)$, $X$是句子，没有parse的，$N^j$表示第j个non-terminal</li>
<li>M-step: 利用我们的expected counts 我们就能重新更新我们 grammar rules 的概率了！$\theta^{t+1}_{jrs}=P(N^j \to N^r N^s)=\frac{C(N^j\to N^r N^s \ used | X, \theta^t)}{C(N^j \ used | X, \theta^t)}$</li>
<li>进入step2, 反复迭代。</li>
</ul>
<h3 id="最关键的问题-如何计算expected-count"><a href="#最关键的问题-如何计算expected-count" class="headerlink" title="最关键的问题: 如何计算expected count?"></a>最关键的问题: 如何计算expected count?</h3><script type="math/tex; mode=display">C(N^j\to N^r N^s \ used | X, \theta^t) = \sum_{(w_1 \cdots w_m) \in X} E_{p(t|w_1 \cdots w_m)} C(N^j\to N^r N^s \ used\ in\ t)</script><p>这一句话的意思是：对于一句话的每一个parse tree $t$, 我们都能够数出对于这个parse tree某个rule $N^j\to N^r N^s$的使用的次数，于是如果我们枚举出了所有的parse trees以及其对应的概率,那么我们就可以求出这个rule出现在这句话里面的次数的期望，也就是我们这句话的expected counts!最后对于每一句话$x \in X$都加起来，就得到了我们整个corpus的expected counts.但是一个句子的parse tree可能有很多，所以我们是无法枚举的。</p>
<h3 id="所以我们通过计算inside-outside-probability来计算出expected-counts"><a href="#所以我们通过计算inside-outside-probability来计算出expected-counts" class="headerlink" title="所以我们通过计算inside-outside probability来计算出expected counts"></a><strong>所以我们通过计算inside-outside probability来计算出expected counts</strong></h3><p>expected count 可以这样表示，给定一个rule: $(N^j \to N^r \ N^s ,p,d,q)$(见最开始的定义)，对于每一个句子，我们可以通过枚举所有的$p,d,q$,加起来得到这个句子的这个grammar rule的expected counts,并且对每个句子加起来，最终得到这个corpus的expected counts<br>这里的$P$写清楚一点的话是$P(N^j \to N^r \ N^s ,p,d,q|w_{1,m},\theta^t)$<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/49723064.jpg" alt=""></p>
<h2 id="那么我们如何计算-P-N-j-to-N-r-N-s-p-d-q-w-1-m-theta-t"><a href="#那么我们如何计算-P-N-j-to-N-r-N-s-p-d-q-w-1-m-theta-t" class="headerlink" title="那么我们如何计算 $P(N^j \to N^r \ N^s ,p,d,q|w_{1,m},\theta^t)$"></a>那么我们如何计算 $P(N^j \to N^r \ N^s ,p,d,q|w_{1,m},\theta^t)$</h2><p>我们通过计算inside probability 和 outside probability 来计算。<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/71696567.jpg" alt=""></p>
<h3 id="Inside-Probability"><a href="#Inside-Probability" class="headerlink" title="Inside Probability"></a>Inside Probability</h3><p>Inside score $\beta_j(p,q)$是non-terminal $N^j$ 生成terminal $w_p \cdots w_q$的概率,也就是$P(w_{pq}|N^j_{pq},G)$, 我们可以通过动态规划(Dynamic Programming)来计算。<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/44133812.jpg" alt=""><br>我们可以把原问题划分为这样的子问题。$N^j$的spanning可以分解为$N^j \to N^r, N^s$，并且$N^r$ spanning成$w_p \cdots w_d$, $N^s$ spanning成$w_{d+1} \cdots w_q$, 我们只要枚举不同的split $d$ 并且加起来就行了。<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/23219939.jpg" alt=""><br>Base case<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/79155619.jpg" alt=""><br><strong>这个过程就等同于CYK parsing,只不过把max换成了sum</strong></p>
<h3 id="Outside-Probability"><a href="#Outside-Probability" class="headerlink" title="Outside Probability"></a>Outside Probability</h3><p>Outside score $\alpha_j(p,q)$是non-terminal $N^j$ 生成terminal $w_p \cdots w_q$<strong>之外</strong>的所有的non-ternimal的概率,也就是$P(w_{1(p-1)},N^j_{pq},w_{(q+1)m}|G)$, 我们可以通过动态规划(Dynamic Programming)来计算。<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/53921346.jpg" alt=""><br>怎么分解成子问题呢？如果是$N^f \to N^g, N^j$我们需要计算的是$w_1 \cdots w_{(p-1)}, w_q \cdots w_{m}$这两段的概率，那我们可以通过计算$N^f$的outside score乘上$N^g$的inside score来计算，如果是$N^f \to N^j, N^g$那么与之前类似，我们需要这两种情况。最终我们得到。<br><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/73296021.jpg" alt=""></p>
<h3 id="用-Inside-Outside-score-来表示-P-N-j-to-N-r-N-s-p-d-q-w-1-m-theta-t"><a href="#用-Inside-Outside-score-来表示-P-N-j-to-N-r-N-s-p-d-q-w-1-m-theta-t" class="headerlink" title="用 Inside+Outside score 来表示 $P(N^j \to N^r \ N^s ,p,d,q|w_{1,m},\theta^t)$"></a>用 Inside+Outside score 来表示 $P(N^j \to N^r \ N^s ,p,d,q|w_{1,m},\theta^t)$</h3><p><img src="http://oj4pv4f25.bkt.clouddn.com/18-8-5/77178935.jpg" alt=""><br>这个就很直观啦，$N^j \to N^r, N^s$并且生成$w_{1,m}$的概率$P(N^j \to N^r \ N^s ,p,d,q|w_{1,m},\theta^t)$ 可以表示为：$N^r$的inside score 乘上 $N^s$的 Inside Score 乘上$N^j$的outside score 乘上$N^j \to N^r, N^s$的概率在 condition on (除以得到条件概率) $S$的inside score!</p>
<h3 id="用人话总结-Inside-Outside"><a href="#用人话总结-Inside-Outside" class="headerlink" title="用人话总结 Inside-Outside"></a>用人话总结 Inside-Outside</h3><ul>
<li>我们没有parse tree 的标注(annotation)，所以我么希望通过EM算法进行无监督学习</li>
<li>EM Algorithm<ul>
<li>Initialize parameters (每个grammar rules $r$在corpus$X$中出现的概率)</li>
<li>E-step: 计算Expected Count (grammar rules $r$在corpus$X$中出现的次数的期望)<ul>
<li>计算Inside Score (DP)</li>
<li>计算Outside Score (DP)</li>
<li>得到用Inside/Outside Score表示Expected Count</li>
</ul>
</li>
<li>M-step: Update Probabilities Use expected counts</li>
<li>跳到E-step, 迭代</li>
</ul>
</li>
<li>最终我们得到了每个grammar rules的概率，于是我们可以用类似<strong>CYK</strong>的算法进行语法树的parsing了。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jeffchy.github.io/2018/08/04/EM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeff Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeff-Chiang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/04/EM/" itemprop="url">Expectation-Maximization Algorithm</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-04T20:31:21+08:00">
                2018-08-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Note/" itemprop="url" rel="index">
                    <span itemprop="name">Note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/04/EM/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/04/EM/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这是M.Collins的Notes中文笔记,内容跟着Notes走,但是笔者会融合一些笔者平时积累的其他资料,Notes原版在M.Collins大神的主页.<br><a href="http://www.cs.columbia.edu/~mcollins/" target="_blank" rel="external">http://www.cs.columbia.edu/~mcollins/</a></p>
<h3 id="Expectation-Maximization-Algorithm"><a href="#Expectation-Maximization-Algorithm" class="headerlink" title="Expectation Maximization Algorithm"></a>Expectation Maximization Algorithm</h3><p>EM 算法是无监督学习(Unsupervised Learning)中的重要算法。Collins大佬的notes中先用我们之前提到的Naive Bayes进行举例，所以如果之前的notes没有看的话，先看一下呗? <a href="/2018/08/04/Naive-Bayes/">Naive-Bayes</a></p>
<h3 id="如果Naive-Bayes的training-data中没有label了怎么办？"><a href="#如果Naive-Bayes的training-data中没有label了怎么办？" class="headerlink" title="如果Naive Bayes的training data中没有label了怎么办？"></a>如果Naive Bayes的training data中没有label了怎么办？</h3><p>首先简单回忆一下Naive Bayes模型。在Supervised，即有(feature, label) pairs 的情况下，我们可以通过统计出$p(y),p(x|y)$计算出一个classifier，之前我们说过！统计出$p(y),p(x|y)$的过程虽然简单，数个数就完事了，但是这实际上是一个<a href="/2018/08/04/MLE/">Maximum Likelihood Estimation</a>的过程！我们通过我们手上的(feature,label) pair，计算出生成这组数据可能性最大的分布的参数！也就是最终计算出了生成这组数据的概率分布，也就是$p(y),p(x|y)$. 为什么要这么麻烦？因为理解MLE的过程是对理解EM算法非常重要的。</p>
<p>没有label是无监督学习的一个常见的设定。如果没有label，那么我们就没有办法通过最大似然估计MLE计算出$p(y),p(x|y)$等一系列概率了。</p>
<h3 id="无监督学习的怪圈"><a href="#无监督学习的怪圈" class="headerlink" title="无监督学习的怪圈"></a>无监督学习的怪圈</h3><p>所以现在出现了严重的问题</p>
<ul>
<li>如果没有label, 我们不可能估计出分布的参数, NB模型中即$p(y),p(x|y)$</li>
<li>如果我们有了分布$p(y),p(x|y)$，我们就可以通过计算联合分布来产生一个NB classifier, 并且可以用它来判别新的数据的label.</li>
</ul>
<h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><p>怎么解决呢？我们没有参数，那么我们就假设一个合理的呗？</p>
<ul>
<li>随机初始化$p(y),p(x|y)$，并且满足他们是合理的概率就行了，概率和条件概率都大于零小于一，并且加起来等于一。现在我们就有我们计算classifier所需要的$p(y),p(x|y)$了。</li>
<li>那么我们就计算classifier呗 <span>$p(y, x_1, x_2, \cdots, x_n) = p(y) \prod_{j=1}^n p(x_j | y)$</span><!-- Has MathJax --></li>
<li>我们可以通过这个classifier计算出每个样本点的feature集合使得样本点表现为不同label的概率，这里的$\theta$指得是我们上一次嘉定的参数。即$p(y|x^{(i)}, \theta^{t-1})=\frac{p(y,x_1,x_2,\cdots,x_n)}{p(x_1,x_2,\cdots,x_n)}=\frac{p(y,x_1,x_2,\cdots,x_n)}{\sum_j p(y_j, x_1,x_2,\cdots,x_n)}$, 这一步就是普通的条件概率的计算。分母marginalize掉所有可能的label.</li>
<li>我们通过我们这个暂时的classifier得到了所有的样本点的label的条件概率分布之后，我们就可对通过计算获得最新的参数了，$p(y),p(x|y)$，反复迭代</li>
<li>整个算法的具体过程如下图<img src="http://oj4pv4f25.bkt.clouddn.com/18-8-4/36887730.jpg" alt=""></li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>EM 算法通常用在无监督学习，没有label的任务中</li>
<li>input: Data样本和最终准备归为的几类 output: 每个样本被明确assign</li>
<li>EM 算法的具体步骤<ul>
<li>随机初始化模型分布的参数 $\theta$</li>
<li>已知参数计算出每个样本的label的分布$p(y|x^{(i)},\theta)$</li>
<li>用类似MLE的方法更新参数 $\theta$</li>
<li>回到第二步，不断迭代。</li>
</ul>
</li>
<li>EM 中更新参数与 MLE 的关系：如果EM中的y的分布非0即1的话那么久等同于MLE,但是EM中的y是一个分布,比如$p(y=y^{(i)})=0.3$</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Jeff Chiang" />
          <p class="site-author-name" itemprop="name">Jeff Chiang</p>
           
              <p class="site-description motion-element" itemprop="description">Personal portal for Sharing Computer Science Technology and some petty things in my life</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">58</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">34</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/jeffchy" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="jiangchy@shanghaitech.edu.cn" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                    
                      E-Mail
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/jeffchiang/" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      微博
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jeff Chiang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jeffchy-github-io.disqus.com/count.js" async></script>
    

    

  




	





  










  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
